{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from task.TaskGenerator import SinTaskGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from network.regressors_network import RegressorNetwork\n",
    "from learner.MetaLearner import RegressorMAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(torch.nn.Module):\n",
    "    def __init__(self, n_x=1):\n",
    "        super(ImprovedModel, self).__init__()\n",
    "        self.sampling_layer = torch.nn.Linear(6, 16) # 6 input: x, f_t(x), f_(t-1)(x), z_(t-1)\n",
    "        self.sampling_layer2 = torch.nn.Linear(16, n_x)\n",
    "        \n",
    "        self.enc1 = torch.nn.Linear(8, 32) # 6 input of before + output of sampling layer\n",
    "        self.enc2 = torch.nn.GRU(input_size=32, hidden_size=16, num_layers=3, batch_first=True)\n",
    "        self.enc3 = torch.nn.Linear(16, 16)\n",
    "        self.enc4 = torch.nn.Linear(16, 3)\n",
    "        \n",
    "    def build_input(self, context, prev_z):\n",
    "        n_batch = context.shape[0]\n",
    "        seq_len = context.shape[1]\n",
    "        \n",
    "        prev_z = prev_z.reshape(n_batch, 1, 3)\n",
    "        prev_z = prev_z.repeat(1, seq_len, 1)\n",
    "\n",
    "        context = torch.cat([context, prev_z], dim=2)\n",
    "        return context\n",
    "        \n",
    "    def sample(self, x, f, curr_param):\n",
    "        x = F.elu(self.sampling_layer(x))\n",
    "        x = F.elu(self.sampling_layer2(x))\n",
    "        y = f(x, curr_param)\n",
    "        \n",
    "        return y, x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        n_batch = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        t = F.elu(self.enc1(x)).view(n_batch, seq_len, 32)\n",
    "        t = self.enc2(t)[0][:, -1, :] # we are interested only in the last output of the sequence\n",
    "        t = F.elu(t)\n",
    "        t = F.elu(self.enc3(t))\n",
    "        return  self.enc4(t)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, context, prev_z, prev_f):\n",
    "        original_x = self.build_input(context=context, prev_z=prev_z)\n",
    "        sample_y, sample_x = self.sample(x=original_x, f=prev_f, curr_param=prev_z)\n",
    "        final_context = torch.cat([original_x, sample_x, sample_y], dim=2)\n",
    "        return self.encode(final_context)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImprovedModel()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_batch_function(x, curr_param):\n",
    "    n_batch = x.shape[0]\n",
    "    seq_len = x.shape[1]\n",
    "    \n",
    "    res = torch.empty(n_batch, seq_len, 1)\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        res[i, :, :] = curr_param[i, 0] * torch.sin(curr_param[i, 2] * x[i] + curr_param[i, 1])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_f, f):\n",
    "    MSE = F.mse_loss(recon_f, f)\n",
    "    \n",
    "    return MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_gen = SinTaskGenerator(x_min=-5, x_max=5)\n",
    "dataset = []\n",
    "\n",
    "# Task parameters range\n",
    "max_a = 0.1\n",
    "min_a = 10\n",
    "\n",
    "min_phase = -2\n",
    "max_phase = 2\n",
    "\n",
    "min_freq = 0.1\n",
    "max_freq = 10\n",
    "\n",
    "def sample_task(n_batches=10, test_perc=0, batch_size=128):\n",
    "    a = (min_a - max_a) * torch.rand(1) + max_a\n",
    "    phase = (min_phase - max_phase) * torch.rand(1) + max_phase\n",
    "    f = (min_freq - max_freq) * torch.rand(1) + max_freq\n",
    "    \n",
    "    \"\"\"\n",
    "    a_t2 = a_t1 + torch.rand(1) * offset_a_max\n",
    "    phase_t2 = phase_t1 + torch.rand(1) * offset_phase_max\n",
    "    f_t2 = f_t1 + torch.rand(1) * offset_f_max\n",
    "    \"\"\"\n",
    "    data = task_gen.get_data_loader(amplitude=a, \n",
    "                                    phase=phase,\n",
    "                                    frequency=f,\n",
    "                                    num_batches=n_batches,\n",
    "                                    test_perc=test_perc, \n",
    "                                    batch_size=batch_size)\n",
    "\n",
    "    return data, a, phase, f\n",
    "\n",
    "# Dataset creation\n",
    "n_tasks = 1000\n",
    "data_set = []\n",
    "a_set = []\n",
    "phase_set = []\n",
    "f_set = []\n",
    "param = []\n",
    "for _ in range(n_tasks):\n",
    "    data, a, phase, f = sample_task(n_batches=1, test_perc=0, batch_size=128)\n",
    "    data_set.append(data)\n",
    "    a_set.append(a)\n",
    "    phase_set.append(phase)\n",
    "    f_set.append(f)\n",
    "    param.append((a.item(), phase.item(), f.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 2\n",
    "\n",
    "def get_input(n_batch=10):\n",
    "    train_loss = 0\n",
    "    n_batch = 32\n",
    "    batch_per_task = 1\n",
    "    task_idx = torch.randint(low=0, high=n_tasks, size=(n_batch,))\n",
    "    task_loader = [data_set[i] for i in task_idx]\n",
    "    curr_param = [param[i] for i in task_idx]\n",
    "\n",
    "    for k in range(batch_per_task):\n",
    "        num_data_context = torch.randint(low=5, high=25, size=(1,)).item()\n",
    "        idx = torch.randint(0, 128, (128,))\n",
    "        ctx_idx = idx[0:num_data_context]\n",
    "        \n",
    "        context = torch.empty(n_batch, num_data_context, 3)\n",
    "        target = torch.empty(n_batch, 3)\n",
    "\n",
    "        # Retrieving data to be fed to the network \n",
    "        i = 0\n",
    "        for t_idx, task in enumerate(task_loader):\n",
    "            # Creating new task\n",
    "            a_offset = torch.rand(1) * offset\n",
    "            p_offset = torch.rand(1) * offset\n",
    "            f_offset = torch.rand(1) * offset\n",
    "            offset_param = torch.Tensor([a_offset, p_offset, f_offset])\n",
    "            target[i] = offset_param\n",
    "\n",
    "            # Creating context to be fed to the network \n",
    "            batch = task[k]['train']\n",
    "            x = batch[0]\n",
    "            task_pred = (curr_param[t_idx][0]+a_offset) * torch.sin((curr_param[t_idx][2]+f_offset)*x+(curr_param[t_idx][1]+p_offset))\n",
    "            batch = torch.cat([batch[0], batch[1], task_pred], dim=1)\n",
    "            context[i] = batch[ctx_idx]\n",
    "            i+=1\n",
    "\n",
    "        curr_param = torch.Tensor(curr_param)\n",
    "        return context, curr_param\n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "def batch_train_offset(epoch, n_batch=10):\n",
    "    train_loss = 0\n",
    "    n_batch = 32\n",
    "    batch_per_task = 1\n",
    "    task_idx = torch.randint(low=0, high=n_tasks, size=(n_batch,))\n",
    "    task_loader = [data_set[i] for i in task_idx]\n",
    "    curr_param = [param[i] for i in task_idx]\n",
    "\n",
    "    for k in range(batch_per_task):\n",
    "        num_data_context = torch.randint(low=5, high=25, size=(1,)).item()\n",
    "        idx = torch.randint(0, 128, (128,))\n",
    "        ctx_idx = idx[0:num_data_context]\n",
    "        \n",
    "        context = torch.empty(n_batch, num_data_context, 3)\n",
    "        target = torch.empty(n_batch, 3)\n",
    "\n",
    "        # Retrieving data to be fed to the network \n",
    "        i = 0\n",
    "        for t_idx, task in enumerate(task_loader):\n",
    "            # Creating new task\n",
    "            a_offset = torch.rand(1) * offset\n",
    "            p_offset = torch.rand(1) * offset\n",
    "            f_offset = torch.rand(1) * offset\n",
    "            offset_param = torch.Tensor([a_offset, p_offset, f_offset])\n",
    "            target[i] = offset_param\n",
    "\n",
    "            # Creating context to be fed to the network \n",
    "            batch = task[k]['train']\n",
    "            x = batch[0]\n",
    "            task_pred = (curr_param[t_idx][0]+a_offset) * torch.sin((curr_param[t_idx][2]+f_offset)*x+(curr_param[t_idx][1]+p_offset))\n",
    "            batch = torch.cat([batch[0], batch[1], task_pred], dim=1)\n",
    "            context[i] = batch[ctx_idx]\n",
    "            i+=1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        curr_param = torch.Tensor(curr_param)\n",
    "        z_hat = model(context, curr_param, prev_batch_function)\n",
    "\n",
    "        # Compute reconstruction\n",
    "        loss = loss_function(z_hat, target)\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return train_loss / (batch_per_task)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average loss: 0.2426\n",
      "====> Epoch: 100 Average loss: 0.2425\n",
      "====> Epoch: 200 Average loss: 0.2425\n",
      "====> Epoch: 300 Average loss: 0.2424\n",
      "====> Epoch: 400 Average loss: 0.2424\n",
      "====> Epoch: 500 Average loss: 0.2423\n",
      "====> Epoch: 600 Average loss: 0.2423\n",
      "====> Epoch: 700 Average loss: 0.2422\n",
      "====> Epoch: 800 Average loss: 0.2422\n",
      "====> Epoch: 900 Average loss: 0.2421\n",
      "====> Epoch: 1000 Average loss: 0.2421\n",
      "====> Epoch: 1100 Average loss: 0.2420\n",
      "====> Epoch: 1200 Average loss: 0.2420\n",
      "====> Epoch: 1300 Average loss: 0.2419\n",
      "====> Epoch: 1400 Average loss: 0.2419\n",
      "====> Epoch: 1500 Average loss: 0.2418\n",
      "====> Epoch: 1600 Average loss: 0.2418\n",
      "====> Epoch: 1700 Average loss: 0.2418\n",
      "====> Epoch: 1800 Average loss: 0.2417\n",
      "====> Epoch: 1900 Average loss: 0.2417\n",
      "====> Epoch: 2000 Average loss: 0.2416\n",
      "====> Epoch: 2100 Average loss: 0.2415\n",
      "====> Epoch: 2200 Average loss: 0.2415\n",
      "====> Epoch: 2300 Average loss: 0.2414\n",
      "====> Epoch: 2400 Average loss: 0.2414\n",
      "====> Epoch: 2500 Average loss: 0.2413\n",
      "====> Epoch: 2600 Average loss: 0.2413\n",
      "====> Epoch: 2700 Average loss: 0.2413\n",
      "====> Epoch: 2800 Average loss: 0.2412\n",
      "====> Epoch: 2900 Average loss: 0.2412\n",
      "====> Epoch: 3000 Average loss: 0.2411\n",
      "====> Epoch: 3100 Average loss: 0.2411\n",
      "====> Epoch: 3200 Average loss: 0.2411\n",
      "====> Epoch: 3300 Average loss: 0.2410\n",
      "====> Epoch: 3400 Average loss: 0.2410\n",
      "====> Epoch: 3500 Average loss: 0.2409\n",
      "====> Epoch: 3600 Average loss: 0.2409\n",
      "====> Epoch: 3700 Average loss: 0.2409\n",
      "====> Epoch: 3800 Average loss: 0.2408\n",
      "====> Epoch: 3900 Average loss: 0.2408\n",
      "====> Epoch: 4000 Average loss: 0.2408\n",
      "====> Epoch: 4100 Average loss: 0.2407\n",
      "====> Epoch: 4200 Average loss: 0.2407\n",
      "====> Epoch: 4300 Average loss: 0.2407\n",
      "====> Epoch: 4400 Average loss: 0.2406\n",
      "====> Epoch: 4500 Average loss: 0.2406\n",
      "====> Epoch: 4600 Average loss: 0.2405\n",
      "====> Epoch: 4700 Average loss: 0.2405\n",
      "====> Epoch: 4800 Average loss: 0.2405\n",
      "====> Epoch: 4900 Average loss: 0.2404\n",
      "====> Epoch: 5000 Average loss: 0.2404\n",
      "====> Epoch: 5100 Average loss: 0.2404\n",
      "====> Epoch: 5200 Average loss: 0.2403\n",
      "====> Epoch: 5300 Average loss: 0.2403\n",
      "====> Epoch: 5400 Average loss: 0.2403\n",
      "====> Epoch: 5500 Average loss: 0.2402\n",
      "====> Epoch: 5600 Average loss: 0.2402\n",
      "====> Epoch: 5700 Average loss: 0.2402\n",
      "====> Epoch: 5800 Average loss: 0.2401\n",
      "====> Epoch: 5900 Average loss: 0.2401\n",
      "====> Epoch: 6000 Average loss: 0.2400\n",
      "====> Epoch: 6100 Average loss: 0.2400\n",
      "====> Epoch: 6200 Average loss: 0.2400\n",
      "====> Epoch: 6300 Average loss: 0.2399\n",
      "====> Epoch: 6400 Average loss: 0.2399\n",
      "====> Epoch: 6500 Average loss: 0.2398\n",
      "====> Epoch: 6600 Average loss: 0.2398\n",
      "====> Epoch: 6700 Average loss: 0.2398\n",
      "====> Epoch: 6800 Average loss: 0.2397\n",
      "====> Epoch: 6900 Average loss: 0.2397\n",
      "====> Epoch: 7000 Average loss: 0.2397\n",
      "====> Epoch: 7100 Average loss: 0.2396\n",
      "====> Epoch: 7200 Average loss: 0.2396\n",
      "====> Epoch: 7300 Average loss: 0.2396\n",
      "====> Epoch: 7400 Average loss: 0.2395\n",
      "====> Epoch: 7500 Average loss: 0.2395\n",
      "====> Epoch: 7600 Average loss: 0.2395\n",
      "====> Epoch: 7700 Average loss: 0.2395\n",
      "====> Epoch: 7800 Average loss: 0.2394\n",
      "====> Epoch: 7900 Average loss: 0.2394\n",
      "====> Epoch: 8000 Average loss: 0.2394\n",
      "====> Epoch: 8100 Average loss: 0.2393\n",
      "====> Epoch: 8200 Average loss: 0.2393\n",
      "====> Epoch: 8300 Average loss: 0.2393\n",
      "====> Epoch: 8400 Average loss: 0.2393\n",
      "====> Epoch: 8500 Average loss: 0.2392\n",
      "====> Epoch: 8600 Average loss: 0.2392\n",
      "====> Epoch: 8700 Average loss: 0.2392\n",
      "====> Epoch: 8800 Average loss: 0.2392\n",
      "====> Epoch: 8900 Average loss: 0.2391\n",
      "====> Epoch: 9000 Average loss: 0.2391\n",
      "====> Epoch: 9100 Average loss: 0.2391\n",
      "====> Epoch: 9200 Average loss: 0.2390\n",
      "====> Epoch: 9300 Average loss: 0.2390\n",
      "====> Epoch: 9400 Average loss: 0.2390\n",
      "====> Epoch: 9500 Average loss: 0.2389\n",
      "====> Epoch: 9600 Average loss: 0.2389\n",
      "====> Epoch: 9700 Average loss: 0.2389\n",
      "====> Epoch: 9800 Average loss: 0.2389\n",
      "====> Epoch: 9900 Average loss: 0.2389\n",
      "====> Epoch: 10000 Average loss: 0.2389\n",
      "====> Epoch: 10100 Average loss: 0.2388\n",
      "====> Epoch: 10200 Average loss: 0.2388\n",
      "====> Epoch: 10300 Average loss: 0.2388\n",
      "====> Epoch: 10400 Average loss: 0.2387\n",
      "====> Epoch: 10500 Average loss: 0.2387\n",
      "====> Epoch: 10600 Average loss: 0.2387\n",
      "====> Epoch: 10700 Average loss: 0.2387\n",
      "====> Epoch: 10800 Average loss: 0.2386\n",
      "====> Epoch: 10900 Average loss: 0.2386\n",
      "====> Epoch: 11000 Average loss: 0.2386\n",
      "====> Epoch: 11100 Average loss: 0.2386\n",
      "====> Epoch: 11200 Average loss: 0.2385\n",
      "====> Epoch: 11300 Average loss: 0.2385\n",
      "====> Epoch: 11400 Average loss: 0.2385\n",
      "====> Epoch: 11500 Average loss: 0.2385\n",
      "====> Epoch: 11600 Average loss: 0.2385\n",
      "====> Epoch: 11700 Average loss: 0.2384\n",
      "====> Epoch: 11800 Average loss: 0.2384\n",
      "====> Epoch: 11900 Average loss: 0.2384\n",
      "====> Epoch: 12000 Average loss: 0.2383\n",
      "====> Epoch: 12100 Average loss: 0.2383\n",
      "====> Epoch: 12200 Average loss: 0.2383\n",
      "====> Epoch: 12300 Average loss: 0.2383\n",
      "====> Epoch: 12400 Average loss: 0.2383\n",
      "====> Epoch: 12500 Average loss: 0.2382\n",
      "====> Epoch: 12600 Average loss: 0.2382\n",
      "====> Epoch: 12700 Average loss: 0.2382\n",
      "====> Epoch: 12800 Average loss: 0.2382\n",
      "====> Epoch: 12900 Average loss: 0.2382\n",
      "====> Epoch: 13000 Average loss: 0.2381\n",
      "====> Epoch: 13100 Average loss: 0.2381\n",
      "====> Epoch: 13200 Average loss: 0.2381\n",
      "====> Epoch: 13300 Average loss: 0.2381\n",
      "====> Epoch: 13400 Average loss: 0.2380\n",
      "====> Epoch: 13500 Average loss: 0.2380\n",
      "====> Epoch: 13600 Average loss: 0.2380\n",
      "====> Epoch: 13700 Average loss: 0.2380\n",
      "====> Epoch: 13800 Average loss: 0.2380\n",
      "====> Epoch: 13900 Average loss: 0.2380\n",
      "====> Epoch: 14000 Average loss: 0.2379\n",
      "====> Epoch: 14100 Average loss: 0.2379\n",
      "====> Epoch: 14200 Average loss: 0.2379\n",
      "====> Epoch: 14300 Average loss: 0.2378\n",
      "====> Epoch: 14400 Average loss: 0.2378\n",
      "====> Epoch: 14500 Average loss: 0.2378\n",
      "====> Epoch: 14600 Average loss: 0.2378\n",
      "====> Epoch: 14700 Average loss: 0.2378\n",
      "====> Epoch: 14800 Average loss: 0.2378\n",
      "====> Epoch: 14900 Average loss: 0.2378\n",
      "====> Epoch: 15000 Average loss: 0.2378\n",
      "====> Epoch: 15100 Average loss: 0.2378\n",
      "====> Epoch: 15200 Average loss: 0.2377\n",
      "====> Epoch: 15300 Average loss: 0.2377\n",
      "====> Epoch: 15400 Average loss: 0.2377\n",
      "====> Epoch: 15500 Average loss: 0.2377\n",
      "====> Epoch: 15600 Average loss: 0.2377\n",
      "====> Epoch: 15700 Average loss: 0.2377\n",
      "====> Epoch: 15800 Average loss: 0.2377\n",
      "====> Epoch: 15900 Average loss: 0.2376\n",
      "====> Epoch: 16000 Average loss: 0.2376\n",
      "====> Epoch: 16100 Average loss: 0.2376\n",
      "====> Epoch: 16200 Average loss: 0.2376\n",
      "====> Epoch: 16300 Average loss: 0.2376\n",
      "====> Epoch: 16400 Average loss: 0.2376\n",
      "====> Epoch: 16500 Average loss: 0.2376\n",
      "====> Epoch: 16600 Average loss: 0.2375\n",
      "====> Epoch: 16700 Average loss: 0.2375\n",
      "====> Epoch: 16800 Average loss: 0.2375\n",
      "====> Epoch: 16900 Average loss: 0.2375\n",
      "====> Epoch: 17000 Average loss: 0.2375\n",
      "====> Epoch: 17100 Average loss: 0.2375\n",
      "====> Epoch: 17200 Average loss: 0.2374\n",
      "====> Epoch: 17300 Average loss: 0.2374\n",
      "====> Epoch: 17400 Average loss: 0.2374\n",
      "====> Epoch: 17500 Average loss: 0.2374\n",
      "====> Epoch: 17600 Average loss: 0.2374\n",
      "====> Epoch: 17700 Average loss: 0.2374\n",
      "====> Epoch: 17800 Average loss: 0.2374\n",
      "====> Epoch: 17900 Average loss: 0.2374\n",
      "====> Epoch: 18000 Average loss: 0.2374\n",
      "====> Epoch: 18100 Average loss: 0.2373\n",
      "====> Epoch: 18200 Average loss: 0.2373\n",
      "====> Epoch: 18300 Average loss: 0.2373\n",
      "====> Epoch: 18400 Average loss: 0.2373\n",
      "====> Epoch: 18500 Average loss: 0.2373\n",
      "====> Epoch: 18600 Average loss: 0.2373\n",
      "====> Epoch: 18700 Average loss: 0.2373\n",
      "====> Epoch: 18800 Average loss: 0.2373\n",
      "====> Epoch: 18900 Average loss: 0.2372\n",
      "====> Epoch: 19000 Average loss: 0.2372\n",
      "====> Epoch: 19100 Average loss: 0.2372\n",
      "====> Epoch: 19200 Average loss: 0.2372\n",
      "====> Epoch: 19300 Average loss: 0.2371\n",
      "====> Epoch: 19400 Average loss: 0.2371\n",
      "====> Epoch: 19500 Average loss: 0.2371\n",
      "====> Epoch: 19600 Average loss: 0.2371\n",
      "====> Epoch: 19700 Average loss: 0.2371\n",
      "====> Epoch: 19800 Average loss: 0.2371\n",
      "====> Epoch: 19900 Average loss: 0.2371\n",
      "====> Epoch: 20000 Average loss: 0.2370\n",
      "====> Epoch: 20100 Average loss: 0.2370\n",
      "====> Epoch: 20200 Average loss: 0.2370\n",
      "====> Epoch: 20300 Average loss: 0.2370\n",
      "====> Epoch: 20400 Average loss: 0.2370\n",
      "====> Epoch: 20500 Average loss: 0.2370\n",
      "====> Epoch: 20600 Average loss: 0.2370\n",
      "====> Epoch: 20700 Average loss: 0.2370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20800 Average loss: 0.2370\n",
      "====> Epoch: 20900 Average loss: 0.2370\n",
      "====> Epoch: 21000 Average loss: 0.2369\n",
      "====> Epoch: 21100 Average loss: 0.2369\n",
      "====> Epoch: 21200 Average loss: 0.2369\n",
      "====> Epoch: 21300 Average loss: 0.2369\n",
      "====> Epoch: 21400 Average loss: 0.2369\n",
      "====> Epoch: 21500 Average loss: 0.2369\n",
      "====> Epoch: 21600 Average loss: 0.2369\n",
      "====> Epoch: 21700 Average loss: 0.2368\n",
      "====> Epoch: 21800 Average loss: 0.2368\n",
      "====> Epoch: 21900 Average loss: 0.2368\n",
      "====> Epoch: 22000 Average loss: 0.2368\n",
      "====> Epoch: 22100 Average loss: 0.2368\n",
      "====> Epoch: 22200 Average loss: 0.2368\n",
      "====> Epoch: 22300 Average loss: 0.2368\n",
      "====> Epoch: 22400 Average loss: 0.2368\n",
      "====> Epoch: 22500 Average loss: 0.2368\n",
      "====> Epoch: 22600 Average loss: 0.2367\n",
      "====> Epoch: 22700 Average loss: 0.2367\n",
      "====> Epoch: 22800 Average loss: 0.2367\n",
      "====> Epoch: 22900 Average loss: 0.2367\n",
      "====> Epoch: 23000 Average loss: 0.2367\n",
      "====> Epoch: 23100 Average loss: 0.2367\n",
      "====> Epoch: 23200 Average loss: 0.2367\n",
      "====> Epoch: 23300 Average loss: 0.2366\n",
      "====> Epoch: 23400 Average loss: 0.2366\n",
      "====> Epoch: 23500 Average loss: 0.2366\n",
      "====> Epoch: 23600 Average loss: 0.2366\n",
      "====> Epoch: 23700 Average loss: 0.2366\n",
      "====> Epoch: 23800 Average loss: 0.2366\n",
      "====> Epoch: 23900 Average loss: 0.2366\n",
      "====> Epoch: 24000 Average loss: 0.2366\n",
      "====> Epoch: 24100 Average loss: 0.2365\n",
      "====> Epoch: 24200 Average loss: 0.2365\n",
      "====> Epoch: 24300 Average loss: 0.2365\n",
      "====> Epoch: 24400 Average loss: 0.2365\n",
      "====> Epoch: 24500 Average loss: 0.2365\n",
      "====> Epoch: 24600 Average loss: 0.2365\n",
      "====> Epoch: 24700 Average loss: 0.2365\n",
      "====> Epoch: 24800 Average loss: 0.2365\n",
      "====> Epoch: 24900 Average loss: 0.2365\n",
      "====> Epoch: 25000 Average loss: 0.2365\n",
      "====> Epoch: 25100 Average loss: 0.2365\n",
      "====> Epoch: 25200 Average loss: 0.2364\n",
      "====> Epoch: 25300 Average loss: 0.2364\n",
      "====> Epoch: 25400 Average loss: 0.2364\n",
      "====> Epoch: 25500 Average loss: 0.2364\n",
      "====> Epoch: 25600 Average loss: 0.2364\n",
      "====> Epoch: 25700 Average loss: 0.2364\n",
      "====> Epoch: 25800 Average loss: 0.2364\n",
      "====> Epoch: 25900 Average loss: 0.2364\n",
      "====> Epoch: 26000 Average loss: 0.2363\n",
      "====> Epoch: 26100 Average loss: 0.2363\n",
      "====> Epoch: 26200 Average loss: 0.2363\n",
      "====> Epoch: 26300 Average loss: 0.2363\n",
      "====> Epoch: 26400 Average loss: 0.2363\n",
      "====> Epoch: 26500 Average loss: 0.2363\n",
      "====> Epoch: 26600 Average loss: 0.2363\n",
      "====> Epoch: 26700 Average loss: 0.2363\n",
      "====> Epoch: 26800 Average loss: 0.2363\n",
      "====> Epoch: 26900 Average loss: 0.2363\n",
      "====> Epoch: 27000 Average loss: 0.2363\n",
      "====> Epoch: 27100 Average loss: 0.2363\n",
      "====> Epoch: 27200 Average loss: 0.2363\n",
      "====> Epoch: 27300 Average loss: 0.2363\n",
      "====> Epoch: 27400 Average loss: 0.2363\n",
      "====> Epoch: 27500 Average loss: 0.2363\n",
      "====> Epoch: 27600 Average loss: 0.2362\n",
      "====> Epoch: 27700 Average loss: 0.2362\n",
      "====> Epoch: 27800 Average loss: 0.2362\n",
      "====> Epoch: 27900 Average loss: 0.2362\n",
      "====> Epoch: 28000 Average loss: 0.2362\n",
      "====> Epoch: 28100 Average loss: 0.2362\n",
      "====> Epoch: 28200 Average loss: 0.2362\n",
      "====> Epoch: 28300 Average loss: 0.2362\n",
      "====> Epoch: 28400 Average loss: 0.2362\n",
      "====> Epoch: 28500 Average loss: 0.2362\n",
      "====> Epoch: 28600 Average loss: 0.2362\n",
      "====> Epoch: 28700 Average loss: 0.2362\n",
      "====> Epoch: 28800 Average loss: 0.2361\n",
      "====> Epoch: 28900 Average loss: 0.2361\n",
      "====> Epoch: 29000 Average loss: 0.2361\n",
      "====> Epoch: 29100 Average loss: 0.2361\n",
      "====> Epoch: 29200 Average loss: 0.2361\n",
      "====> Epoch: 29300 Average loss: 0.2361\n",
      "====> Epoch: 29400 Average loss: 0.2361\n",
      "====> Epoch: 29500 Average loss: 0.2360\n",
      "====> Epoch: 29600 Average loss: 0.2360\n",
      "====> Epoch: 29700 Average loss: 0.2360\n",
      "====> Epoch: 29800 Average loss: 0.2360\n",
      "====> Epoch: 29900 Average loss: 0.2359\n",
      "====> Epoch: 30000 Average loss: 0.2359\n",
      "====> Epoch: 30100 Average loss: 0.2359\n",
      "====> Epoch: 30200 Average loss: 0.2359\n",
      "====> Epoch: 30300 Average loss: 0.2358\n",
      "====> Epoch: 30400 Average loss: 0.2358\n",
      "====> Epoch: 30500 Average loss: 0.2358\n",
      "====> Epoch: 30600 Average loss: 0.2357\n",
      "====> Epoch: 30700 Average loss: 0.2357\n",
      "====> Epoch: 30800 Average loss: 0.2357\n",
      "====> Epoch: 30900 Average loss: 0.2356\n",
      "====> Epoch: 31000 Average loss: 0.2356\n",
      "====> Epoch: 31100 Average loss: 0.2355\n",
      "====> Epoch: 31200 Average loss: 0.2355\n",
      "====> Epoch: 31300 Average loss: 0.2355\n",
      "====> Epoch: 31400 Average loss: 0.2354\n",
      "====> Epoch: 31500 Average loss: 0.2354\n",
      "====> Epoch: 31600 Average loss: 0.2354\n",
      "====> Epoch: 31700 Average loss: 0.2353\n",
      "====> Epoch: 31800 Average loss: 0.2353\n",
      "====> Epoch: 31900 Average loss: 0.2352\n",
      "====> Epoch: 32000 Average loss: 0.2352\n",
      "====> Epoch: 32100 Average loss: 0.2352\n",
      "====> Epoch: 32200 Average loss: 0.2351\n",
      "====> Epoch: 32300 Average loss: 0.2351\n",
      "====> Epoch: 32400 Average loss: 0.2350\n",
      "====> Epoch: 32500 Average loss: 0.2350\n",
      "====> Epoch: 32600 Average loss: 0.2349\n",
      "====> Epoch: 32700 Average loss: 0.2349\n",
      "====> Epoch: 32800 Average loss: 0.2349\n",
      "====> Epoch: 32900 Average loss: 0.2348\n",
      "====> Epoch: 33000 Average loss: 0.2348\n",
      "====> Epoch: 33100 Average loss: 0.2347\n",
      "====> Epoch: 33200 Average loss: 0.2347\n",
      "====> Epoch: 33300 Average loss: 0.2347\n",
      "====> Epoch: 33400 Average loss: 0.2346\n",
      "====> Epoch: 33500 Average loss: 0.2346\n",
      "====> Epoch: 33600 Average loss: 0.2345\n",
      "====> Epoch: 33700 Average loss: 0.2345\n",
      "====> Epoch: 33800 Average loss: 0.2345\n",
      "====> Epoch: 33900 Average loss: 0.2344\n",
      "====> Epoch: 34000 Average loss: 0.2344\n",
      "====> Epoch: 34100 Average loss: 0.2343\n",
      "====> Epoch: 34200 Average loss: 0.2343\n",
      "====> Epoch: 34300 Average loss: 0.2343\n",
      "====> Epoch: 34400 Average loss: 0.2342\n",
      "====> Epoch: 34500 Average loss: 0.2342\n",
      "====> Epoch: 34600 Average loss: 0.2341\n",
      "====> Epoch: 34700 Average loss: 0.2341\n",
      "====> Epoch: 34800 Average loss: 0.2341\n",
      "====> Epoch: 34900 Average loss: 0.2340\n",
      "====> Epoch: 35000 Average loss: 0.2340\n",
      "====> Epoch: 35100 Average loss: 0.2339\n",
      "====> Epoch: 35200 Average loss: 0.2339\n",
      "====> Epoch: 35300 Average loss: 0.2338\n",
      "====> Epoch: 35400 Average loss: 0.2338\n",
      "====> Epoch: 35500 Average loss: 0.2337\n",
      "====> Epoch: 35600 Average loss: 0.2337\n",
      "====> Epoch: 35700 Average loss: 0.2337\n",
      "====> Epoch: 35800 Average loss: 0.2336\n",
      "====> Epoch: 35900 Average loss: 0.2336\n",
      "====> Epoch: 36000 Average loss: 0.2335\n",
      "====> Epoch: 36100 Average loss: 0.2335\n",
      "====> Epoch: 36200 Average loss: 0.2335\n",
      "====> Epoch: 36300 Average loss: 0.2334\n",
      "====> Epoch: 36400 Average loss: 0.2334\n",
      "====> Epoch: 36500 Average loss: 0.2333\n",
      "====> Epoch: 36600 Average loss: 0.2333\n",
      "====> Epoch: 36700 Average loss: 0.2333\n",
      "====> Epoch: 36800 Average loss: 0.2332\n",
      "====> Epoch: 36900 Average loss: 0.2332\n",
      "====> Epoch: 37000 Average loss: 0.2332\n",
      "====> Epoch: 37100 Average loss: 0.2331\n",
      "====> Epoch: 37200 Average loss: 0.2331\n",
      "====> Epoch: 37300 Average loss: 0.2330\n",
      "====> Epoch: 37400 Average loss: 0.2330\n",
      "====> Epoch: 37500 Average loss: 0.2329\n",
      "====> Epoch: 37600 Average loss: 0.2329\n",
      "====> Epoch: 37700 Average loss: 0.2328\n",
      "====> Epoch: 37800 Average loss: 0.2328\n",
      "====> Epoch: 37900 Average loss: 0.2327\n",
      "====> Epoch: 38000 Average loss: 0.2327\n",
      "====> Epoch: 38100 Average loss: 0.2327\n",
      "====> Epoch: 38200 Average loss: 0.2326\n",
      "====> Epoch: 38300 Average loss: 0.2326\n",
      "====> Epoch: 38400 Average loss: 0.2325\n",
      "====> Epoch: 38500 Average loss: 0.2325\n",
      "====> Epoch: 38600 Average loss: 0.2324\n",
      "====> Epoch: 38700 Average loss: 0.2324\n",
      "====> Epoch: 38800 Average loss: 0.2324\n",
      "====> Epoch: 38900 Average loss: 0.2323\n",
      "====> Epoch: 39000 Average loss: 0.2323\n",
      "====> Epoch: 39100 Average loss: 0.2323\n",
      "====> Epoch: 39200 Average loss: 0.2322\n",
      "====> Epoch: 39300 Average loss: 0.2322\n",
      "====> Epoch: 39400 Average loss: 0.2321\n",
      "====> Epoch: 39500 Average loss: 0.2321\n",
      "====> Epoch: 39600 Average loss: 0.2321\n",
      "====> Epoch: 39700 Average loss: 0.2320\n",
      "====> Epoch: 39800 Average loss: 0.2320\n",
      "====> Epoch: 39900 Average loss: 0.2319\n",
      "====> Epoch: 40000 Average loss: 0.2319\n",
      "====> Epoch: 40100 Average loss: 0.2318\n",
      "====> Epoch: 40200 Average loss: 0.2318\n",
      "====> Epoch: 40300 Average loss: 0.2318\n",
      "====> Epoch: 40400 Average loss: 0.2317\n",
      "====> Epoch: 40500 Average loss: 0.2317\n",
      "====> Epoch: 40600 Average loss: 0.2316\n",
      "====> Epoch: 40700 Average loss: 0.2316\n",
      "====> Epoch: 40800 Average loss: 0.2315\n",
      "====> Epoch: 40900 Average loss: 0.2315\n",
      "====> Epoch: 41000 Average loss: 0.2315\n",
      "====> Epoch: 41100 Average loss: 0.2314\n",
      "====> Epoch: 41200 Average loss: 0.2314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 41300 Average loss: 0.2314\n",
      "====> Epoch: 41400 Average loss: 0.2313\n",
      "====> Epoch: 41500 Average loss: 0.2313\n",
      "====> Epoch: 41600 Average loss: 0.2312\n",
      "====> Epoch: 41700 Average loss: 0.2312\n",
      "====> Epoch: 41800 Average loss: 0.2312\n",
      "====> Epoch: 41900 Average loss: 0.2311\n",
      "====> Epoch: 42000 Average loss: 0.2311\n",
      "====> Epoch: 42100 Average loss: 0.2310\n",
      "====> Epoch: 42200 Average loss: 0.2310\n",
      "====> Epoch: 42300 Average loss: 0.2310\n",
      "====> Epoch: 42400 Average loss: 0.2309\n",
      "====> Epoch: 42500 Average loss: 0.2309\n",
      "====> Epoch: 42600 Average loss: 0.2308\n",
      "====> Epoch: 42700 Average loss: 0.2308\n",
      "====> Epoch: 42800 Average loss: 0.2308\n",
      "====> Epoch: 42900 Average loss: 0.2307\n",
      "====> Epoch: 43000 Average loss: 0.2307\n",
      "====> Epoch: 43100 Average loss: 0.2306\n",
      "====> Epoch: 43200 Average loss: 0.2306\n",
      "====> Epoch: 43300 Average loss: 0.2305\n",
      "====> Epoch: 43400 Average loss: 0.2305\n",
      "====> Epoch: 43500 Average loss: 0.2305\n",
      "====> Epoch: 43600 Average loss: 0.2304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-822916a0801d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_train_offset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0ml_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-142-a18305502fdc>\u001b[0m in \u001b[0;36mbatch_train_offset\u001b[1;34m(epoch, n_batch)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mcurr_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mz_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_batch_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# Compute reconstruction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Riccardo\\Anaconda3\\envs\\sequential-transfer-rl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-c70602dea902>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, context, prev_z, prev_f)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0msample_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moriginal_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprev_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprev_z\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mfinal_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moriginal_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-c70602dea902>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# we are interested only in the last output of the sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Riccardo\\Anaconda3\\envs\\sequential-transfer-rl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Riccardo\\Anaconda3\\envs\\sequential-transfer-rl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 716\u001b[1;33m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100000\n",
    "#losses = []\n",
    "#l_sum = 0\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    l = batch_train_offset(i)\n",
    "    l_sum += l\n",
    "    losses.append(l)\n",
    "    if i % 100 == 0:\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(i, l_sum/(i+1+16878)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "y = np.array(losses)\n",
    "x = np.arange(len(y))\n",
    "yhat = savgol_filter(y,501, 3) \n",
    "\n",
    "plt.plot(x[500:], yhat[500:], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
