{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Product, ConstantKernel as C\n",
    "\n",
    "import gym_sin\n",
    "from gym import spaces\n",
    "\n",
    "from active_learning.arguments import get_args\n",
    "from active_learning.oracle import OracleAgent\n",
    "from active_learning.FixedIdentification import FixedIDAgent\n",
    "from network.vae import InferenceNetwork, InferenceNetwork2, InferenceNetworkNoPrev, InferenceNetworkDirectlyRec\n",
    "from task.GuassianTaskGenerator import GaussianTaskGenerator\n",
    "from utilities.folder_management import handle_folder_creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_sequence(alpha, n_restarts, num_test_processes):\n",
    "    kernel = C(1.0, (1e-5, 1e5)) * RBF(1, (1e-5, 1e5))\n",
    "\n",
    "    gp_list = []\n",
    "    for i in range(2):\n",
    "        gp_list.append([GaussianProcessRegressor(kernel=kernel,\n",
    "                                                 alpha=alpha ** 2,\n",
    "                                                 normalize_y=True,\n",
    "                                                 n_restarts_optimizer=n_restarts)\n",
    "                        for _ in range(num_test_processes)])\n",
    "    test_kwargs = []\n",
    "    init_prior_test = [torch.tensor([[15], [5]], dtype=torch.float32) for _ in range(num_test_processes)]\n",
    "\n",
    "    for idx in range(50):\n",
    "        if idx < 15:\n",
    "            mean = 15\n",
    "            std = 5\n",
    "        elif idx > 40:\n",
    "            mean = 30\n",
    "            std = 5\n",
    "        else:\n",
    "            mean = 15 - idx\n",
    "            std = 5\n",
    "\n",
    "        test_kwargs.append({'amplitude': 1,\n",
    "                            'mean': mean,\n",
    "                            'std': std,\n",
    "                            'noise_std': 0.001,\n",
    "                            'scale_reward': False})\n",
    "\n",
    "    return gp_list, test_kwargs, init_prior_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"gauss-v0\"\n",
    "\n",
    "action_space = spaces.Box(low=np.array([-1]), high=np.array([1]))\n",
    "latent_dim = 1\n",
    "\n",
    "x_min = -100\n",
    "x_max = 100\n",
    "\n",
    "min_mean = -40\n",
    "max_mean = 40\n",
    "\n",
    "prior_mu_min = -10\n",
    "prior_mu_max = 10\n",
    "prior_std_min = 1\n",
    "prior_std_max = 10\n",
    "\n",
    "std = 15\n",
    "amplitude=1\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_generator = GaussianTaskGenerator(x_min, x_max, min_mean, max_mean,\n",
    "                 prior_mu_min, prior_mu_max, prior_std_min, prior_std_max, std, amplitude)\n",
    "fam = task_generator.create_task_family(n_tasks=500, n_batches=1, test_perc=0, batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_old = [100, 10]\n",
    "min_old = [-100, 0]\n",
    "# max_old = None\n",
    "# min_old = None\n",
    "\n",
    "vae_min_seq = 1\n",
    "vae_max_seq = 150\n",
    "\n",
    "obs_shape = (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = InferenceNetworkDirectlyRec(n_in=4, z_dim=latent_dim)\n",
    "vi_optim = torch.optim.Adam(vi.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "num_steps = 30\n",
    "num_processes = 32\n",
    "clip_param = 0.2\n",
    "ppo_epoch=4\n",
    "num_mini_batch=8\n",
    "value_loss_coef=0.5\n",
    "entropy_coef=0.\n",
    "lr=0.0001\n",
    "eps=1e-6 \n",
    "max_grad_norm=0.5\n",
    "use_linear_lr_decay=False\n",
    "use_gae=False\n",
    "gae_lambda=0.95\n",
    "use_proper_time_limits=False\n",
    "obs_shape=obs_shape\n",
    "latent_dim=latent_dim\n",
    "recurrent_policy=False\n",
    "hidden_size=32\n",
    "use_elu=True\n",
    "latent_dim = 1\n",
    "\n",
    "agent = FixedIDAgent(action_space=action_space, device=device, gamma=gamma,\n",
    "                             num_steps=num_steps, num_processes=num_processes,\n",
    "                             clip_param=clip_param, ppo_epoch=ppo_epoch,\n",
    "                             num_mini_batch=num_mini_batch,\n",
    "                             value_loss_coef=value_loss_coef,\n",
    "                             entropy_coef=entropy_coef,\n",
    "                             lr=lr,\n",
    "                             eps=eps, max_grad_norm=max_grad_norm,\n",
    "                             use_linear_lr_decay=use_linear_lr_decay,\n",
    "                             use_gae=use_gae,\n",
    "                             gae_lambda=gae_lambda,\n",
    "                             use_proper_time_limits=use_proper_time_limits,\n",
    "                             obs_shape_opt=(1,),\n",
    "                             latent_dim=latent_dim,\n",
    "                             recurrent_policy=recurrent_policy,\n",
    "                             hidden_size=hidden_size,\n",
    "                             use_elu=use_elu,\n",
    "                             variational_model=vi,\n",
    "                             vae_optim=vi_optim,\n",
    "                             rescale_obs=True,\n",
    "                             max_old=max_old,\n",
    "                             min_old=min_old,\n",
    "                             vae_min_seq=vae_min_seq,\n",
    "                             vae_max_seq=vae_max_seq,\n",
    "                             max_action=100,\n",
    "                             min_action=-100,\n",
    "                             obs_shape_id=(4,),\n",
    "                             hidden_size_id=32,\n",
    "                             use_elu_id=True,\n",
    "                             clip_param_id=0.2,\n",
    "                             ppo_epoch_id=4,\n",
    "                             value_loss_coef_id=0.5,\n",
    "                             lr_id=0.00001,\n",
    "                             eps_id=1e-6,\n",
    "                             max_grad_norm_id=0.5,\n",
    "                             num_steps_id=30,\n",
    "                             gamma_identification=0.99,\n",
    "                             recurrent_policy_id=False,\n",
    "                             num_mini_batch_id=8,\n",
    "                             entropy_coef_id=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE 505.9447937011719 KLD 63.848793029785156\n",
      "Epoch 100 MSE 91.19701385498047 KLD 11.663049697875977\n",
      "Epoch 200 MSE 22.800859451293945 KLD 1.6729371547698975\n",
      "Epoch 300 MSE 39.73860549926758 KLD 1.223404884338379\n",
      "Epoch 400 MSE 49.04765319824219 KLD 4.834357261657715\n",
      "Epoch 500 MSE 38.799190521240234 KLD 0.9696248173713684\n",
      "Epoch 600 MSE 37.86405944824219 KLD 1.1136504411697388\n",
      "Epoch 700 MSE 50.58119583129883 KLD 4.2966132164001465\n",
      "Epoch 800 MSE 41.06340026855469 KLD 1.4738373756408691\n",
      "Epoch 900 MSE 23.370546340942383 KLD 2.522120475769043\n",
      "Epoch 1000 MSE 17.31281089782715 KLD 1.235701084136963\n",
      "Epoch 1100 MSE 15.509777069091797 KLD 1.4792022705078125\n",
      "Epoch 1200 MSE 12.995144844055176 KLD 1.9265873432159424\n",
      "Epoch 1300 MSE 21.683015823364258 KLD 2.304891586303711\n",
      "Epoch 1400 MSE 10.304110527038574 KLD 2.078232526779175\n",
      "Epoch 1500 MSE 21.650434494018555 KLD 2.1016640663146973\n",
      "Epoch 1600 MSE 10.708993911743164 KLD 2.4135842323303223\n",
      "Epoch 1700 MSE 15.273523330688477 KLD 2.5189425945281982\n",
      "Epoch 1800 MSE 10.658411979675293 KLD 2.2201168537139893\n",
      "Epoch 1900 MSE 7.565423011779785 KLD 2.754187822341919\n",
      "Epoch 2000 MSE 7.282788276672363 KLD 2.388817071914673\n",
      "Epoch 2100 MSE 6.897698879241943 KLD 1.9194673299789429\n",
      "Epoch 2200 MSE 5.965217113494873 KLD 2.5667874813079834\n",
      "Epoch 2300 MSE 3.673299789428711 KLD 2.6417441368103027\n",
      "Epoch 2400 MSE 9.44402027130127 KLD 2.7273716926574707\n",
      "Epoch 2500 MSE 5.957530975341797 KLD 2.3358592987060547\n",
      "Epoch 2600 MSE 3.2674307823181152 KLD 3.056546449661255\n",
      "Epoch 2700 MSE 3.0887033939361572 KLD 1.7115296125411987\n",
      "Epoch 2800 MSE 5.577410697937012 KLD 1.348982572555542\n",
      "Epoch 2900 MSE 4.668912887573242 KLD 2.6253089904785156\n",
      "Epoch 3000 MSE 31.1258544921875 KLD 2.6817262172698975\n",
      "Epoch 3100 MSE 7.617189407348633 KLD 4.052412033081055\n",
      "Epoch 3200 MSE 5.049114227294922 KLD 3.334040880203247\n",
      "Epoch 3300 MSE 2.235260486602783 KLD 2.458787679672241\n",
      "Epoch 3400 MSE 4.985236644744873 KLD 2.378329038619995\n",
      "Epoch 3500 MSE 2.5623409748077393 KLD 2.188321590423584\n",
      "Epoch 3600 MSE 12.831103324890137 KLD 2.428805351257324\n",
      "Epoch 3700 MSE 2.7093539237976074 KLD 2.8412721157073975\n",
      "Epoch 3800 MSE 2.484466075897217 KLD 3.0687057971954346\n",
      "Epoch 3900 MSE 4.53235387802124 KLD 4.138710975646973\n",
      "Epoch 4000 MSE 2.035154104232788 KLD 2.865978956222534\n",
      "Epoch 4100 MSE 3.8714981079101562 KLD 3.898512840270996\n",
      "Epoch 4200 MSE 1.7859236001968384 KLD 1.943009376525879\n",
      "Epoch 4300 MSE 1.9925469160079956 KLD 2.856504440307617\n",
      "Epoch 4400 MSE 8.411199569702148 KLD 2.4989163875579834\n",
      "Epoch 4500 MSE 2.408055067062378 KLD 2.991323471069336\n",
      "Epoch 4600 MSE 3.9806065559387207 KLD 4.3338422775268555\n",
      "Epoch 4700 MSE 1.1852999925613403 KLD 2.051029920578003\n",
      "Epoch 4800 MSE 2.3443243503570557 KLD 3.8270554542541504\n",
      "Epoch 4900 MSE 2.6909592151641846 KLD 3.151242256164551\n",
      "Epoch 5000 MSE 1.1942954063415527 KLD 3.9131247997283936\n",
      "Epoch 5100 MSE 1.6735261678695679 KLD 2.2022695541381836\n",
      "Epoch 5200 MSE 3.7991485595703125 KLD 2.7057390213012695\n",
      "Epoch 5300 MSE 6.425454139709473 KLD 2.4291765689849854\n",
      "Epoch 5400 MSE 2.9568042755126953 KLD 2.9324116706848145\n",
      "Epoch 5500 MSE 2.1505677700042725 KLD 3.9290144443511963\n",
      "Epoch 5600 MSE 1.5807929039001465 KLD 3.0972647666931152\n",
      "Epoch 5700 MSE 1.4188673496246338 KLD 1.7436277866363525\n",
      "Epoch 5800 MSE 1.6738018989562988 KLD 4.1088056564331055\n",
      "Epoch 5900 MSE 2.3635995388031006 KLD 2.807706832885742\n",
      "Epoch 6000 MSE 1.542379379272461 KLD 3.102799892425537\n",
      "Epoch 6100 MSE 1.68729829788208 KLD 3.1935577392578125\n",
      "Epoch 6200 MSE 2.8286402225494385 KLD 2.799729108810425\n",
      "Epoch 6300 MSE 1.720845103263855 KLD 2.4736716747283936\n",
      "Epoch 6400 MSE 1.5392554998397827 KLD 1.9549511671066284\n",
      "Epoch 6500 MSE 1.0194413661956787 KLD 3.22678804397583\n",
      "Epoch 6600 MSE 1.2967795133590698 KLD 1.9425239562988281\n",
      "Epoch 6700 MSE 3.0184340476989746 KLD 3.8398263454437256\n",
      "Epoch 6800 MSE 1.5348987579345703 KLD 3.8742942810058594\n",
      "Epoch 6900 MSE 16.620361328125 KLD 1.7813153266906738\n"
     ]
    }
   ],
   "source": [
    "agent.train_vae(7000, task_generator, True)\n",
    "torch.save(agent.train_vae, \"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 15.85742825\n",
      "Epoch 20 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 15.2663169375\n",
      "Epoch 40 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.6075385\n",
      "Epoch 60 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.88143625\n",
      "Epoch 80 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 18.105459375000002\n",
      "Epoch 100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 17.196218625\n",
      "Epoch 120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 18.89067834375\n",
      "Epoch 140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 19.34236159375\n",
      "Epoch 160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 20.02747409375\n",
      "Epoch 180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 18.7834175625\n",
      "Epoch 200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 20.408661062500002\n",
      "Epoch 220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 21.064300437500002\n",
      "Epoch 240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 20.33025240625\n",
      "Epoch 260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 20.7981139375\n",
      "Epoch 280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 21.792264125\n",
      "Epoch 300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 21.06579071875\n",
      "Epoch 320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 22.73460359375\n",
      "Epoch 340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 25.37095765625\n",
      "Epoch 360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 24.49803465625\n",
      "Epoch 380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 25.7976226875\n",
      "Epoch 400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 25.797629\n",
      "Epoch 420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 26.985466125000002\n",
      "Epoch 440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 28.19723096875\n",
      "Epoch 460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 27.66614921875\n",
      "Epoch 480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 29.55268965625\n",
      "Epoch 500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 29.9841326875\n",
      "Epoch 520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 31.167515218749998\n",
      "Epoch 540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 31.670477437499997\n",
      "Epoch 560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 31.3620756875\n",
      "Epoch 580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.83936875\n",
      "Epoch 600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.961467187500006\n",
      "Epoch 620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.17268840625\n",
      "Epoch 640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 34.738279375000005\n",
      "Epoch 660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 36.271241968750005\n",
      "Epoch 680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 35.258753625\n",
      "Epoch 700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.4011225\n",
      "Epoch 720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.08813153125\n",
      "Epoch 740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.164402249999995\n",
      "Epoch 760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 40.095634187499996\n",
      "Epoch 780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 43.449669687500005\n",
      "Epoch 800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 44.056705031250004\n",
      "Epoch 820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.9118420625\n",
      "Epoch 840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 44.76706996874999\n",
      "Epoch 860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.649410499999995\n",
      "Epoch 880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 48.557062906249996\n",
      "Epoch 900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 49.17857328125\n",
      "Epoch 920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 51.191319218749996\n",
      "Epoch 940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 51.41115525\n",
      "Epoch 960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 53.438670062499995\n",
      "Epoch 980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 53.66592471875\n",
      "Epoch 1000 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 55.573067125\n",
      "Epoch 1020 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 57.46247365625\n",
      "Epoch 1040 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 58.0097449375\n",
      "Epoch 1060 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 60.36770196875\n",
      "Epoch 1080 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 61.25168665625\n",
      "Epoch 1100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 63.30270884375\n",
      "Epoch 1120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 62.71790596875\n",
      "Epoch 1140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 65.89108643750001\n",
      "Epoch 1160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 67.82082815625\n",
      "Epoch 1180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 67.85869971874999\n",
      "Epoch 1200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 71.05149678125\n",
      "Epoch 1220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 71.5486295\n",
      "Epoch 1240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 73.33212209375\n",
      "Epoch 1260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 76.147983625\n",
      "Epoch 1280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 76.26721046875\n",
      "Epoch 1300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 78.575128125\n",
      "Epoch 1320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 79.58837975\n",
      "Epoch 1340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 80.49184125\n",
      "Epoch 1360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 80.61675809375001\n",
      "Epoch 1380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 83.4142660625\n",
      "Epoch 1400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 84.88951225\n",
      "Epoch 1420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 85.82799700000001\n",
      "Epoch 1440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 88.18409249999999\n",
      "Epoch 1460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 89.61423562499999\n",
      "Epoch 1480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 87.20020690625\n",
      "Epoch 1500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 92.5115515\n",
      "Epoch 1520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 94.60379409375001\n",
      "Epoch 1540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 95.66052937500001\n",
      "Epoch 1560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 95.23290315625\n",
      "Epoch 1580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 98.38116456249999\n",
      "Epoch 1600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 99.66485374999999\n",
      "Epoch 1620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 100.77067240624999\n",
      "Epoch 1640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 100.87962909375\n",
      "Epoch 1660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 103.15173659375\n",
      "Epoch 1680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 103.89436950000001\n",
      "Epoch 1700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 105.4589963125\n",
      "Epoch 1720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 106.0087516875\n",
      "Epoch 1740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 107.17795271874999\n",
      "Epoch 1760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 107.87385925\n",
      "Epoch 1780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 107.701442625\n",
      "Epoch 1800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 109.26426965625001\n",
      "Epoch 1820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 111.49767390625\n",
      "Epoch 1840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 111.848748875\n",
      "Epoch 1860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 111.6621955\n",
      "Epoch 1880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 112.70061321875\n",
      "Epoch 1900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 112.66110540625\n",
      "Epoch 1920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 113.93310890625\n",
      "Epoch 1940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 116.09224040625\n",
      "Epoch 1960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 114.126729875\n",
      "Epoch 1980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 115.74433896875\n",
      "Epoch 2000 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 118.89277462499999\n",
      "Epoch 2020 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 117.53685684375\n",
      "Epoch 2040 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 119.4476249375\n",
      "Epoch 2060 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 120.85304887500001\n",
      "Epoch 2080 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 121.796036375\n",
      "Epoch 2100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 119.6312136875\n",
      "Epoch 2120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 122.97980384375\n",
      "Epoch 2140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 123.655081625\n",
      "Epoch 2160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 124.436419\n",
      "Epoch 2180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 124.38910509375\n",
      "Epoch 2200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 124.31640753125001\n",
      "Epoch 2220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 123.730573125\n",
      "Epoch 2240 / 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation using 32 tasks. Mean reward: 125.95359590625\n",
      "Epoch 2260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 126.7635501875\n",
      "Epoch 2280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 125.0654635625\n",
      "Epoch 2300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 126.00708415625\n",
      "Epoch 2320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 127.21660659375002\n",
      "Epoch 2340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 126.42244734375\n",
      "Epoch 2360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 128.1332628125\n",
      "Epoch 2380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 127.81961025\n",
      "Epoch 2400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 126.60561834375\n",
      "Epoch 2420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 129.17569459375\n",
      "Epoch 2440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 129.3864028125\n",
      "Epoch 2460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 128.03714628125\n",
      "Epoch 2480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 129.15206534375\n",
      "Epoch 2500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 129.36990628125\n",
      "Epoch 2520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 129.5277191875\n",
      "Epoch 2540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 127.8102520625\n",
      "Epoch 2560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 128.765104875\n",
      "Epoch 2580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 131.580982125\n",
      "Epoch 2600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 132.11699699999997\n",
      "Epoch 2620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 132.56164246875\n",
      "Epoch 2640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 127.96471425\n",
      "Epoch 2660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 130.5693914375\n",
      "Epoch 2680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 132.40915215625\n",
      "Epoch 2700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 132.5396938125\n",
      "Epoch 2720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 131.81517275000002\n",
      "Epoch 2740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 134.02930928125\n",
      "Epoch 2760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 134.03682696875\n",
      "Epoch 2780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 131.8566190625\n",
      "Epoch 2800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 134.5403125625\n",
      "Epoch 2820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 133.60509346875\n",
      "Epoch 2840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 126.8613308125\n",
      "Epoch 2860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.67656503125\n",
      "Epoch 2880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 133.93014284375\n",
      "Epoch 2900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 134.77486559375\n",
      "Epoch 2920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.55693540624998\n",
      "Epoch 2940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.85962178124998\n",
      "Epoch 2960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 136.71366749999999\n",
      "Epoch 2980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.78217206250002\n",
      "Epoch 3000 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 136.82911759375\n",
      "Epoch 3020 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.48406762500002\n",
      "Epoch 3040 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 136.276658\n",
      "Epoch 3060 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.4688845\n",
      "Epoch 3080 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.69796759374998\n",
      "Epoch 3100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 137.26494325\n",
      "Epoch 3120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 137.76221015624998\n",
      "Epoch 3140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 137.167050125\n",
      "Epoch 3160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 137.2563945\n",
      "Epoch 3180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 137.60093606249998\n",
      "Epoch 3200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.559515\n",
      "Epoch 3220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.48738909374998\n",
      "Epoch 3240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.28352506250002\n",
      "Epoch 3260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.8832774375\n",
      "Epoch 3280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.80822293749998\n",
      "Epoch 3300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.2387665\n",
      "Epoch 3320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 136.34527590624998\n",
      "Epoch 3340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.5763931875\n",
      "Epoch 3360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.27731240625002\n",
      "Epoch 3380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.24708487499998\n",
      "Epoch 3400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 134.68655456250002\n",
      "Epoch 3420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 136.35506840624998\n",
      "Epoch 3440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.38712240625\n",
      "Epoch 3460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 140.30732815625\n",
      "Epoch 3480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 140.29984371875003\n",
      "Epoch 3500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.64548440625\n",
      "Epoch 3520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.33678753125\n",
      "Epoch 3540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.1333470625\n",
      "Epoch 3560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 136.69504071875002\n",
      "Epoch 3580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 140.49816718749997\n",
      "Epoch 3600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 140.28525684375\n",
      "Epoch 3620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 137.41747806249998\n",
      "Epoch 3640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 137.54944553124997\n",
      "Epoch 3660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 132.63511640625\n",
      "Epoch 3680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 135.78480337500002\n",
      "Epoch 3700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 140.2431928125\n",
      "Epoch 3720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.18518809374999\n",
      "Epoch 3740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.20650412499998\n",
      "Epoch 3760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 138.216147875\n",
      "Epoch 3780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.81992568750002\n",
      "Epoch 3800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 140.9874806875\n",
      "Epoch 3820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.72758503125\n",
      "Epoch 3840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.62838687500002\n",
      "Epoch 3860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.0931831875\n",
      "Epoch 3880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.8060405\n",
      "Epoch 3900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.05330487499998\n",
      "Epoch 3920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.55160553125\n",
      "Epoch 3940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.76815453125\n",
      "Epoch 3960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.12247390624998\n",
      "Epoch 3980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.79408040625\n",
      "Epoch 4000 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.60466659375\n",
      "Epoch 4020 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.12139424999998\n",
      "Epoch 4040 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.08011840625\n",
      "Epoch 4060 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.07540959375\n",
      "Epoch 4080 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.17875728125\n",
      "Epoch 4100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.86541021875\n",
      "Epoch 4120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.82250684374998\n",
      "Epoch 4140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.93185421875\n",
      "Epoch 4160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 139.518200375\n",
      "Epoch 4180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.41607628124999\n",
      "Epoch 4200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.6215190625\n",
      "Epoch 4220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.826292625\n",
      "Epoch 4240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.48104834375\n",
      "Epoch 4260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.72270337499998\n",
      "Epoch 4280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.2726593125\n",
      "Epoch 4300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.09629275\n",
      "Epoch 4320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.03167384375\n",
      "Epoch 4340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.251168\n",
      "Epoch 4360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.41070821875002\n",
      "Epoch 4380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.83680515625002\n",
      "Epoch 4400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.23524471874998\n",
      "Epoch 4420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.503350625\n",
      "Epoch 4440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.4165295625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.41395096875002\n",
      "Epoch 4480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.5865653125\n",
      "Epoch 4500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.3712723125\n",
      "Epoch 4520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.72357096874998\n",
      "Epoch 4540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.30921559375\n",
      "Epoch 4560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.79323931250002\n",
      "Epoch 4580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 144.19182390625\n",
      "Epoch 4600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.3671963125\n",
      "Epoch 4620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.34815887499997\n",
      "Epoch 4640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.25030296875002\n",
      "Epoch 4660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.0680465625\n",
      "Epoch 4680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.64406574999998\n",
      "Epoch 4700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.26209025\n",
      "Epoch 4720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.36790159375\n",
      "Epoch 4740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.3872820625\n",
      "Epoch 4760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.65544571875\n",
      "Epoch 4780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.67598403124998\n",
      "Epoch 4800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.59449625\n",
      "Epoch 4820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.76407009375\n",
      "Epoch 4840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.66690868749998\n",
      "Epoch 4860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 145.82859353125\n",
      "Epoch 4880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.321673375\n",
      "Epoch 4900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 143.21207203125\n",
      "Epoch 4920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 142.4212018125\n",
      "Epoch 4940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 140.24265596875\n",
      "Epoch 4960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.7339400625\n",
      "Epoch 4980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 141.2055748125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.85742825,\n",
       " 15.2663169375,\n",
       " 16.6075385,\n",
       " 16.88143625,\n",
       " 18.105459375000002,\n",
       " 17.196218625,\n",
       " 18.89067834375,\n",
       " 19.34236159375,\n",
       " 20.02747409375,\n",
       " 18.7834175625,\n",
       " 20.408661062500002,\n",
       " 21.064300437500002,\n",
       " 20.33025240625,\n",
       " 20.7981139375,\n",
       " 21.792264125,\n",
       " 21.06579071875,\n",
       " 22.73460359375,\n",
       " 25.37095765625,\n",
       " 24.49803465625,\n",
       " 25.7976226875,\n",
       " 25.797629,\n",
       " 26.985466125000002,\n",
       " 28.19723096875,\n",
       " 27.66614921875,\n",
       " 29.55268965625,\n",
       " 29.9841326875,\n",
       " 31.167515218749998,\n",
       " 31.670477437499997,\n",
       " 31.3620756875,\n",
       " 33.83936875,\n",
       " 33.961467187500006,\n",
       " 33.17268840625,\n",
       " 34.738279375000005,\n",
       " 36.271241968750005,\n",
       " 35.258753625,\n",
       " 38.4011225,\n",
       " 39.08813153125,\n",
       " 39.164402249999995,\n",
       " 40.095634187499996,\n",
       " 43.449669687500005,\n",
       " 44.056705031250004,\n",
       " 42.9118420625,\n",
       " 44.76706996874999,\n",
       " 46.649410499999995,\n",
       " 48.557062906249996,\n",
       " 49.17857328125,\n",
       " 51.191319218749996,\n",
       " 51.41115525,\n",
       " 53.438670062499995,\n",
       " 53.66592471875,\n",
       " 55.573067125,\n",
       " 57.46247365625,\n",
       " 58.0097449375,\n",
       " 60.36770196875,\n",
       " 61.25168665625,\n",
       " 63.30270884375,\n",
       " 62.71790596875,\n",
       " 65.89108643750001,\n",
       " 67.82082815625,\n",
       " 67.85869971874999,\n",
       " 71.05149678125,\n",
       " 71.5486295,\n",
       " 73.33212209375,\n",
       " 76.147983625,\n",
       " 76.26721046875,\n",
       " 78.575128125,\n",
       " 79.58837975,\n",
       " 80.49184125,\n",
       " 80.61675809375001,\n",
       " 83.4142660625,\n",
       " 84.88951225,\n",
       " 85.82799700000001,\n",
       " 88.18409249999999,\n",
       " 89.61423562499999,\n",
       " 87.20020690625,\n",
       " 92.5115515,\n",
       " 94.60379409375001,\n",
       " 95.66052937500001,\n",
       " 95.23290315625,\n",
       " 98.38116456249999,\n",
       " 99.66485374999999,\n",
       " 100.77067240624999,\n",
       " 100.87962909375,\n",
       " 103.15173659375,\n",
       " 103.89436950000001,\n",
       " 105.4589963125,\n",
       " 106.0087516875,\n",
       " 107.17795271874999,\n",
       " 107.87385925,\n",
       " 107.701442625,\n",
       " 109.26426965625001,\n",
       " 111.49767390625,\n",
       " 111.848748875,\n",
       " 111.6621955,\n",
       " 112.70061321875,\n",
       " 112.66110540625,\n",
       " 113.93310890625,\n",
       " 116.09224040625,\n",
       " 114.126729875,\n",
       " 115.74433896875,\n",
       " 118.89277462499999,\n",
       " 117.53685684375,\n",
       " 119.4476249375,\n",
       " 120.85304887500001,\n",
       " 121.796036375,\n",
       " 119.6312136875,\n",
       " 122.97980384375,\n",
       " 123.655081625,\n",
       " 124.436419,\n",
       " 124.38910509375,\n",
       " 124.31640753125001,\n",
       " 123.730573125,\n",
       " 125.95359590625,\n",
       " 126.7635501875,\n",
       " 125.0654635625,\n",
       " 126.00708415625,\n",
       " 127.21660659375002,\n",
       " 126.42244734375,\n",
       " 128.1332628125,\n",
       " 127.81961025,\n",
       " 126.60561834375,\n",
       " 129.17569459375,\n",
       " 129.3864028125,\n",
       " 128.03714628125,\n",
       " 129.15206534375,\n",
       " 129.36990628125,\n",
       " 129.5277191875,\n",
       " 127.8102520625,\n",
       " 128.765104875,\n",
       " 131.580982125,\n",
       " 132.11699699999997,\n",
       " 132.56164246875,\n",
       " 127.96471425,\n",
       " 130.5693914375,\n",
       " 132.40915215625,\n",
       " 132.5396938125,\n",
       " 131.81517275000002,\n",
       " 134.02930928125,\n",
       " 134.03682696875,\n",
       " 131.8566190625,\n",
       " 134.5403125625,\n",
       " 133.60509346875,\n",
       " 126.8613308125,\n",
       " 135.67656503125,\n",
       " 133.93014284375,\n",
       " 134.77486559375,\n",
       " 135.55693540624998,\n",
       " 135.85962178124998,\n",
       " 136.71366749999999,\n",
       " 135.78217206250002,\n",
       " 136.82911759375,\n",
       " 135.48406762500002,\n",
       " 136.276658,\n",
       " 135.4688845,\n",
       " 135.69796759374998,\n",
       " 137.26494325,\n",
       " 137.76221015624998,\n",
       " 137.167050125,\n",
       " 137.2563945,\n",
       " 137.60093606249998,\n",
       " 138.559515,\n",
       " 138.48738909374998,\n",
       " 139.28352506250002,\n",
       " 138.8832774375,\n",
       " 138.80822293749998,\n",
       " 138.2387665,\n",
       " 136.34527590624998,\n",
       " 138.5763931875,\n",
       " 139.27731240625002,\n",
       " 138.24708487499998,\n",
       " 134.68655456250002,\n",
       " 136.35506840624998,\n",
       " 139.38712240625,\n",
       " 140.30732815625,\n",
       " 140.29984371875003,\n",
       " 139.64548440625,\n",
       " 139.33678753125,\n",
       " 138.1333470625,\n",
       " 136.69504071875002,\n",
       " 140.49816718749997,\n",
       " 140.28525684375,\n",
       " 137.41747806249998,\n",
       " 137.54944553124997,\n",
       " 132.63511640625,\n",
       " 135.78480337500002,\n",
       " 140.2431928125,\n",
       " 139.18518809374999,\n",
       " 139.20650412499998,\n",
       " 138.216147875,\n",
       " 139.81992568750002,\n",
       " 140.9874806875,\n",
       " 142.72758503125,\n",
       " 141.62838687500002,\n",
       " 142.0931831875,\n",
       " 141.8060405,\n",
       " 141.05330487499998,\n",
       " 142.55160553125,\n",
       " 141.76815453125,\n",
       " 142.12247390624998,\n",
       " 142.79408040625,\n",
       " 142.60466659375,\n",
       " 142.12139424999998,\n",
       " 143.08011840625,\n",
       " 142.07540959375,\n",
       " 143.17875728125,\n",
       " 143.86541021875,\n",
       " 143.82250684374998,\n",
       " 141.93185421875,\n",
       " 139.518200375,\n",
       " 141.41607628124999,\n",
       " 143.6215190625,\n",
       " 143.826292625,\n",
       " 143.48104834375,\n",
       " 143.72270337499998,\n",
       " 144.2726593125,\n",
       " 144.09629275,\n",
       " 144.03167384375,\n",
       " 144.251168,\n",
       " 144.41070821875002,\n",
       " 143.83680515625002,\n",
       " 144.23524471874998,\n",
       " 143.503350625,\n",
       " 143.4165295625,\n",
       " 143.41395096875002,\n",
       " 144.5865653125,\n",
       " 144.3712723125,\n",
       " 144.72357096874998,\n",
       " 145.30921559375,\n",
       " 143.79323931250002,\n",
       " 144.19182390625,\n",
       " 145.3671963125,\n",
       " 145.34815887499997,\n",
       " 145.25030296875002,\n",
       " 145.0680465625,\n",
       " 145.64406574999998,\n",
       " 145.26209025,\n",
       " 145.36790159375,\n",
       " 145.3872820625,\n",
       " 145.65544571875,\n",
       " 145.67598403124998,\n",
       " 145.59449625,\n",
       " 145.76407009375,\n",
       " 145.66690868749998,\n",
       " 145.82859353125,\n",
       " 143.321673375,\n",
       " 143.21207203125,\n",
       " 142.4212018125,\n",
       " 140.24265596875,\n",
       " 141.7339400625,\n",
       " 141.2055748125]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_list, test_kwargs, init_prior_test = get_task_sequence(alpha=0.25,\n",
    "                                                          n_restarts=1,\n",
    "                                                          num_test_processes=2)\n",
    "\n",
    "agent.train_optimal(5000, task_generator, 'gauss-v0', 0, \".\", False, 20,\n",
    "                      32, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation using 32 tasks. Mean reward: -2938.032958984375. Mean MSE: 63.76 || 40.28 || 23.60 || 8.03\n",
      "Meta-testing...\n",
      "Reward : 5.46775397\n",
      "Evaluation using 32 tasks. Mean reward: -1317.0181884765625. Mean MSE: 39.32 || 13.84 || 6.61 || 3.94\n",
      "Meta-testing...\n",
      "Reward : 5.0634320399999995\n",
      "Evaluation using 32 tasks. Mean reward: -2178.77099609375. Mean MSE: 53.12 || 28.27 || 4.87 || 6.50\n",
      "Meta-testing...\n",
      "Reward : 5.1183856\n",
      "Evaluation using 32 tasks. Mean reward: -1423.07861328125. Mean MSE: 32.85 || 17.21 || 8.95 || 7.08\n",
      "Meta-testing...\n",
      "Reward : 4.899368739999999\n",
      "Evaluation using 32 tasks. Mean reward: -1817.9688720703125. Mean MSE: 53.99 || 39.91 || 7.67 || 8.67\n",
      "Meta-testing...\n",
      "Reward : 5.30527358\n",
      "Evaluation using 32 tasks. Mean reward: -1537.9530029296875. Mean MSE: 36.43 || 30.78 || 4.67 || 13.73\n",
      "Meta-testing...\n",
      "Reward : 5.2344939099999985\n",
      "Evaluation using 32 tasks. Mean reward: -811.0360717773438. Mean MSE: 30.66 || 7.56 || 3.64 || 4.07\n",
      "Meta-testing...\n",
      "Reward : 5.34720819\n",
      "Evaluation using 32 tasks. Mean reward: -969.4575805664062. Mean MSE: 27.31 || 11.92 || 5.41 || 4.19\n",
      "Meta-testing...\n",
      "Reward : 5.2820875\n",
      "Evaluation using 32 tasks. Mean reward: -1142.3779296875. Mean MSE: 40.80 || 19.22 || 4.44 || 5.19\n",
      "Meta-testing...\n",
      "Reward : 5.23068941\n",
      "Evaluation using 32 tasks. Mean reward: -1225.8974609375. Mean MSE: 28.91 || 14.99 || 4.48 || 4.69\n",
      "Meta-testing...\n",
      "Reward : 5.09303567\n",
      "Evaluation using 32 tasks. Mean reward: -1132.2901611328125. Mean MSE: 27.88 || 19.22 || 3.06 || 6.19\n",
      "Meta-testing...\n",
      "Reward : 5.3461588099999995\n",
      "Evaluation using 32 tasks. Mean reward: -1079.1334228515625. Mean MSE: 25.12 || 13.76 || 5.67 || 7.46\n",
      "Meta-testing...\n",
      "Reward : 5.40391025\n",
      "Evaluation using 32 tasks. Mean reward: -837.5829467773438. Mean MSE: 23.07 || 10.91 || 4.95 || 3.61\n",
      "Meta-testing...\n",
      "Reward : 5.353037649999999\n",
      "Evaluation using 32 tasks. Mean reward: -1369.3330078125. Mean MSE: 38.94 || 22.11 || 5.35 || 8.61\n",
      "Meta-testing...\n",
      "Reward : 5.54105094\n",
      "Evaluation using 32 tasks. Mean reward: -1326.935546875. Mean MSE: 36.42 || 14.91 || 3.58 || 7.40\n",
      "Meta-testing...\n",
      "Reward : 5.148900319999999\n",
      "Evaluation using 32 tasks. Mean reward: -862.6154174804688. Mean MSE: 12.73 || 8.85 || 5.67 || 4.77\n",
      "Meta-testing...\n",
      "Reward : 5.515015220000001\n",
      "Evaluation using 32 tasks. Mean reward: -777.9337158203125. Mean MSE: 30.01 || 10.52 || 2.75 || 4.50\n",
      "Meta-testing...\n",
      "Reward : 5.2471132\n",
      "Evaluation using 32 tasks. Mean reward: -1467.27978515625. Mean MSE: 48.83 || 26.53 || 8.39 || 7.52\n",
      "Meta-testing...\n",
      "Reward : 5.535969869999999\n",
      "Evaluation using 32 tasks. Mean reward: -2682.970703125. Mean MSE: 69.27 || 32.30 || 19.38 || 11.26\n",
      "Meta-testing...\n",
      "Reward : 5.131209340000001\n",
      "Evaluation using 32 tasks. Mean reward: -1635.88427734375. Mean MSE: 50.41 || 31.29 || 8.14 || 8.89\n",
      "Meta-testing...\n",
      "Reward : 5.86535591\n",
      "Evaluation using 32 tasks. Mean reward: -908.20361328125. Mean MSE: 40.63 || 8.37 || 2.66 || 4.31\n",
      "Meta-testing...\n",
      "Reward : 5.80483442\n",
      "Evaluation using 32 tasks. Mean reward: -1839.3699951171875. Mean MSE: 31.70 || 29.37 || 11.13 || 6.86\n",
      "Meta-testing...\n",
      "Reward : 5.14650446\n",
      "Evaluation using 32 tasks. Mean reward: -1295.3048095703125. Mean MSE: 28.96 || 15.97 || 6.62 || 3.88\n",
      "Meta-testing...\n",
      "Reward : 5.43341825\n",
      "Evaluation using 32 tasks. Mean reward: -1971.429443359375. Mean MSE: 58.18 || 39.32 || 9.25 || 8.41\n",
      "Meta-testing...\n",
      "Reward : 5.33107859\n",
      "Evaluation using 32 tasks. Mean reward: -986.9891357421875. Mean MSE: 31.62 || 12.37 || 3.99 || 3.75\n",
      "Meta-testing...\n",
      "Reward : 5.441337859999999\n",
      "Evaluation using 32 tasks. Mean reward: -1173.4837646484375. Mean MSE: 30.13 || 15.43 || 5.36 || 3.93\n",
      "Meta-testing...\n",
      "Reward : 4.97721615\n",
      "Evaluation using 32 tasks. Mean reward: -930.4561767578125. Mean MSE: 22.90 || 13.98 || 1.85 || 2.74\n",
      "Meta-testing...\n",
      "Reward : 5.25067493\n",
      "Evaluation using 32 tasks. Mean reward: -1772.594970703125. Mean MSE: 44.13 || 20.03 || 8.22 || 8.52\n",
      "Meta-testing...\n",
      "Reward : 5.219229240000001\n",
      "Evaluation using 32 tasks. Mean reward: -842.1160888671875. Mean MSE: 18.32 || 10.29 || 4.90 || 3.28\n",
      "Meta-testing...\n",
      "Reward : 5.10952367\n",
      "Evaluation using 32 tasks. Mean reward: -917.4612426757812. Mean MSE: 22.55 || 11.11 || 3.30 || 3.10\n",
      "Meta-testing...\n",
      "Reward : 5.25722529\n",
      "Evaluation using 32 tasks. Mean reward: -1007.90673828125. Mean MSE: 32.95 || 13.71 || 3.24 || 4.52\n",
      "Meta-testing...\n",
      "Reward : 5.14252593\n",
      "Evaluation using 32 tasks. Mean reward: -1793.8206787109375. Mean MSE: 43.33 || 31.16 || 12.18 || 7.63\n",
      "Meta-testing...\n",
      "Reward : 5.36433865\n",
      "Evaluation using 32 tasks. Mean reward: -1048.61376953125. Mean MSE: 36.91 || 16.58 || 4.93 || 3.45\n",
      "Meta-testing...\n",
      "Reward : 5.217941339999999\n",
      "Evaluation using 32 tasks. Mean reward: -1513.5704345703125. Mean MSE: 55.59 || 19.89 || 7.17 || 5.76\n",
      "Meta-testing...\n",
      "Reward : 5.40072971\n",
      "Evaluation using 32 tasks. Mean reward: -1216.973876953125. Mean MSE: 35.39 || 14.31 || 6.66 || 7.34\n",
      "Meta-testing...\n",
      "Reward : 5.3709055999999995\n",
      "Evaluation using 32 tasks. Mean reward: -1768.5362548828125. Mean MSE: 46.64 || 21.31 || 6.80 || 7.22\n",
      "Meta-testing...\n",
      "Reward : 5.199901960000001\n",
      "Evaluation using 32 tasks. Mean reward: -1760.650634765625. Mean MSE: 40.50 || 25.25 || 7.45 || 6.94\n",
      "Meta-testing...\n",
      "Reward : 5.39724245\n",
      "Evaluation using 32 tasks. Mean reward: -1128.632080078125. Mean MSE: 44.56 || 11.24 || 6.85 || 4.27\n",
      "Meta-testing...\n",
      "Reward : 5.598289139999999\n",
      "Evaluation using 32 tasks. Mean reward: -1134.1484375. Mean MSE: 27.34 || 15.62 || 4.82 || 5.77\n",
      "Meta-testing...\n",
      "Reward : 5.2741439\n",
      "Evaluation using 32 tasks. Mean reward: -945.2201538085938. Mean MSE: 31.17 || 18.83 || 6.10 || 3.53\n",
      "Meta-testing...\n",
      "Reward : 5.192828679999999\n",
      "Evaluation using 32 tasks. Mean reward: -1204.972412109375. Mean MSE: 43.12 || 13.58 || 5.99 || 3.71\n",
      "Meta-testing...\n",
      "Reward : 5.63438263\n",
      "Evaluation using 32 tasks. Mean reward: -1472.661865234375. Mean MSE: 57.56 || 24.87 || 4.81 || 5.05\n",
      "Meta-testing...\n",
      "Reward : 5.32085583\n",
      "Evaluation using 32 tasks. Mean reward: -1343.4774169921875. Mean MSE: 38.26 || 21.38 || 5.28 || 5.02\n",
      "Meta-testing...\n",
      "Reward : 5.21732\n",
      "Evaluation using 32 tasks. Mean reward: -1008.412109375. Mean MSE: 24.46 || 12.73 || 4.27 || 6.48\n",
      "Meta-testing...\n",
      "Reward : 5.2018757\n",
      "Evaluation using 32 tasks. Mean reward: -1356.35888671875. Mean MSE: 39.90 || 17.80 || 8.82 || 6.04\n",
      "Meta-testing...\n",
      "Reward : 5.2035667\n",
      "Evaluation using 32 tasks. Mean reward: -1580.996337890625. Mean MSE: 62.51 || 32.45 || 5.91 || 5.74\n",
      "Meta-testing...\n",
      "Reward : 5.3956548699999995\n",
      "Evaluation using 32 tasks. Mean reward: -842.5951538085938. Mean MSE: 26.53 || 8.71 || 4.86 || 7.23\n",
      "Meta-testing...\n",
      "Reward : 5.609172460000001\n",
      "Evaluation using 32 tasks. Mean reward: -1388.7955322265625. Mean MSE: 44.19 || 14.48 || 3.38 || 5.98\n",
      "Meta-testing...\n",
      "Reward : 5.356259879999998\n",
      "Evaluation using 32 tasks. Mean reward: -1194.9698486328125. Mean MSE: 40.52 || 16.74 || 6.58 || 4.21\n",
      "Meta-testing...\n",
      "Reward : 5.259660349999999\n",
      "Evaluation using 32 tasks. Mean reward: -1323.2911376953125. Mean MSE: 47.08 || 32.92 || 2.88 || 3.82\n",
      "Meta-testing...\n",
      "Reward : 5.665191599999999\n",
      "Evaluation using 32 tasks. Mean reward: -1007.0445556640625. Mean MSE: 35.54 || 14.07 || 5.76 || 3.66\n",
      "Meta-testing...\n",
      "Reward : 5.790030530000001\n",
      "Evaluation using 32 tasks. Mean reward: -766.1758422851562. Mean MSE: 32.68 || 11.43 || 2.33 || 3.58\n",
      "Meta-testing...\n",
      "Reward : 5.237944480000001\n",
      "Evaluation using 32 tasks. Mean reward: -743.5238037109375. Mean MSE: 19.92 || 12.28 || 4.45 || 3.42\n",
      "Meta-testing...\n",
      "Reward : 5.415248399999999\n",
      "Evaluation using 32 tasks. Mean reward: -755.2978515625. Mean MSE: 27.03 || 14.82 || 3.48 || 3.47\n",
      "Meta-testing...\n",
      "Reward : 5.509609739999999\n",
      "Evaluation using 32 tasks. Mean reward: -1578.3740234375. Mean MSE: 47.91 || 30.37 || 4.69 || 8.42\n",
      "Meta-testing...\n",
      "Reward : 5.842228529999999\n",
      "Evaluation using 32 tasks. Mean reward: -785.9388427734375. Mean MSE: 36.50 || 8.00 || 3.18 || 5.51\n",
      "Meta-testing...\n",
      "Reward : 5.50712494\n",
      "Evaluation using 32 tasks. Mean reward: -1400.0743408203125. Mean MSE: 42.16 || 21.03 || 5.85 || 4.44\n",
      "Meta-testing...\n",
      "Reward : 5.816105680000001\n",
      "Evaluation using 32 tasks. Mean reward: -725.105224609375. Mean MSE: 27.00 || 12.33 || 3.75 || 2.53\n",
      "Meta-testing...\n",
      "Reward : 5.7643707299999996\n",
      "Evaluation using 32 tasks. Mean reward: -627.220947265625. Mean MSE: 25.19 || 9.73 || 1.49 || 3.55\n",
      "Meta-testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward : 5.816043759999999\n",
      "Evaluation using 32 tasks. Mean reward: -1708.2996826171875. Mean MSE: 59.25 || 27.63 || 7.95 || 6.04\n",
      "Meta-testing...\n",
      "Reward : 5.29866909\n",
      "Evaluation using 32 tasks. Mean reward: -800.1090087890625. Mean MSE: 26.67 || 14.75 || 3.86 || 1.69\n",
      "Meta-testing...\n",
      "Reward : 5.765225020000002\n",
      "Evaluation using 32 tasks. Mean reward: -1511.416748046875. Mean MSE: 48.26 || 19.68 || 4.98 || 7.87\n",
      "Meta-testing...\n",
      "Reward : 5.58387379\n",
      "Evaluation using 32 tasks. Mean reward: -1647.3538818359375. Mean MSE: 27.88 || 26.42 || 5.76 || 8.11\n",
      "Meta-testing...\n",
      "Reward : 5.73874951\n",
      "Evaluation using 32 tasks. Mean reward: -653.8416137695312. Mean MSE: 16.36 || 7.28 || 2.72 || 2.97\n",
      "Meta-testing...\n",
      "Reward : 5.729134709999999\n",
      "Evaluation using 32 tasks. Mean reward: -1155.74609375. Mean MSE: 27.47 || 15.51 || 6.59 || 3.36\n",
      "Meta-testing...\n",
      "Reward : 5.510876660000001\n",
      "Evaluation using 32 tasks. Mean reward: -1132.95751953125. Mean MSE: 42.78 || 14.07 || 4.38 || 3.64\n",
      "Meta-testing...\n",
      "Reward : 5.663533910000001\n",
      "Evaluation using 32 tasks. Mean reward: -842.7083740234375. Mean MSE: 22.29 || 11.19 || 4.30 || 2.71\n",
      "Meta-testing...\n",
      "Reward : 5.66113139\n",
      "Evaluation using 32 tasks. Mean reward: -1073.841796875. Mean MSE: 34.83 || 10.53 || 4.66 || 7.47\n",
      "Meta-testing...\n",
      "Reward : 5.3880177499999995\n",
      "Evaluation using 32 tasks. Mean reward: -618.038818359375. Mean MSE: 22.46 || 12.70 || 1.86 || 2.83\n",
      "Meta-testing...\n",
      "Reward : 5.79662554\n",
      "Evaluation using 32 tasks. Mean reward: -791.7738037109375. Mean MSE: 34.34 || 8.26 || 3.79 || 5.24\n",
      "Meta-testing...\n",
      "Reward : 5.917226949999999\n",
      "Evaluation using 32 tasks. Mean reward: -1261.8089599609375. Mean MSE: 44.78 || 14.61 || 5.17 || 6.68\n",
      "Meta-testing...\n",
      "Reward : 5.59156622\n",
      "Evaluation using 32 tasks. Mean reward: -1350.884521484375. Mean MSE: 48.08 || 14.53 || 7.63 || 4.58\n",
      "Meta-testing...\n",
      "Reward : 5.24301393\n",
      "Evaluation using 32 tasks. Mean reward: -1425.2337646484375. Mean MSE: 70.08 || 18.82 || 5.35 || 8.00\n",
      "Meta-testing...\n",
      "Reward : 5.8395488\n",
      "Evaluation using 32 tasks. Mean reward: -762.705322265625. Mean MSE: 31.67 || 9.32 || 2.42 || 3.86\n",
      "Meta-testing...\n",
      "Reward : 5.872096500000001\n",
      "Evaluation using 32 tasks. Mean reward: -901.2072143554688. Mean MSE: 25.36 || 17.16 || 3.27 || 3.92\n",
      "Meta-testing...\n",
      "Reward : 5.73751742\n",
      "Evaluation using 32 tasks. Mean reward: -1239.833740234375. Mean MSE: 44.13 || 21.82 || 5.71 || 3.39\n",
      "Meta-testing...\n",
      "Reward : 5.57098339\n",
      "Evaluation using 32 tasks. Mean reward: -856.0623168945312. Mean MSE: 28.95 || 13.04 || 2.72 || 3.88\n",
      "Meta-testing...\n",
      "Reward : 5.75297435\n",
      "Evaluation using 32 tasks. Mean reward: -1389.3111572265625. Mean MSE: 59.46 || 19.44 || 5.03 || 6.06\n",
      "Meta-testing...\n",
      "Reward : 6.04834062\n",
      "Evaluation using 32 tasks. Mean reward: -1442.5848388671875. Mean MSE: 49.94 || 19.31 || 9.68 || 7.37\n",
      "Meta-testing...\n",
      "Reward : 5.6340904499999995\n",
      "Evaluation using 32 tasks. Mean reward: -883.032958984375. Mean MSE: 32.41 || 7.63 || 2.90 || 4.17\n",
      "Meta-testing...\n",
      "Reward : 5.604283789999999\n",
      "Evaluation using 32 tasks. Mean reward: -859.815673828125. Mean MSE: 35.30 || 14.46 || 2.69 || 3.97\n",
      "Meta-testing...\n",
      "Reward : 5.63318213\n",
      "Evaluation using 32 tasks. Mean reward: -1033.7293701171875. Mean MSE: 24.41 || 18.35 || 2.85 || 3.52\n",
      "Meta-testing...\n",
      "Reward : 5.4292693299999994\n",
      "Evaluation using 32 tasks. Mean reward: -1398.4345703125. Mean MSE: 41.15 || 25.18 || 5.73 || 4.53\n",
      "Meta-testing...\n",
      "Reward : 5.839065509999999\n",
      "Evaluation using 32 tasks. Mean reward: -782.9046630859375. Mean MSE: 20.92 || 18.44 || 2.48 || 1.96\n",
      "Meta-testing...\n",
      "Reward : 5.66108403\n",
      "Evaluation using 32 tasks. Mean reward: -1366.5506591796875. Mean MSE: 54.82 || 21.14 || 7.11 || 4.68\n",
      "Meta-testing...\n",
      "Reward : 5.58897195\n",
      "Evaluation using 32 tasks. Mean reward: -1193.85302734375. Mean MSE: 20.31 || 16.14 || 6.95 || 3.03\n",
      "Meta-testing...\n",
      "Reward : 5.77783403\n",
      "Evaluation using 32 tasks. Mean reward: -1814.4576416015625. Mean MSE: 61.80 || 45.25 || 6.59 || 6.52\n",
      "Meta-testing...\n",
      "Reward : 5.885474790000001\n",
      "Evaluation using 32 tasks. Mean reward: -1096.1566162109375. Mean MSE: 39.00 || 13.52 || 5.32 || 3.68\n",
      "Meta-testing...\n",
      "Reward : 5.6639620100000005\n",
      "Evaluation using 32 tasks. Mean reward: -1681.69189453125. Mean MSE: 40.07 || 27.18 || 6.77 || 7.00\n",
      "Meta-testing...\n",
      "Reward : 5.6185950999999985\n",
      "Evaluation using 32 tasks. Mean reward: -1962.89501953125. Mean MSE: 66.94 || 34.31 || 9.82 || 5.96\n",
      "Meta-testing...\n",
      "Reward : 6.117004810000001\n",
      "Evaluation using 32 tasks. Mean reward: -983.01171875. Mean MSE: 29.35 || 17.22 || 4.78 || 3.00\n",
      "Meta-testing...\n",
      "Reward : 6.015259520000001\n",
      "Evaluation using 32 tasks. Mean reward: -648.3046264648438. Mean MSE: 18.29 || 9.81 || 1.94 || 2.61\n",
      "Meta-testing...\n",
      "Reward : 5.844266810000001\n",
      "Evaluation using 32 tasks. Mean reward: -1699.0540771484375. Mean MSE: 42.39 || 20.13 || 7.76 || 4.82\n",
      "Meta-testing...\n",
      "Reward : 5.57984009\n",
      "Evaluation using 32 tasks. Mean reward: -626.50830078125. Mean MSE: 18.73 || 8.30 || 3.46 || 1.84\n",
      "Meta-testing...\n",
      "Reward : 6.180919370000002\n",
      "Evaluation using 32 tasks. Mean reward: -1188.6390380859375. Mean MSE: 42.63 || 14.78 || 4.76 || 4.65\n",
      "Meta-testing...\n",
      "Reward : 5.204977809999999\n",
      "Evaluation using 32 tasks. Mean reward: -994.2274780273438. Mean MSE: 30.82 || 20.33 || 4.22 || 5.11\n",
      "Meta-testing...\n",
      "Reward : 5.580442430000001\n",
      "Evaluation using 32 tasks. Mean reward: -1035.7437744140625. Mean MSE: 38.55 || 15.48 || 5.05 || 6.38\n",
      "Meta-testing...\n",
      "Reward : 5.74848607\n",
      "Evaluation using 32 tasks. Mean reward: -738.918701171875. Mean MSE: 20.53 || 9.39 || 3.57 || 2.60\n",
      "Meta-testing...\n",
      "Reward : 5.642422170000001\n",
      "Evaluation using 32 tasks. Mean reward: -1316.6131591796875. Mean MSE: 48.00 || 27.88 || 5.86 || 5.44\n",
      "Meta-testing...\n",
      "Reward : 6.317443810000001\n",
      "Evaluation using 32 tasks. Mean reward: -901.8302001953125. Mean MSE: 18.91 || 11.96 || 3.64 || 3.69\n",
      "Meta-testing...\n",
      "Reward : 5.664554119999999\n",
      "Evaluation using 32 tasks. Mean reward: -1101.44580078125. Mean MSE: 43.96 || 21.07 || 3.52 || 3.50\n",
      "Meta-testing...\n",
      "Reward : 5.6968456\n",
      "Evaluation using 32 tasks. Mean reward: -1863.3355712890625. Mean MSE: 51.45 || 30.77 || 9.84 || 6.36\n",
      "Meta-testing...\n",
      "Reward : 5.97204953\n",
      "Evaluation using 32 tasks. Mean reward: -1177.6558837890625. Mean MSE: 42.92 || 23.07 || 4.19 || 7.16\n",
      "Meta-testing...\n",
      "Reward : 5.893332710000001\n",
      "Evaluation using 32 tasks. Mean reward: -1089.7178955078125. Mean MSE: 47.03 || 11.15 || 5.19 || 6.70\n",
      "Meta-testing...\n",
      "Reward : 5.69722643\n",
      "Evaluation using 32 tasks. Mean reward: -877.5438232421875. Mean MSE: 32.23 || 18.46 || 3.92 || 4.28\n",
      "Meta-testing...\n",
      "Reward : 5.9026495699999995\n",
      "Evaluation using 32 tasks. Mean reward: -1581.3504638671875. Mean MSE: 46.87 || 26.94 || 5.46 || 6.18\n",
      "Meta-testing...\n",
      "Reward : 5.6809924999999994\n",
      "Evaluation using 32 tasks. Mean reward: -805.2083740234375. Mean MSE: 32.63 || 11.35 || 3.95 || 4.02\n",
      "Meta-testing...\n",
      "Reward : 6.06290141\n",
      "Evaluation using 32 tasks. Mean reward: -1455.9927978515625. Mean MSE: 50.96 || 20.68 || 4.80 || 5.26\n",
      "Meta-testing...\n",
      "Reward : 6.09928325\n",
      "Evaluation using 32 tasks. Mean reward: -1505.66552734375. Mean MSE: 36.78 || 19.51 || 7.61 || 6.83\n",
      "Meta-testing...\n",
      "Reward : 6.016219320000001\n",
      "Evaluation using 32 tasks. Mean reward: -956.5965576171875. Mean MSE: 43.62 || 17.54 || 4.86 || 3.66\n",
      "Meta-testing...\n",
      "Reward : 6.0369943699999995\n",
      "Evaluation using 32 tasks. Mean reward: -958.599853515625. Mean MSE: 28.59 || 15.17 || 4.56 || 4.43\n",
      "Meta-testing...\n",
      "Reward : 5.82251307\n",
      "Evaluation using 32 tasks. Mean reward: -1562.600830078125. Mean MSE: 57.98 || 17.77 || 3.79 || 8.39\n",
      "Meta-testing...\n",
      "Reward : 6.07590255\n",
      "Evaluation using 32 tasks. Mean reward: -1126.388671875. Mean MSE: 32.13 || 25.18 || 6.02 || 3.03\n",
      "Meta-testing...\n",
      "Reward : 5.752780169999999\n",
      "Evaluation using 32 tasks. Mean reward: -1585.825439453125. Mean MSE: 53.68 || 27.59 || 7.93 || 5.54\n",
      "Meta-testing...\n",
      "Reward : 5.5866171399999995\n",
      "Evaluation using 32 tasks. Mean reward: -1597.9510498046875. Mean MSE: 47.17 || 17.34 || 8.75 || 4.97\n",
      "Meta-testing...\n",
      "Reward : 5.439017639999999\n",
      "Evaluation using 32 tasks. Mean reward: -767.0653686523438. Mean MSE: 26.60 || 12.05 || 3.19 || 4.04\n",
      "Meta-testing...\n",
      "Reward : 5.88477991\n",
      "Evaluation using 32 tasks. Mean reward: -1309.0345458984375. Mean MSE: 45.32 || 21.36 || 7.90 || 6.54\n",
      "Meta-testing...\n",
      "Reward : 5.825841359999999\n",
      "Evaluation using 32 tasks. Mean reward: -1954.290283203125. Mean MSE: 35.55 || 21.86 || 8.04 || 5.98\n",
      "Meta-testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward : 6.05318713\n",
      "Evaluation using 32 tasks. Mean reward: -985.8950805664062. Mean MSE: 32.65 || 16.84 || 3.90 || 2.87\n",
      "Meta-testing...\n",
      "Reward : 5.9113981199999985\n",
      "Evaluation using 32 tasks. Mean reward: -1074.0208740234375. Mean MSE: 39.98 || 16.99 || 3.82 || 4.73\n",
      "Meta-testing...\n",
      "Reward : 5.55666134\n",
      "Evaluation using 32 tasks. Mean reward: -809.1198120117188. Mean MSE: 30.20 || 11.45 || 4.85 || 2.48\n",
      "Meta-testing...\n",
      "Reward : 5.69149079\n",
      "Evaluation using 32 tasks. Mean reward: -1198.4410400390625. Mean MSE: 44.75 || 13.96 || 4.83 || 5.31\n",
      "Meta-testing...\n",
      "Reward : 5.681381909999999\n",
      "Evaluation using 32 tasks. Mean reward: -858.2080078125. Mean MSE: 37.50 || 17.20 || 3.20 || 3.38\n",
      "Meta-testing...\n",
      "Reward : 6.07492276\n",
      "Evaluation using 32 tasks. Mean reward: -825.523681640625. Mean MSE: 32.29 || 15.31 || 4.18 || 3.53\n",
      "Meta-testing...\n",
      "Reward : 5.53541057\n",
      "Evaluation using 32 tasks. Mean reward: -1099.7745361328125. Mean MSE: 52.12 || 16.86 || 4.89 || 3.75\n",
      "Meta-testing...\n",
      "Reward : 5.74495271\n",
      "Evaluation using 32 tasks. Mean reward: -686.179931640625. Mean MSE: 23.54 || 8.19 || 3.75 || 4.55\n",
      "Meta-testing...\n",
      "Reward : 5.83605266\n",
      "Evaluation using 32 tasks. Mean reward: -1130.75927734375. Mean MSE: 35.03 || 21.82 || 5.00 || 3.20\n",
      "Meta-testing...\n",
      "Reward : 6.03225419\n",
      "Evaluation using 32 tasks. Mean reward: -1250.0963134765625. Mean MSE: 47.50 || 17.94 || 4.14 || 6.44\n",
      "Meta-testing...\n",
      "Reward : 5.82582114\n",
      "Evaluation using 32 tasks. Mean reward: -1509.718994140625. Mean MSE: 36.22 || 15.74 || 9.75 || 3.56\n",
      "Meta-testing...\n",
      "Reward : 5.9732527300000005\n",
      "Evaluation using 32 tasks. Mean reward: -699.8167114257812. Mean MSE: 31.16 || 13.06 || 2.81 || 2.64\n",
      "Meta-testing...\n",
      "Reward : 5.306576740000001\n",
      "Evaluation using 32 tasks. Mean reward: -687.440673828125. Mean MSE: 36.48 || 9.45 || 3.62 || 2.22\n",
      "Meta-testing...\n",
      "Reward : 6.084286969999999\n",
      "Evaluation using 32 tasks. Mean reward: -930.45166015625. Mean MSE: 35.43 || 15.01 || 3.61 || 4.63\n",
      "Meta-testing...\n",
      "Reward : 5.802810470000001\n",
      "Evaluation using 32 tasks. Mean reward: -1180.7210693359375. Mean MSE: 37.04 || 20.69 || 5.78 || 4.37\n",
      "Meta-testing...\n",
      "Reward : 6.232467969999999\n",
      "Evaluation using 32 tasks. Mean reward: -662.5657348632812. Mean MSE: 31.75 || 9.91 || 3.69 || 2.46\n",
      "Meta-testing...\n",
      "Reward : 5.95447461\n",
      "Evaluation using 32 tasks. Mean reward: -826.4844970703125. Mean MSE: 29.53 || 16.21 || 2.68 || 4.58\n",
      "Meta-testing...\n",
      "Reward : 5.784630619999999\n",
      "Evaluation using 32 tasks. Mean reward: -1331.2930908203125. Mean MSE: 53.89 || 23.23 || 6.74 || 5.67\n",
      "Meta-testing...\n",
      "Reward : 5.912190900000001\n",
      "Evaluation using 32 tasks. Mean reward: -1215.4219970703125. Mean MSE: 59.36 || 21.86 || 5.21 || 3.20\n",
      "Meta-testing...\n",
      "Reward : 5.88767477\n",
      "Evaluation using 32 tasks. Mean reward: -1029.69873046875. Mean MSE: 32.61 || 18.50 || 3.78 || 2.99\n",
      "Meta-testing...\n",
      "Reward : 5.417934720000001\n",
      "Evaluation using 32 tasks. Mean reward: -1033.9520263671875. Mean MSE: 37.31 || 15.27 || 2.70 || 5.32\n",
      "Meta-testing...\n",
      "Reward : 5.91922997\n",
      "Evaluation using 32 tasks. Mean reward: -736.8305053710938. Mean MSE: 28.68 || 15.88 || 2.73 || 1.99\n",
      "Meta-testing...\n",
      "Reward : 6.043089729999999\n",
      "Evaluation using 32 tasks. Mean reward: -637.2991943359375. Mean MSE: 22.11 || 9.19 || 3.02 || 3.01\n",
      "Meta-testing...\n",
      "Reward : 5.67424845\n",
      "Evaluation using 32 tasks. Mean reward: -1219.4805908203125. Mean MSE: 43.01 || 16.72 || 4.74 || 5.31\n",
      "Meta-testing...\n",
      "Reward : 5.83781025\n",
      "Evaluation using 32 tasks. Mean reward: -1090.5947265625. Mean MSE: 30.03 || 21.18 || 4.70 || 4.10\n",
      "Meta-testing...\n",
      "Reward : 6.031940500000001\n",
      "Evaluation using 32 tasks. Mean reward: -680.4896240234375. Mean MSE: 25.36 || 8.50 || 2.77 || 3.78\n",
      "Meta-testing...\n",
      "Reward : 5.898589090000001\n",
      "Evaluation using 32 tasks. Mean reward: -996.1253662109375. Mean MSE: 36.12 || 15.28 || 4.41 || 3.53\n",
      "Meta-testing...\n",
      "Reward : 6.06637313\n",
      "Evaluation using 32 tasks. Mean reward: -1375.407958984375. Mean MSE: 49.67 || 26.15 || 4.59 || 4.09\n",
      "Meta-testing...\n",
      "Reward : 6.58765739\n",
      "Evaluation using 32 tasks. Mean reward: -1281.1192626953125. Mean MSE: 49.58 || 23.66 || 9.36 || 3.83\n",
      "Meta-testing...\n",
      "Reward : 6.08661069\n",
      "Evaluation using 32 tasks. Mean reward: -733.2664794921875. Mean MSE: 37.62 || 12.16 || 2.29 || 3.38\n",
      "Meta-testing...\n",
      "Reward : 6.218439430000001\n",
      "Evaluation using 32 tasks. Mean reward: -889.2078247070312. Mean MSE: 28.38 || 12.52 || 3.51 || 2.53\n",
      "Meta-testing...\n",
      "Reward : 5.8973784999999985\n",
      "Evaluation using 32 tasks. Mean reward: -623.8937377929688. Mean MSE: 33.02 || 9.30 || 2.43 || 2.27\n",
      "Meta-testing...\n",
      "Reward : 5.67900698\n",
      "Evaluation using 32 tasks. Mean reward: -618.70166015625. Mean MSE: 27.72 || 7.83 || 2.87 || 2.20\n",
      "Meta-testing...\n",
      "Reward : 6.07217607\n",
      "Evaluation using 32 tasks. Mean reward: -870.6856689453125. Mean MSE: 32.61 || 13.59 || 2.18 || 2.68\n",
      "Meta-testing...\n",
      "Reward : 6.034579759999999\n",
      "Evaluation using 32 tasks. Mean reward: -806.6168823242188. Mean MSE: 29.76 || 15.35 || 3.11 || 4.44\n",
      "Meta-testing...\n",
      "Reward : 6.146023109999999\n",
      "Evaluation using 32 tasks. Mean reward: -2855.60400390625. Mean MSE: 59.88 || 42.73 || 18.21 || 12.24\n",
      "Meta-testing...\n",
      "Reward : 6.05837578\n",
      "Evaluation using 32 tasks. Mean reward: -847.260498046875. Mean MSE: 36.83 || 16.81 || 4.03 || 3.61\n",
      "Meta-testing...\n",
      "Reward : 6.30737255\n",
      "Evaluation using 32 tasks. Mean reward: -584.7753295898438. Mean MSE: 24.80 || 9.24 || 2.03 || 1.80\n",
      "Meta-testing...\n",
      "Reward : 6.03301879\n",
      "Evaluation using 32 tasks. Mean reward: -896.7073364257812. Mean MSE: 50.53 || 15.58 || 4.17 || 2.43\n",
      "Meta-testing...\n",
      "Reward : 6.02183551\n",
      "Evaluation using 32 tasks. Mean reward: -996.2694091796875. Mean MSE: 29.54 || 15.02 || 4.53 || 3.74\n",
      "Meta-testing...\n",
      "Reward : 6.308012760000001\n",
      "Evaluation using 32 tasks. Mean reward: -1123.425048828125. Mean MSE: 36.89 || 23.35 || 3.33 || 5.65\n",
      "Meta-testing...\n",
      "Reward : 6.307292839999999\n",
      "Evaluation using 32 tasks. Mean reward: -1993.7877197265625. Mean MSE: 51.92 || 23.51 || 11.46 || 12.05\n",
      "Meta-testing...\n",
      "Reward : 6.21418149\n",
      "Evaluation using 32 tasks. Mean reward: -1101.728515625. Mean MSE: 40.21 || 18.75 || 3.59 || 3.21\n",
      "Meta-testing...\n",
      "Reward : 6.228683629999999\n",
      "Evaluation using 32 tasks. Mean reward: -680.2008056640625. Mean MSE: 27.92 || 9.90 || 3.33 || 2.54\n",
      "Meta-testing...\n",
      "Reward : 6.312611459999999\n",
      "Evaluation using 32 tasks. Mean reward: -1239.2664794921875. Mean MSE: 51.45 || 15.04 || 5.04 || 6.36\n",
      "Meta-testing...\n",
      "Reward : 5.993360269999999\n",
      "Evaluation using 32 tasks. Mean reward: -663.1298828125. Mean MSE: 27.17 || 9.59 || 4.73 || 2.63\n",
      "Meta-testing...\n",
      "Reward : 6.296471029999999\n",
      "Evaluation using 32 tasks. Mean reward: -790.6571044921875. Mean MSE: 32.13 || 12.08 || 2.67 || 3.51\n",
      "Meta-testing...\n",
      "Reward : 5.72791561\n",
      "Evaluation using 32 tasks. Mean reward: -1456.9954833984375. Mean MSE: 47.48 || 29.31 || 4.82 || 6.24\n",
      "Meta-testing...\n",
      "Reward : 6.35194054\n",
      "Evaluation using 32 tasks. Mean reward: -851.9417114257812. Mean MSE: 32.70 || 10.46 || 3.16 || 2.96\n",
      "Meta-testing...\n",
      "Reward : 6.202131159999999\n",
      "Evaluation using 32 tasks. Mean reward: -765.93994140625. Mean MSE: 37.89 || 12.34 || 4.43 || 2.43\n",
      "Meta-testing...\n",
      "Reward : 6.698573949999999\n",
      "Evaluation using 32 tasks. Mean reward: -2102.51220703125. Mean MSE: 40.08 || 30.19 || 12.24 || 4.80\n",
      "Meta-testing...\n",
      "Reward : 6.4285593599999995\n",
      "Evaluation using 32 tasks. Mean reward: -1044.634765625. Mean MSE: 47.59 || 14.82 || 5.51 || 4.57\n",
      "Meta-testing...\n",
      "Reward : 6.257235969999999\n",
      "Evaluation using 32 tasks. Mean reward: -1589.2901611328125. Mean MSE: 41.49 || 21.99 || 10.25 || 6.65\n",
      "Meta-testing...\n",
      "Reward : 6.03346456\n",
      "Evaluation using 32 tasks. Mean reward: -1475.3427734375. Mean MSE: 56.16 || 30.37 || 10.26 || 4.29\n",
      "Meta-testing...\n",
      "Reward : 6.338381119999999\n",
      "Evaluation using 32 tasks. Mean reward: -942.18408203125. Mean MSE: 36.32 || 16.64 || 4.07 || 2.56\n",
      "Meta-testing...\n",
      "Reward : 6.19844608\n",
      "Evaluation using 32 tasks. Mean reward: -1689.8330078125. Mean MSE: 51.79 || 26.58 || 4.80 || 5.72\n",
      "Meta-testing...\n",
      "Reward : 6.37868366\n",
      "Evaluation using 32 tasks. Mean reward: -979.2901611328125. Mean MSE: 38.37 || 15.24 || 3.93 || 7.12\n",
      "Meta-testing...\n",
      "Reward : 6.0235505200000015\n",
      "Evaluation using 32 tasks. Mean reward: -1171.10986328125. Mean MSE: 46.59 || 18.72 || 4.51 || 4.73\n",
      "Meta-testing...\n",
      "Reward : 6.490261309999999\n",
      "Evaluation using 32 tasks. Mean reward: -1315.7674560546875. Mean MSE: 39.19 || 24.38 || 5.68 || 3.12\n",
      "Meta-testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward : 6.131806749999999\n",
      "Evaluation using 32 tasks. Mean reward: -586.7000122070312. Mean MSE: 37.29 || 7.48 || 1.29 || 3.65\n",
      "Meta-testing...\n",
      "Reward : 6.018795749999999\n",
      "Evaluation using 32 tasks. Mean reward: -839.5883178710938. Mean MSE: 32.10 || 10.83 || 3.58 || 3.75\n",
      "Meta-testing...\n",
      "Reward : 6.051083040000001\n",
      "Evaluation using 32 tasks. Mean reward: -596.43212890625. Mean MSE: 27.09 || 6.22 || 2.23 || 2.33\n",
      "Meta-testing...\n",
      "Reward : 6.21004806\n",
      "Evaluation using 32 tasks. Mean reward: -1513.443359375. Mean MSE: 84.83 || 31.14 || 4.26 || 5.46\n",
      "Meta-testing...\n",
      "Reward : 6.3203536399999996\n",
      "Evaluation using 32 tasks. Mean reward: -1033.6566162109375. Mean MSE: 57.33 || 25.15 || 3.16 || 4.69\n",
      "Meta-testing...\n",
      "Reward : 5.885676\n",
      "Evaluation using 32 tasks. Mean reward: -739.381103515625. Mean MSE: 27.40 || 16.78 || 4.16 || 4.00\n",
      "Meta-testing...\n",
      "Reward : 5.964707989999999\n",
      "Evaluation using 32 tasks. Mean reward: -836.548095703125. Mean MSE: 30.01 || 12.52 || 4.30 || 3.69\n",
      "Meta-testing...\n",
      "Reward : 6.1160546700000005\n",
      "Evaluation using 32 tasks. Mean reward: -895.5949096679688. Mean MSE: 41.13 || 15.48 || 2.63 || 4.10\n",
      "Meta-testing...\n",
      "Reward : 6.478104399999999\n",
      "Evaluation using 32 tasks. Mean reward: -1354.658935546875. Mean MSE: 63.52 || 21.47 || 4.90 || 4.49\n",
      "Meta-testing...\n",
      "Reward : 6.37946339\n",
      "Evaluation using 32 tasks. Mean reward: -710.45703125. Mean MSE: 22.23 || 6.83 || 4.21 || 2.48\n",
      "Meta-testing...\n",
      "Reward : 6.258691670000001\n",
      "Evaluation using 32 tasks. Mean reward: -899.1712036132812. Mean MSE: 33.46 || 21.25 || 3.98 || 3.73\n",
      "Meta-testing...\n",
      "Reward : 6.21111929\n",
      "Evaluation using 32 tasks. Mean reward: -1106.6209716796875. Mean MSE: 45.11 || 16.92 || 6.42 || 4.41\n",
      "Meta-testing...\n",
      "Reward : 6.5772939699999995\n",
      "Evaluation using 32 tasks. Mean reward: -920.1209106445312. Mean MSE: 27.68 || 13.49 || 4.47 || 4.28\n",
      "Meta-testing...\n",
      "Reward : 6.260410710000001\n",
      "Evaluation using 32 tasks. Mean reward: -1951.499755859375. Mean MSE: 63.39 || 28.81 || 11.03 || 6.75\n",
      "Meta-testing...\n",
      "Reward : 6.302952110000001\n",
      "Evaluation using 32 tasks. Mean reward: -670.314208984375. Mean MSE: 24.99 || 12.25 || 2.56 || 3.95\n",
      "Meta-testing...\n",
      "Reward : 6.2801539800000015\n",
      "Evaluation using 32 tasks. Mean reward: -1810.3114013671875. Mean MSE: 56.16 || 31.21 || 11.27 || 6.35\n",
      "Meta-testing...\n",
      "Reward : 5.970327330000001\n",
      "Evaluation using 32 tasks. Mean reward: -1360.182861328125. Mean MSE: 54.42 || 20.80 || 7.43 || 7.79\n",
      "Meta-testing...\n",
      "Reward : 6.457002799999999\n",
      "Evaluation using 32 tasks. Mean reward: -1033.6912841796875. Mean MSE: 46.54 || 22.72 || 3.67 || 3.46\n",
      "Meta-testing...\n",
      "Reward : 6.1810538500000005\n",
      "Evaluation using 32 tasks. Mean reward: -942.3938598632812. Mean MSE: 29.60 || 22.71 || 4.19 || 2.77\n",
      "Meta-testing...\n",
      "Reward : 6.12210655\n",
      "Evaluation using 32 tasks. Mean reward: -864.9736328125. Mean MSE: 32.80 || 10.65 || 4.24 || 2.82\n",
      "Meta-testing...\n",
      "Reward : 6.43295543\n",
      "Evaluation using 32 tasks. Mean reward: -763.4345703125. Mean MSE: 30.71 || 11.81 || 2.48 || 2.48\n",
      "Meta-testing...\n",
      "Reward : 6.330981319999999\n",
      "Evaluation using 32 tasks. Mean reward: -760.717529296875. Mean MSE: 38.97 || 12.27 || 1.75 || 3.31\n",
      "Meta-testing...\n",
      "Reward : 5.89648174\n",
      "Evaluation using 32 tasks. Mean reward: -953.7042236328125. Mean MSE: 32.71 || 17.22 || 3.12 || 3.25\n",
      "Meta-testing...\n",
      "Reward : 6.392455559999999\n",
      "Evaluation using 32 tasks. Mean reward: -678.67724609375. Mean MSE: 20.78 || 13.96 || 2.40 || 2.57\n",
      "Meta-testing...\n",
      "Reward : 6.0929830900000015\n",
      "Evaluation using 32 tasks. Mean reward: -1990.9254150390625. Mean MSE: 87.54 || 32.60 || 9.20 || 8.47\n",
      "Meta-testing...\n",
      "Reward : 6.2533076\n",
      "Evaluation using 32 tasks. Mean reward: -1123.1824951171875. Mean MSE: 54.72 || 20.03 || 2.91 || 3.86\n",
      "Meta-testing...\n",
      "Reward : 6.8304315099999995\n",
      "Evaluation using 32 tasks. Mean reward: -1168.8380126953125. Mean MSE: 37.30 || 21.61 || 3.34 || 5.04\n",
      "Meta-testing...\n",
      "Reward : 6.332221039999999\n",
      "Evaluation using 32 tasks. Mean reward: -476.7972412109375. Mean MSE: 21.64 || 7.26 || 1.65 || 1.29\n",
      "Meta-testing...\n",
      "Reward : 6.428689029999999\n",
      "Evaluation using 32 tasks. Mean reward: -436.6029052734375. Mean MSE: 12.38 || 6.95 || 2.23 || 1.67\n",
      "Meta-testing...\n",
      "Reward : 6.512294740000001\n",
      "Evaluation using 32 tasks. Mean reward: -1129.3753662109375. Mean MSE: 43.23 || 13.59 || 7.64 || 3.54\n",
      "Meta-testing...\n",
      "Reward : 6.36831539\n",
      "Evaluation using 32 tasks. Mean reward: -489.9901123046875. Mean MSE: 23.13 || 9.25 || 1.54 || 1.65\n",
      "Meta-testing...\n",
      "Reward : 6.112717039999999\n",
      "Evaluation using 32 tasks. Mean reward: -887.1665649414062. Mean MSE: 28.68 || 11.05 || 3.46 || 4.13\n",
      "Meta-testing...\n",
      "Reward : 6.684445790000001\n",
      "Evaluation using 32 tasks. Mean reward: -696.167724609375. Mean MSE: 22.46 || 11.73 || 1.85 || 2.84\n",
      "Meta-testing...\n",
      "Reward : 6.3490525999999985\n",
      "Evaluation using 32 tasks. Mean reward: -1262.0721435546875. Mean MSE: 54.81 || 20.43 || 4.62 || 3.39\n",
      "Meta-testing...\n",
      "Reward : 6.48253937\n",
      "Evaluation using 32 tasks. Mean reward: -950.4552001953125. Mean MSE: 38.97 || 16.11 || 5.00 || 3.01\n",
      "Meta-testing...\n",
      "Reward : 6.18127502\n",
      "Evaluation using 32 tasks. Mean reward: -873.9645385742188. Mean MSE: 32.86 || 18.40 || 3.82 || 3.11\n",
      "Meta-testing...\n",
      "Reward : 6.50606305\n",
      "Evaluation using 32 tasks. Mean reward: -1478.5966796875. Mean MSE: 61.61 || 26.49 || 5.27 || 5.77\n",
      "Meta-testing...\n",
      "Reward : 6.51623017\n",
      "Evaluation using 32 tasks. Mean reward: -959.4440307617188. Mean MSE: 38.94 || 14.35 || 2.21 || 4.29\n",
      "Meta-testing...\n",
      "Reward : 6.236824509999999\n",
      "Evaluation using 32 tasks. Mean reward: -897.22021484375. Mean MSE: 31.34 || 14.90 || 4.91 || 2.58\n",
      "Meta-testing...\n",
      "Reward : 6.45492231\n",
      "Evaluation using 32 tasks. Mean reward: -1156.3095703125. Mean MSE: 47.08 || 15.67 || 4.65 || 5.05\n",
      "Meta-testing...\n",
      "Reward : 6.461608900000001\n",
      "Evaluation using 32 tasks. Mean reward: -818.0879516601562. Mean MSE: 37.21 || 18.32 || 2.39 || 2.42\n",
      "Meta-testing...\n",
      "Reward : 6.56090298\n",
      "Evaluation using 32 tasks. Mean reward: -1475.37646484375. Mean MSE: 51.27 || 28.71 || 5.72 || 6.79\n",
      "Meta-testing...\n",
      "Reward : 6.40574479\n",
      "Evaluation using 32 tasks. Mean reward: -987.6612548828125. Mean MSE: 40.09 || 22.41 || 3.25 || 3.99\n",
      "Meta-testing...\n",
      "Reward : 6.6309779099999995\n",
      "Evaluation using 32 tasks. Mean reward: -1079.0771484375. Mean MSE: 34.68 || 14.22 || 5.62 || 5.57\n",
      "Meta-testing...\n",
      "Reward : 6.702059139999999\n",
      "Evaluation using 32 tasks. Mean reward: -729.898193359375. Mean MSE: 36.94 || 15.71 || 3.77 || 3.24\n",
      "Meta-testing...\n",
      "Reward : 6.555077700000001\n",
      "Evaluation using 32 tasks. Mean reward: -899.0189819335938. Mean MSE: 34.82 || 13.90 || 4.49 || 5.56\n",
      "Meta-testing...\n",
      "Reward : 6.549652779999999\n",
      "Evaluation using 32 tasks. Mean reward: -1554.9429931640625. Mean MSE: 48.66 || 22.65 || 6.91 || 6.33\n",
      "Meta-testing...\n",
      "Reward : 6.37393571\n",
      "Evaluation using 32 tasks. Mean reward: -615.098876953125. Mean MSE: 27.27 || 8.48 || 2.76 || 2.56\n",
      "Meta-testing...\n",
      "Reward : 6.379944940000001\n",
      "Evaluation using 32 tasks. Mean reward: -854.5018310546875. Mean MSE: 34.73 || 12.38 || 3.99 || 3.08\n",
      "Meta-testing...\n",
      "Reward : 6.405445670000001\n",
      "Evaluation using 32 tasks. Mean reward: -520.0150146484375. Mean MSE: 19.64 || 8.89 || 1.94 || 2.49\n",
      "Meta-testing...\n",
      "Reward : 6.510442079999999\n",
      "Evaluation using 32 tasks. Mean reward: -746.1884155273438. Mean MSE: 32.40 || 13.80 || 3.01 || 1.78\n",
      "Meta-testing...\n",
      "Reward : 6.35203012\n",
      "Evaluation using 32 tasks. Mean reward: -504.9707946777344. Mean MSE: 23.56 || 6.72 || 1.77 || 2.40\n",
      "Meta-testing...\n",
      "Reward : 6.609631949999999\n",
      "Evaluation using 32 tasks. Mean reward: -516.1376342773438. Mean MSE: 18.82 || 8.18 || 2.42 || 1.67\n",
      "Meta-testing...\n",
      "Reward : 6.607580160000001\n",
      "Evaluation using 32 tasks. Mean reward: -496.7818603515625. Mean MSE: 20.71 || 8.54 || 1.89 || 3.24\n",
      "Meta-testing...\n",
      "Reward : 6.776508050000001\n",
      "Evaluation using 32 tasks. Mean reward: -908.88134765625. Mean MSE: 40.96 || 16.90 || 1.81 || 3.98\n",
      "Meta-testing...\n",
      "Reward : 6.802769\n",
      "Evaluation using 32 tasks. Mean reward: -1049.6689453125. Mean MSE: 44.11 || 10.27 || 6.64 || 4.62\n",
      "Meta-testing...\n",
      "Reward : 6.7632301\n",
      "Evaluation using 32 tasks. Mean reward: -699.9087524414062. Mean MSE: 22.79 || 11.85 || 2.98 || 2.59\n",
      "Meta-testing...\n",
      "Reward : 6.7841175\n",
      "Evaluation using 32 tasks. Mean reward: -611.0892944335938. Mean MSE: 25.44 || 11.57 || 2.06 || 2.67\n",
      "Meta-testing...\n",
      "Reward : 6.394897710000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation using 32 tasks. Mean reward: -653.3272094726562. Mean MSE: 23.40 || 10.84 || 3.93 || 2.58\n",
      "Meta-testing...\n",
      "Reward : 6.62827546\n",
      "Evaluation using 32 tasks. Mean reward: -855.8544921875. Mean MSE: 50.19 || 11.26 || 3.32 || 3.81\n",
      "Meta-testing...\n",
      "Reward : 6.217001519999999\n",
      "Evaluation using 32 tasks. Mean reward: -489.33038330078125. Mean MSE: 21.47 || 11.56 || 1.61 || 1.51\n",
      "Meta-testing...\n",
      "Reward : 6.2811195799999995\n",
      "Evaluation using 32 tasks. Mean reward: -516.6038208007812. Mean MSE: 28.64 || 12.85 || 2.49 || 1.71\n",
      "Meta-testing...\n",
      "Reward : 6.75455059\n",
      "Evaluation using 32 tasks. Mean reward: -966.8857421875. Mean MSE: 40.73 || 17.19 || 3.84 || 2.90\n",
      "Meta-testing...\n",
      "Reward : 6.7840482600000005\n",
      "Evaluation using 32 tasks. Mean reward: -1439.9071044921875. Mean MSE: 78.64 || 21.43 || 4.98 || 4.64\n",
      "Meta-testing...\n",
      "Reward : 6.4421700799999995\n",
      "Evaluation using 32 tasks. Mean reward: -500.4288330078125. Mean MSE: 30.89 || 6.36 || 1.30 || 2.17\n",
      "Meta-testing...\n",
      "Reward : 6.77222392\n",
      "Evaluation using 32 tasks. Mean reward: -1019.3505859375. Mean MSE: 38.69 || 19.18 || 4.27 || 3.27\n",
      "Meta-testing...\n",
      "Reward : 6.245550350000001\n",
      "Evaluation using 32 tasks. Mean reward: -709.4569091796875. Mean MSE: 28.20 || 12.65 || 2.59 || 4.31\n",
      "Meta-testing...\n",
      "Reward : 6.937389520000002\n",
      "Evaluation using 32 tasks. Mean reward: -745.2093505859375. Mean MSE: 33.00 || 7.99 || 5.25 || 3.18\n",
      "Meta-testing...\n",
      "Reward : 6.795358820000001\n",
      "Evaluation using 32 tasks. Mean reward: -748.9036254882812. Mean MSE: 36.51 || 18.40 || 2.32 || 2.70\n",
      "Meta-testing...\n",
      "Reward : 6.5839902100000005\n",
      "Evaluation using 32 tasks. Mean reward: -576.9553833007812. Mean MSE: 37.98 || 9.70 || 3.10 || 2.83\n",
      "Meta-testing...\n",
      "Reward : 6.6733078\n",
      "Evaluation using 32 tasks. Mean reward: -906.4043579101562. Mean MSE: 28.81 || 14.96 || 5.09 || 2.29\n",
      "Meta-testing...\n",
      "Reward : 6.59151499\n",
      "Evaluation using 32 tasks. Mean reward: -787.6868896484375. Mean MSE: 40.75 || 14.13 || 6.08 || 2.48\n",
      "Meta-testing...\n",
      "Reward : 6.650940150000001\n"
     ]
    }
   ],
   "source": [
    "gp_list, test_kwargs, init_prior_test = get_task_sequence(alpha=0.25,\n",
    "                                                          n_restarts=1,\n",
    "                                                          num_test_processes=2)\n",
    "\n",
    "r = agent.train_identification(5000, task_generator, \"gauss-v0\", 0, \".\",\n",
    "                             20, 32, gp_list, 20,\n",
    "                             test_kwargs, 30, 2, False,\n",
    "                             init_prior_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation using 32 tasks. Mean reward: -711.8223266601562. Mean MSE: 36.34 || 9.07 || 4.67 || 2.05\n",
      "Meta-testing...\n",
      "Reward : 6.501267820000001\n",
      "Evaluation using 32 tasks. Mean reward: -1177.87890625. Mean MSE: 58.97 || 23.07 || 5.13 || 4.16\n",
      "Meta-testing...\n",
      "Reward : 6.685256500000001\n",
      "Evaluation using 32 tasks. Mean reward: -1674.89794921875. Mean MSE: 57.21 || 23.43 || 8.62 || 5.90\n",
      "Meta-testing...\n",
      "Reward : 6.927855830000001\n",
      "Evaluation using 32 tasks. Mean reward: -576.7288208007812. Mean MSE: 44.31 || 8.15 || 3.15 || 2.15\n",
      "Meta-testing...\n",
      "Reward : 6.797231240000001\n",
      "Evaluation using 32 tasks. Mean reward: -400.8381042480469. Mean MSE: 14.88 || 4.79 || 1.94 || 1.58\n",
      "Meta-testing...\n",
      "Reward : 6.66618425\n",
      "Evaluation using 32 tasks. Mean reward: -633.984619140625. Mean MSE: 34.51 || 7.15 || 2.56 || 4.25\n",
      "Meta-testing...\n",
      "Reward : 6.62148848\n",
      "Evaluation using 32 tasks. Mean reward: -715.9114990234375. Mean MSE: 27.42 || 12.60 || 2.10 || 2.54\n",
      "Meta-testing...\n",
      "Reward : 6.3888729999999985\n",
      "Evaluation using 32 tasks. Mean reward: -425.5872802734375. Mean MSE: 13.17 || 5.75 || 1.59 || 2.12\n",
      "Meta-testing...\n",
      "Reward : 7.06364838\n",
      "Evaluation using 32 tasks. Mean reward: -481.4290771484375. Mean MSE: 19.45 || 8.10 || 1.90 || 1.78\n",
      "Meta-testing...\n",
      "Reward : 7.073629079999998\n",
      "Evaluation using 32 tasks. Mean reward: -1146.9654541015625. Mean MSE: 49.95 || 29.60 || 4.97 || 2.59\n",
      "Meta-testing...\n",
      "Reward : 7.38850239\n",
      "Evaluation using 32 tasks. Mean reward: -673.6096801757812. Mean MSE: 23.97 || 9.89 || 3.12 || 2.35\n",
      "Meta-testing...\n",
      "Reward : 6.88077105\n",
      "Evaluation using 32 tasks. Mean reward: -389.05914306640625. Mean MSE: 19.42 || 5.94 || 2.23 || 1.08\n",
      "Meta-testing...\n",
      "Reward : 6.616462350000001\n",
      "Evaluation using 32 tasks. Mean reward: -386.93536376953125. Mean MSE: 14.86 || 6.81 || 1.52 || 2.06\n",
      "Meta-testing...\n",
      "Reward : 6.95098019\n",
      "Evaluation using 32 tasks. Mean reward: -618.3004760742188. Mean MSE: 29.72 || 13.99 || 2.36 || 1.73\n",
      "Meta-testing...\n",
      "Reward : 6.6243922500000005\n",
      "Evaluation using 32 tasks. Mean reward: -649.344970703125. Mean MSE: 34.65 || 9.55 || 4.09 || 1.04\n",
      "Meta-testing...\n",
      "Reward : 6.61566002\n",
      "Evaluation using 32 tasks. Mean reward: -987.6802368164062. Mean MSE: 32.38 || 16.78 || 3.21 || 5.13\n",
      "Meta-testing...\n",
      "Reward : 6.96189518\n",
      "Evaluation using 32 tasks. Mean reward: -685.3971557617188. Mean MSE: 31.48 || 11.01 || 4.34 || 1.93\n",
      "Meta-testing...\n",
      "Reward : 7.07157793\n",
      "Evaluation using 32 tasks. Mean reward: -578.0546875. Mean MSE: 25.65 || 8.21 || 2.42 || 2.17\n",
      "Meta-testing...\n",
      "Reward : 6.90040494\n",
      "Evaluation using 32 tasks. Mean reward: -1232.74951171875. Mean MSE: 47.32 || 23.18 || 5.06 || 3.09\n",
      "Meta-testing...\n",
      "Reward : 6.7858795899999995\n",
      "Evaluation using 32 tasks. Mean reward: -372.1156921386719. Mean MSE: 16.69 || 8.19 || 0.91 || 1.41\n",
      "Meta-testing...\n",
      "Reward : 7.24077969\n",
      "Evaluation using 32 tasks. Mean reward: -634.8455200195312. Mean MSE: 27.04 || 15.02 || 2.07 || 1.92\n",
      "Meta-testing...\n",
      "Reward : 7.11750873\n",
      "Evaluation using 32 tasks. Mean reward: -933.2802734375. Mean MSE: 33.06 || 18.27 || 2.59 || 2.03\n",
      "Meta-testing...\n",
      "Reward : 7.189512189999999\n",
      "Evaluation using 32 tasks. Mean reward: -619.9495849609375. Mean MSE: 27.87 || 19.32 || 2.16 || 2.73\n",
      "Meta-testing...\n",
      "Reward : 6.7691140700000005\n",
      "Evaluation using 32 tasks. Mean reward: -690.4668579101562. Mean MSE: 26.55 || 11.50 || 2.73 || 3.26\n",
      "Meta-testing...\n",
      "Reward : 7.326528110000001\n",
      "Evaluation using 32 tasks. Mean reward: -611.8038330078125. Mean MSE: 30.63 || 7.92 || 3.17 || 1.60\n",
      "Meta-testing...\n",
      "Reward : 6.654257670000001\n",
      "Evaluation using 32 tasks. Mean reward: -480.2091064453125. Mean MSE: 36.69 || 6.89 || 2.21 || 1.05\n",
      "Meta-testing...\n",
      "Reward : 6.97236893\n",
      "Evaluation using 32 tasks. Mean reward: -1201.4736328125. Mean MSE: 39.44 || 28.50 || 3.28 || 2.46\n",
      "Meta-testing...\n",
      "Reward : 6.920713869999999\n",
      "Evaluation using 32 tasks. Mean reward: -1225.42822265625. Mean MSE: 58.55 || 17.58 || 6.42 || 4.05\n",
      "Meta-testing...\n",
      "Reward : 6.79555287\n",
      "Evaluation using 32 tasks. Mean reward: -1040.8416748046875. Mean MSE: 47.79 || 20.87 || 2.82 || 2.19\n",
      "Meta-testing...\n",
      "Reward : 7.182285130000001\n",
      "Evaluation using 32 tasks. Mean reward: -653.0698852539062. Mean MSE: 30.54 || 14.62 || 1.64 || 1.84\n",
      "Meta-testing...\n",
      "Reward : 6.91238496\n",
      "Evaluation using 32 tasks. Mean reward: -1176.4246826171875. Mean MSE: 45.18 || 16.66 || 5.93 || 3.57\n",
      "Meta-testing...\n",
      "Reward : 7.22033298\n",
      "Evaluation using 32 tasks. Mean reward: -1479.363037109375. Mean MSE: 56.17 || 33.25 || 6.95 || 4.14\n",
      "Meta-testing...\n",
      "Reward : 7.18147431\n",
      "Evaluation using 32 tasks. Mean reward: -486.2027587890625. Mean MSE: 27.68 || 13.51 || 1.12 || 1.71\n",
      "Meta-testing...\n",
      "Reward : 7.07707558\n",
      "Evaluation using 32 tasks. Mean reward: -519.7279052734375. Mean MSE: 37.56 || 6.18 || 3.05 || 1.63\n",
      "Meta-testing...\n",
      "Reward : 6.884093849999999\n",
      "Evaluation using 32 tasks. Mean reward: -644.5567626953125. Mean MSE: 28.85 || 12.45 || 2.20 || 1.84\n",
      "Meta-testing...\n",
      "Reward : 6.983387409999999\n",
      "Evaluation using 32 tasks. Mean reward: -765.307373046875. Mean MSE: 32.49 || 15.08 || 3.67 || 1.60\n",
      "Meta-testing...\n",
      "Reward : 6.76420722\n",
      "Evaluation using 32 tasks. Mean reward: -871.1480102539062. Mean MSE: 33.14 || 11.98 || 2.56 || 3.62\n",
      "Meta-testing...\n",
      "Reward : 7.241711849999999\n",
      "Evaluation using 32 tasks. Mean reward: -1544.8465576171875. Mean MSE: 59.35 || 28.63 || 5.75 || 4.90\n",
      "Meta-testing...\n",
      "Reward : 6.870144419999999\n",
      "Evaluation using 32 tasks. Mean reward: -1399.0941162109375. Mean MSE: 61.24 || 33.81 || 2.45 || 4.20\n",
      "Meta-testing...\n",
      "Reward : 6.734682160000001\n",
      "Evaluation using 32 tasks. Mean reward: -589.9278564453125. Mean MSE: 26.58 || 10.36 || 3.23 || 1.89\n",
      "Meta-testing...\n",
      "Reward : 6.990787180000001\n",
      "Evaluation using 32 tasks. Mean reward: -1242.6318359375. Mean MSE: 62.36 || 29.97 || 2.57 || 3.63\n",
      "Meta-testing...\n",
      "Reward : 6.89204339\n",
      "Evaluation using 32 tasks. Mean reward: -1206.97998046875. Mean MSE: 57.99 || 25.11 || 4.91 || 3.71\n",
      "Meta-testing...\n",
      "Reward : 7.154299900000001\n",
      "Evaluation using 32 tasks. Mean reward: -628.7392578125. Mean MSE: 39.86 || 6.51 || 2.28 || 2.59\n",
      "Meta-testing...\n",
      "Reward : 7.08868202\n",
      "Evaluation using 32 tasks. Mean reward: -683.0697021484375. Mean MSE: 26.31 || 12.13 || 3.24 || 2.20\n",
      "Meta-testing...\n",
      "Reward : 7.54715026\n",
      "Evaluation using 32 tasks. Mean reward: -587.9825439453125. Mean MSE: 39.89 || 10.07 || 2.00 || 2.56\n",
      "Meta-testing...\n",
      "Reward : 7.020215390000001\n",
      "Evaluation using 32 tasks. Mean reward: -390.28204345703125. Mean MSE: 16.67 || 4.77 || 1.41 || 1.38\n",
      "Meta-testing...\n",
      "Reward : 6.835515600000001\n",
      "Evaluation using 32 tasks. Mean reward: -836.673583984375. Mean MSE: 41.45 || 9.95 || 3.64 || 4.40\n",
      "Meta-testing...\n",
      "Reward : 6.98271273\n",
      "Evaluation using 32 tasks. Mean reward: -479.5376281738281. Mean MSE: 22.04 || 9.69 || 1.88 || 1.66\n",
      "Meta-testing...\n",
      "Reward : 7.16756635\n",
      "Evaluation using 32 tasks. Mean reward: -539.0261840820312. Mean MSE: 24.97 || 8.43 || 2.44 || 1.61\n",
      "Meta-testing...\n",
      "Reward : 6.902151669999999\n",
      "Evaluation using 32 tasks. Mean reward: -733.33935546875. Mean MSE: 43.75 || 14.34 || 2.40 || 1.45\n",
      "Meta-testing...\n",
      "Reward : 6.8759988199999995\n",
      "Evaluation using 32 tasks. Mean reward: -420.4824523925781. Mean MSE: 26.26 || 6.34 || 1.17 || 1.74\n",
      "Meta-testing...\n",
      "Reward : 6.868090339999998\n",
      "Evaluation using 32 tasks. Mean reward: -675.5846557617188. Mean MSE: 21.30 || 12.15 || 3.41 || 2.69\n",
      "Meta-testing...\n",
      "Reward : 7.311100270000001\n",
      "Evaluation using 32 tasks. Mean reward: -934.9107055664062. Mean MSE: 40.24 || 10.46 || 3.34 || 4.11\n",
      "Meta-testing...\n",
      "Reward : 7.25053282\n",
      "Evaluation using 32 tasks. Mean reward: -657.6488037109375. Mean MSE: 33.58 || 13.59 || 2.06 || 2.33\n",
      "Meta-testing...\n",
      "Reward : 7.472547089999999\n",
      "Evaluation using 32 tasks. Mean reward: -1472.5965576171875. Mean MSE: 61.04 || 32.12 || 7.29 || 4.57\n",
      "Meta-testing...\n",
      "Reward : 7.126643179999999\n",
      "Evaluation using 32 tasks. Mean reward: -617.8916625976562. Mean MSE: 45.79 || 14.95 || 2.51 || 1.98\n",
      "Meta-testing...\n",
      "Reward : 7.181251960000001\n",
      "Evaluation using 32 tasks. Mean reward: -822.9639282226562. Mean MSE: 42.70 || 18.07 || 2.94 || 2.09\n",
      "Meta-testing...\n",
      "Reward : 7.335974660000001\n",
      "Evaluation using 32 tasks. Mean reward: -943.9264526367188. Mean MSE: 44.68 || 18.20 || 3.51 || 2.63\n",
      "Meta-testing...\n",
      "Reward : 7.236124500000001\n",
      "Evaluation using 32 tasks. Mean reward: -575.855712890625. Mean MSE: 30.65 || 8.97 || 2.15 || 1.59\n",
      "Meta-testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward : 7.3955402800000005\n",
      "Evaluation using 32 tasks. Mean reward: -600.6283569335938. Mean MSE: 29.06 || 9.38 || 1.67 || 1.80\n",
      "Meta-testing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-216ac176f471>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                              \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgp_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                              \u001b[0mtest_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                              init_prior_test)\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\non-stationary-meta\\active_learning\\FixedIdentification.py\u001b[0m in \u001b[0;36mtrain_identification\u001b[1;34m(self, n_iter, task_generator, env_name, seed, log_dir, eval_interval, num_task_to_eval, gp_list, sw_size, test_kwargs, max_id_iteration, num_test_processes, use_env_obs, init_prior_test)\u001b[0m\n\u001b[0;32m    375\u001b[0m                                    \u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvs_kwargs_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                                    \u001b[0minit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_prior_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_env_obs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_env_obs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                                    num_eval_processes=num_test_processes, max_id_iteration=max_id_iteration)\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[0mtest_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\non-stationary-meta\\active_learning\\FixedIdentification.py\u001b[0m in \u001b[0;36mmeta_test\u001b[1;34m(self, gp_list, sw_size, env_name, seed, log_dir, envs_kwargs_list, init_prior, use_env_obs, num_eval_processes, max_id_iteration)\u001b[0m\n\u001b[0;32m    518\u001b[0m                     obs = al_augment_obs(obs, self.latent_dim, posterior,\n\u001b[0;32m    519\u001b[0m                                          \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrescale_obs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrescale_obs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m                                          max_old=self.max_old, min_old=self.min_old)\n\u001b[0m\u001b[0;32m    521\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_id_iteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\non-stationary-meta\\active_learning\\observation_utils.py\u001b[0m in \u001b[0;36mal_augment_obs\u001b[1;34m(obs, latent_dim, posterior, prior, rescale_obs, max_old, min_old)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrescale_obs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mposterior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrescale_posterior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposterior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_old\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_old\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_old\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mprior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrescale_posterior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_old\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_old\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_old\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\non-stationary-meta\\active_learning\\observation_utils.py\u001b[0m in \u001b[0;36mrescale_posterior\u001b[1;34m(num_proc, old_var, latent_dim, max_old, min_old, verbose)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_old\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_old\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mold_var\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmax_old\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gp_list, test_kwargs, init_prior_test = get_task_sequence(alpha=0.25,\n",
    "                                                          n_restarts=1,\n",
    "                                                          num_test_processes=2)\n",
    "\n",
    "r = agent.train_identification(5000, task_generator, \"gauss-v0\", 0, \".\",\n",
    "                             20, 32, gp_list, 20,\n",
    "                             test_kwargs, 30, 2, False,\n",
    "                             init_prior_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.actor_critic_optimal, \"actor_critic_optimal\")\n",
    "torch.save(agent.actor_critic_identification, \"actor_critic_identification\")\n",
    "torch.save(agent.vae, \"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "temp = [r[0][i][0] for i in range(len(r[0]))]\n",
    "y = np.array(temp)\n",
    "x = np.arange(len(y))\n",
    "yhat = savgol_filter(y,31, 3) \n",
    "\n",
    "plt.plot(x, yhat, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from active_learning.training_utils import get_reward\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from active_learning.observation_utils import augment_obs_posterior, get_posterior_no_prev, al_augment_obs\n",
    "from network.vae_utils import loss_inference_closed_form\n",
    "from ppo_a2c.algo.ppo import PPO\n",
    "from ppo_a2c.envs import make_vec_envs_multi_task\n",
    "from ppo_a2c.model import MLPBase, Policy\n",
    "from ppo_a2c.storage import RolloutStorage\n",
    "\n",
    "def train_identification(agent, n_iter, task_generator, env_name, seed, log_dir,\n",
    "                         eval_interval, num_task_to_eval, gp_list, sw_size,\n",
    "                         test_kwargs, max_id_iteration, num_test_processes, use_env_obs,\n",
    "                         init_prior_test):\n",
    "    eval_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    for k in range(n_iter):\n",
    "        envs_kwargs, _, prior_list, new_tasks = task_generator.sample_pair_tasks(agent.num_processes)\n",
    "\n",
    "        envs = make_vec_envs_multi_task(env_name,\n",
    "                                        seed,\n",
    "                                        agent.num_processes,\n",
    "                                        None,\n",
    "                                        log_dir,\n",
    "                                        agent.device,\n",
    "                                        False,\n",
    "                                        envs_kwargs,\n",
    "                                        num_frame_stack=None)\n",
    "\n",
    "        obs = envs.reset()\n",
    "        obs = al_augment_obs(obs=obs, latent_dim=agent.latent_dim, posterior=prior_list,\n",
    "                             prior=prior_list,\n",
    "                             rescale_obs=agent.rescale_obs, max_old=agent.max_old,\n",
    "                             min_old=agent.min_old)\n",
    "        \n",
    "        rollouts = RolloutStorage(agent.num_steps_id, agent.num_processes,\n",
    "                                  agent.obs_shape_id, agent.action_space,\n",
    "                                  agent.actor_critic_identification.recurrent_hidden_state_size)\n",
    "\n",
    "        rollouts.obs[0].copy_(obs)\n",
    "        rollouts.to(agent.device)\n",
    "\n",
    "        use_prev_state = False\n",
    "\n",
    "        # Collect observations and store them into the storage\n",
    "        for step in range(agent.num_steps_id):\n",
    "            # Sample actions\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = agent.actor_critic_identification.act(\n",
    "                    rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "            # Observe reward and next obs\n",
    "            obs, reward, done, infos = envs.step(action)\n",
    "            posterior = get_posterior_no_prev(agent.vae, action, reward, prior_list,\n",
    "                                              use_prev_state=use_prev_state, max_action=agent.max_action,\n",
    "                                              min_action=agent.min_action)\n",
    "            action_list.append(action)\n",
    "            reward_list.append(reward)\n",
    "            use_prev_state = True\n",
    "            reward = get_reward(posterior, new_tasks, agent.latent_dim, agent.num_processes)\n",
    "\n",
    "            obs = al_augment_obs(obs=obs, latent_dim=agent.latent_dim, posterior=posterior,\n",
    "                                 prior=prior_list,\n",
    "                                 rescale_obs=agent.rescale_obs, max_old=agent.max_old,\n",
    "                                 min_old=agent.min_old)\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            if done.any():\n",
    "                use_prev_state = False\n",
    "            masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "            bad_masks = torch.FloatTensor(\n",
    "                [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "                 for info in infos])\n",
    "\n",
    "            rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                            action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "        return action_list, reward_list, new_tasks[0], prior_list[0]\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.actor_critic_identification.get_value(\n",
    "                rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "                rollouts.masks[-1]).detach()\n",
    "\n",
    "        rollouts.compute_returns(next_value, agent.use_gae, agent.gamma_identification,\n",
    "                                 agent.gae_lambda, agent.use_proper_time_limits)\n",
    "\n",
    "        _, _, _ = agent.agent_identification.update(rollouts)\n",
    "\n",
    "        rollouts.after_update()\n",
    "\n",
    "        if eval_interval is not None and k % eval_interval == 0 and k > 1:\n",
    "            e = agent.evaluate_identification(num_task_to_eval=num_task_to_eval, task_generator=task_generator\n",
    "                                             , seed=seed, env_name=env_name, log_dir=log_dir)\n",
    "            eval_list.append(e)\n",
    "\n",
    "            e = agent.meta_test(gp_list=gp_list, sw_size=sw_size, env_name=env_name, seed=seed,\n",
    "                               log_dir=log_dir, envs_kwargs_list=test_kwargs,\n",
    "                               init_prior=init_prior_test, use_env_obs=use_env_obs,\n",
    "                               num_eval_processes=num_test_processes, max_id_iteration=max_id_iteration)\n",
    "            test_list.append(e)\n",
    "\n",
    "    return eval_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.37789154052734"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, r, task, prior = train_identification(agent, 1, task_generator, \"gauss-v0\", 0, \".\",\n",
    "                             20, 32, gp_list, 20,\n",
    "                             test_kwargs, 30, 2, False,\n",
    "                             init_prior_test)\n",
    "r[0] = (100 - (-100)) / (1 - (-1)) * (action[i] - 1) + max_action\n",
    "context = torch.cat([a[0], r[0]], 1)\n",
    "context = context.reshape(1, context.shape[0], 2)\n",
    "mean = agent.vae(context=context, prior=prior.flatten().reshape(1, 2))[1]\n",
    "F.mse_loss(task, mean[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.7576)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(task, prior[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1619)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34.8220])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8868500619661064\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "m = 0\n",
    "tot = 100\n",
    "for _ in range(tot):\n",
    "    data, _, prior, new = task_generator.sample_pair_tasks_data_loader(1)\n",
    "    data = data[0][0]['train']\n",
    "    y = data[1]\n",
    "    idx = torch.where(y < 1)[0]\n",
    "    y = y[idx]\n",
    "    x = data[0][idx]\n",
    "    x = x[0:30, :]\n",
    "    y = y[0:30, :]\n",
    "    context = torch.cat([x, y], 1)\n",
    "    context = context.reshape(1, context.shape[0], 2)\n",
    "    mean = agent.vae(context=context, prior=prior[0].flatten().reshape(1, 2))[1]\n",
    "    m += F.mse_loss(new, mean).item()\n",
    "print(m / tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
