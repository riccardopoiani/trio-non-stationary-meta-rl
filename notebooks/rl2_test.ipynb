{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Product, ConstantKernel as C\n",
    "\n",
    "import gym_sin\n",
    "from gym import spaces\n",
    "\n",
    "from active_learning.recurrent import RL2\n",
    "from task.GuassianTaskGenerator import GaussianTaskGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_const_task_sequence(alpha, n_restarts, num_test_processes):\n",
    "    kernel = C(1.0, (1e-5, 1e5)) * RBF(1, (1e-5, 1e5))\n",
    "\n",
    "    gp_list = []\n",
    "    for i in range(2):\n",
    "        gp_list.append([GaussianProcessRegressor(kernel=kernel,\n",
    "                                                 alpha=alpha ** 2,\n",
    "                                                 normalize_y=True,\n",
    "                                                 n_restarts_optimizer=n_restarts)\n",
    "                        for _ in range(num_test_processes)])\n",
    "    test_kwargs = []\n",
    "    init_prior_test = [torch.tensor([[-10], [5]], dtype=torch.float32) for _ in range(num_test_processes)]\n",
    "        \n",
    "    mean = -5\n",
    "    std = 15\n",
    "    \n",
    "    for idx in range(50):\n",
    "        test_kwargs.append({'amplitude': 1,\n",
    "                            'mean': mean,\n",
    "                            'std': std,\n",
    "                            'noise_std': 0.001,\n",
    "                            'scale_reward': False})\n",
    "\n",
    "    return gp_list, test_kwargs, init_prior_test\n",
    "\n",
    "def get_linear_task_sequence(alpha, n_restarts, num_test_processes):\n",
    "    kernel = C(1.0, (1e-5, 1e5)) * RBF(1, (1e-5, 1e5))\n",
    "\n",
    "    gp_list = []\n",
    "    for i in range(2):\n",
    "        gp_list.append([GaussianProcessRegressor(kernel=kernel,\n",
    "                                                 alpha=alpha ** 2,\n",
    "                                                 normalize_y=True,\n",
    "                                                 n_restarts_optimizer=n_restarts)\n",
    "                        for _ in range(num_test_processes)])\n",
    "    test_kwargs = []\n",
    "    init_prior_test = [torch.tensor([[30], [5]], dtype=torch.float32) for _ in range(num_test_processes)]\n",
    "\n",
    "    for idx in range(50):\n",
    "        std = 15\n",
    "        mean = 30 - idx\n",
    "        \n",
    "        test_kwargs.append({'amplitude': 1,\n",
    "                            'mean': mean,\n",
    "                            'std': std,\n",
    "                            'noise_std': 0.001,\n",
    "                            'scale_reward': False})\n",
    "\n",
    "    return gp_list, test_kwargs, init_prior_test\n",
    "\n",
    "def get_phase_task_sequence(alpha, n_restarts, num_test_processes):\n",
    "    kernel = C(1.0, (1e-5, 1e5)) * RBF(1, (1e-5, 1e5))\n",
    "\n",
    "    gp_list = []\n",
    "    for i in range(2):\n",
    "        gp_list.append([GaussianProcessRegressor(kernel=kernel,\n",
    "                                                 alpha=alpha ** 2,\n",
    "                                                 normalize_y=True,\n",
    "                                                 n_restarts_optimizer=n_restarts)\n",
    "                        for _ in range(num_test_processes)])\n",
    "    test_kwargs = []\n",
    "    init_prior_test = [torch.tensor([[-5], [5]], dtype=torch.float32) for _ in range(num_test_processes)]\n",
    "\n",
    "    for idx in range(50):\n",
    "        if idx < 15:\n",
    "            std = 15\n",
    "            mean = 0\n",
    "        elif idx < 30:\n",
    "            std = 15\n",
    "            mean = 10\n",
    "        else:\n",
    "            std = 15\n",
    "            mean = 0\n",
    "            \n",
    "        test_kwargs.append({'amplitude': 1,\n",
    "                            'mean': mean,\n",
    "                            'std': std,\n",
    "                            'noise_std': 0.001,\n",
    "                            'scale_reward': False})\n",
    "\n",
    "    return gp_list, test_kwargs, init_prior_test\n",
    "\n",
    "def get_abrupt_and_smooth(alpha, n_restarts, num_test_processes):\n",
    "    kernel = C(1.0, (1e-5, 1e5)) * RBF(1, (1e-5, 1e5))\n",
    "\n",
    "    gp_list = []\n",
    "    for i in range(2):\n",
    "        gp_list.append([GaussianProcessRegressor(kernel=kernel,\n",
    "                                                 alpha=alpha ** 2,\n",
    "                                                 normalize_y=True,\n",
    "                                                 n_restarts_optimizer=n_restarts)\n",
    "                        for _ in range(num_test_processes)])\n",
    "    test_kwargs = []\n",
    "    init_prior_test = [torch.tensor([[-30], [5]], dtype=torch.float32) for _ in range(num_test_processes)]\n",
    "\n",
    "    for idx in range(80):\n",
    "        if idx < 15:\n",
    "            std = 15\n",
    "            mean = -30\n",
    "        elif idx < 50:\n",
    "            std = 15\n",
    "            mean = -20 + (idx - 15)\n",
    "        else:\n",
    "            std = 15\n",
    "            mean = -20 + 50 - 15 \n",
    "\n",
    "            \n",
    "        test_kwargs.append({'amplitude': 1,\n",
    "                            'mean': mean,\n",
    "                            'std': std,\n",
    "                            'noise_std': 0.001,\n",
    "                            'scale_reward': False})\n",
    "\n",
    "    return gp_list, test_kwargs, init_prior_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"gauss-v0\"\n",
    "\n",
    "action_space = spaces.Box(low=np.array([-1]), high=np.array([1]))\n",
    "latent_dim = 1\n",
    "\n",
    "x_min = -100\n",
    "x_max = 100\n",
    "\n",
    "min_mean = -40\n",
    "max_mean = 40\n",
    "\n",
    "prior_mu_min = -10\n",
    "prior_mu_max = 10\n",
    "prior_std_min = 1\n",
    "prior_std_max = 10\n",
    "\n",
    "std = 15\n",
    "amplitude=1\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "task_generator = GaussianTaskGenerator(x_min, x_max, min_mean, max_mean,\n",
    "                 prior_mu_min, prior_mu_max, prior_std_min, prior_std_max, std, amplitude)\n",
    "fam = task_generator.create_task_family(n_tasks=5000, n_batches=1, test_perc=0, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "use_elu = True\n",
    "clip_param = 0.2\n",
    "ppo_epoch = 4 \n",
    "num_mini_batch = 8\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.\n",
    "lr = 0.0001\n",
    "eps = 1e-6\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "use_obs_env = False\n",
    "obs_shape = (2,)\n",
    "num_processes = 32\n",
    "gamma = 1\n",
    "device = \"cpu\"\n",
    "num_steps = 150\n",
    "action_dim = 1\n",
    "use_gae = False\n",
    "gae_lambda = 0.95\n",
    "use_proper_time_limits = False\n",
    "\n",
    "agent = RL2(hidden_size, use_elu, clip_param, ppo_epoch, num_mini_batch, value_loss_coef,\n",
    "                 entropy_coef, lr, eps, max_grad_norm, action_space, obs_shape, use_obs_env,\n",
    "                 num_processes, gamma, device, num_steps, action_dim, use_gae, gae_lambda,\n",
    "                 use_proper_time_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5000\n",
    "env_name = 'gauss-v0'\n",
    "seed = 0\n",
    "eval_interval = 20\n",
    "num_test_processes = 1\n",
    "num_random_task_to_eval = 32\n",
    "\n",
    "test_kwargs_sequences = [get_const_task_sequence(0.25, 1, 1)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 15.670923062499998\n",
      "Iteration 20 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 14.608395156250001\n",
      "Iteration 40 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 15.230333593749998\n",
      "Iteration 60 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.773078062499998\n",
      "Iteration 80 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.14672053125\n",
      "Iteration 100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.49008359375\n",
      "Iteration 120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.240571125\n",
      "Iteration 140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.36997071875\n",
      "Iteration 160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 16.21297646875\n",
      "Iteration 180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 15.97599171875\n",
      "Iteration 200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 17.0086595\n",
      "Iteration 220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 17.51644803125\n",
      "Iteration 240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 18.59178071875\n",
      "Iteration 260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 18.65666346875\n",
      "Iteration 280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 18.08603175\n",
      "Iteration 300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 19.159381187500003\n",
      "Iteration 320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 19.723207343749998\n",
      "Iteration 340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 18.695997187499998\n",
      "Iteration 360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 19.86492728125\n",
      "Iteration 380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 19.904114375\n",
      "Iteration 400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 20.254894874999998\n",
      "Iteration 420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 21.3034211875\n",
      "Iteration 440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 21.8581611875\n",
      "Iteration 460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 21.013406687499998\n",
      "Iteration 480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 22.187716906250003\n",
      "Iteration 500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 21.8604529375\n",
      "Iteration 520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 23.0939154375\n",
      "Iteration 540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 22.828444500000003\n",
      "Iteration 560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 23.944782\n",
      "Iteration 580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 23.749503562500003\n",
      "Iteration 600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 25.17963465625\n",
      "Iteration 620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 26.753435125\n",
      "Iteration 640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 25.805157656250003\n",
      "Iteration 660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 25.229447843750002\n",
      "Iteration 680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 25.742457281249997\n",
      "Iteration 700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 27.646725593750002\n",
      "Iteration 720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 27.229386750000003\n",
      "Iteration 740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 27.53533528125\n",
      "Iteration 760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 28.7245271875\n",
      "Iteration 780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 29.1739689375\n",
      "Iteration 800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 30.262900656249997\n",
      "Iteration 820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 31.31094840625\n",
      "Iteration 840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 32.07612215625\n",
      "Iteration 860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 30.549744687500002\n",
      "Iteration 880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 31.39409965625\n",
      "Iteration 900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 32.693406874999994\n",
      "Iteration 920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.43253290625\n",
      "Iteration 940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.89406284375001\n",
      "Iteration 960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 32.11953234375\n",
      "Iteration 980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.650701312500004\n",
      "Iteration 1000 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 35.01583075\n",
      "Iteration 1020 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.46167696875\n",
      "Iteration 1040 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 34.02064403125\n",
      "Iteration 1060 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.514861875\n",
      "Iteration 1080 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 35.373638031249996\n",
      "Iteration 1100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 36.229014125000006\n",
      "Iteration 1120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 37.96180746875\n",
      "Iteration 1140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 36.9344866875\n",
      "Iteration 1160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 33.921327625\n",
      "Iteration 1180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 37.223309218749996\n",
      "Iteration 1200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.93571146875\n",
      "Iteration 1220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 37.45383240625\n",
      "Iteration 1240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.667463125\n",
      "Iteration 1260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 41.24768615625\n",
      "Iteration 1280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.07127621875\n",
      "Iteration 1300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.665759249999994\n",
      "Iteration 1320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 41.47184521875\n",
      "Iteration 1340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.639899125\n",
      "Iteration 1360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.7012814375\n",
      "Iteration 1380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 41.53686503125\n",
      "Iteration 1400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.11459025\n",
      "Iteration 1420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 40.15162121875\n",
      "Iteration 1440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.04005740625\n",
      "Iteration 1460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.0516003125\n",
      "Iteration 1480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.68011046875\n",
      "Iteration 1500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.71340478125\n",
      "Iteration 1520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 40.2150811875\n",
      "Iteration 1540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.66415281250001\n",
      "Iteration 1560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 47.815216093749996\n",
      "Iteration 1580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 43.95937346875\n",
      "Iteration 1600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 45.67893403125\n",
      "Iteration 1620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.64332590625\n",
      "Iteration 1640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.58713228125\n",
      "Iteration 1660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.80468171875\n",
      "Iteration 1680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.8433246875\n",
      "Iteration 1700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 44.627934187499996\n",
      "Iteration 1720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 45.14836534375\n",
      "Iteration 1740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 47.69776328125\n",
      "Iteration 1760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.8462454375\n",
      "Iteration 1780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 47.348065375000004\n",
      "Iteration 1800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.74047378125\n",
      "Iteration 1820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.377044437500004\n",
      "Iteration 1840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 43.1773663125\n",
      "Iteration 1860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 35.3314848125\n",
      "Iteration 1880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 36.31095059375\n",
      "Iteration 1900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 41.91116821875\n",
      "Iteration 1920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.3530233125\n",
      "Iteration 1940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.7109555\n",
      "Iteration 1960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.76585209375\n",
      "Iteration 1980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 38.54249703125\n",
      "Iteration 2000 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.63739996875\n",
      "Iteration 2020 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 39.1573235\n",
      "Iteration 2040 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 40.852053187500005\n",
      "Iteration 2060 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 42.39478615625001\n",
      "Iteration 2080 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 43.4001571875\n",
      "Iteration 2100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 49.319577968749996\n",
      "Iteration 2120 / 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation using 32 tasks. Mean reward: 48.57933953125\n",
      "Iteration 2140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 48.77452446875\n",
      "Iteration 2160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 45.80615190625\n",
      "Iteration 2180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 45.7687906875\n",
      "Iteration 2200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 47.30917303125\n",
      "Iteration 2220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 44.666329968750006\n",
      "Iteration 2240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 51.202162875\n",
      "Iteration 2260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.778560562500004\n",
      "Iteration 2280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 37.2735596875\n",
      "Iteration 2300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 49.693194625\n",
      "Iteration 2320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 49.9084514375\n",
      "Iteration 2340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 50.341419218750005\n",
      "Iteration 2360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 47.493546656250004\n",
      "Iteration 2380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 46.63868325\n",
      "Iteration 2400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 50.066070656250005\n",
      "Iteration 2420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 48.47205765625\n",
      "Iteration 2440 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 51.741834656250006\n",
      "Iteration 2460 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 56.278559781249996\n",
      "Iteration 2480 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 51.767054062499994\n",
      "Iteration 2500 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 59.375178625000004\n",
      "Iteration 2520 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 47.03822940625\n",
      "Iteration 2540 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 57.0191215\n",
      "Iteration 2560 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 53.970745593749996\n",
      "Iteration 2580 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 55.38363259375001\n",
      "Iteration 2600 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 58.25355596875\n",
      "Iteration 2620 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 55.3169318125\n",
      "Iteration 2640 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 52.66851446875\n",
      "Iteration 2660 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 61.46148528125\n",
      "Iteration 2680 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 57.97469140625\n",
      "Iteration 2700 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 61.43852909375\n",
      "Iteration 2720 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 59.98892715625\n",
      "Iteration 2740 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 61.539898468749996\n",
      "Iteration 2760 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 63.21835865625\n",
      "Iteration 2780 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 66.79365562500001\n",
      "Iteration 2800 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 65.43477071875\n",
      "Iteration 2820 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 55.725316875000004\n",
      "Iteration 2840 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 63.02252078125\n",
      "Iteration 2860 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 72.577554125\n",
      "Iteration 2880 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 64.281641875\n",
      "Iteration 2900 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 64.4840848125\n",
      "Iteration 2920 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 66.46071171874999\n",
      "Iteration 2940 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 71.62326884375\n",
      "Iteration 2960 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 72.12193790625\n",
      "Iteration 2980 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 70.11739356250001\n",
      "Iteration 3000 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 68.88488118750001\n",
      "Iteration 3020 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 69.658327125\n",
      "Iteration 3040 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 75.34246040625\n",
      "Iteration 3060 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 75.81829109374999\n",
      "Iteration 3080 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 73.65598184375\n",
      "Iteration 3100 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 71.79275159375\n",
      "Iteration 3120 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 71.551185125\n",
      "Iteration 3140 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 74.60081418749999\n",
      "Iteration 3160 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 69.426722\n",
      "Iteration 3180 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 82.8424321875\n",
      "Iteration 3200 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 74.74078859375\n",
      "Iteration 3220 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 79.59351259375\n",
      "Iteration 3240 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 66.04587940625\n",
      "Iteration 3260 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 82.3129625\n",
      "Iteration 3280 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 81.87746153124999\n",
      "Iteration 3300 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 67.73805956250001\n",
      "Iteration 3320 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 76.69167071875\n",
      "Iteration 3340 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 73.95880221875001\n",
      "Iteration 3360 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 78.84325959374999\n",
      "Iteration 3380 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 82.46552096875\n",
      "Iteration 3400 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 82.206163\n",
      "Iteration 3420 / 5000\n",
      "Evaluation using 32 tasks. Mean reward: 84.9297163125\n"
     ]
    }
   ],
   "source": [
    "eval_list, test_list = agent.train(n_iter, env_name, seed, task_generator,\n",
    "            eval_interval, num_test_processes, num_random_task_to_eval,\n",
    "            test_kwargs_sequences=test_kwargs_sequences, log_dir=\".\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
