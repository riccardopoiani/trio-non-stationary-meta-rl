{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_sin\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "import gym\n",
    "import gym_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "from baselines import bench\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from baselines.common.vec_env import VecEnvWrapper\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from baselines.common.vec_env.shmem_vec_env import ShmemVecEnv\n",
    "from baselines.common.vec_env.vec_normalize import \\\n",
    "    VecNormalize as VecNormalize_\n",
    "\n",
    "try:\n",
    "    import dm_control2gym\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import roboschool\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import pybullet_envs\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, rank, log_dir, allow_early_resets):\n",
    "    def _thunk():\n",
    "        if env_id.startswith(\"dm\"):\n",
    "            _, domain, task = env_id.split('.')\n",
    "            env = dm_control2gym.make(domain_name=domain, task_name=task)\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "\n",
    "        is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "            env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "        if is_atari:\n",
    "            env = make_atari(env_id)\n",
    "\n",
    "        env.seed(seed + rank)\n",
    "\n",
    "        if str(env.__class__.__name__).find('TimeLimit') >= 0:\n",
    "            env = TimeLimitMask(env)\n",
    "\n",
    "        if log_dir is not None:\n",
    "            env = bench.Monitor(\n",
    "                env,\n",
    "                os.path.join(log_dir, str(rank)),\n",
    "                allow_early_resets=allow_early_resets)\n",
    "\n",
    "        if is_atari:\n",
    "            if len(env.observation_space.shape) == 3:\n",
    "                env = wrap_deepmind(env)\n",
    "        elif len(env.observation_space.shape) == 3:\n",
    "            raise NotImplementedError(\n",
    "                \"CNN models work only for atari,\\n\"\n",
    "                \"please use a custom wrapper for a custom pixel input env.\\n\"\n",
    "                \"See wrap_deepmind for an example.\")\n",
    "\n",
    "        # If the input has shape (W,H,3), wrap for PyTorch convolutions\n",
    "        obs_shape = env.observation_space.shape\n",
    "        if len(obs_shape) == 3 and obs_shape[2] in [1, 3]:\n",
    "            env = TransposeImage(env, op=[2, 0, 1])\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "\n",
    "def make_vec_envs(env_name,\n",
    "                  seed,\n",
    "                  num_processes,\n",
    "                  gamma,\n",
    "                  log_dir,\n",
    "                  device,\n",
    "                  allow_early_resets,\n",
    "                  num_frame_stack=None):\n",
    "    envs = [\n",
    "        make_env(env_name, seed, i, log_dir, allow_early_resets)\n",
    "        for i in range(num_processes)\n",
    "    ]\n",
    "    \n",
    "    if len(envs) > 1:\n",
    "        n = len(envs)\n",
    "        envs = DummyVecEnv(envs)\n",
    "        # envs = ShmemVecEnv(envs)\n",
    "    else:\n",
    "        envs = DummyVecEnv(envs)\n",
    "\n",
    "    if len(envs.observation_space.shape) == 1:\n",
    "        if gamma is None:\n",
    "            envs = VecNormalize(envs, ret=False)\n",
    "        else:\n",
    "            envs = VecNormalize(envs, gamma=gamma)\n",
    "\n",
    "    envs = VecPyTorch(envs, device)\n",
    "\n",
    "    if num_frame_stack is not None:\n",
    "        envs = VecPyTorchFrameStack(envs, num_frame_stack, device)\n",
    "    elif len(envs.observation_space.shape) == 3:\n",
    "        envs = VecPyTorchFrameStack(envs, 4, device)\n",
    "\n",
    "    return envs\n",
    "\n",
    "\n",
    "# Checks whether done was caused my timit limits or not\n",
    "class TimeLimitMask(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        if done and self.env._max_episode_steps == self.env._elapsed_steps:\n",
    "            info['bad_transition'] = True\n",
    "\n",
    "        return obs, rew, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# Can be used to test recurrent policies for Reacher-v2\n",
    "class MaskGoal(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        if self.env._elapsed_steps > 0:\n",
    "            observation[-2:] = 0\n",
    "        return observation\n",
    "\n",
    "\n",
    "class TransposeObs(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"\n",
    "        Transpose observation space (base class)\n",
    "        \"\"\"\n",
    "        super(TransposeObs, self).__init__(env)\n",
    "\n",
    "\n",
    "class TransposeImage(TransposeObs):\n",
    "    def __init__(self, env=None, op=[2, 0, 1]):\n",
    "        \"\"\"\n",
    "        Transpose observation space for images\n",
    "        \"\"\"\n",
    "        super(TransposeImage, self).__init__(env)\n",
    "        assert len(op) == 3, \"Error: Operation, \" + str(op) + \", must be dim3\"\n",
    "        self.op = op\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0, 0, 0],\n",
    "            self.observation_space.high[0, 0, 0], [\n",
    "                obs_shape[self.op[0]], obs_shape[self.op[1]],\n",
    "                obs_shape[self.op[2]]\n",
    "            ],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, ob):\n",
    "        return ob.transpose(self.op[0], self.op[1], self.op[2])\n",
    "\n",
    "\n",
    "class VecPyTorch(VecEnvWrapper):\n",
    "    def __init__(self, venv, device):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(VecPyTorch, self).__init__(venv)\n",
    "        self.device = device\n",
    "        # TODO: Fix data types\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.venv.reset()\n",
    "        obs = torch.from_numpy(obs).float().to(self.device)\n",
    "        return obs\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        if isinstance(actions, torch.LongTensor):\n",
    "            # Squeeze the dimension for discrete actions\n",
    "            actions = actions.squeeze(1)\n",
    "        actions = actions.cpu().numpy()\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        obs, reward, done, info = self.venv.step_wait()\n",
    "        obs = torch.from_numpy(obs).float().to(self.device)\n",
    "        reward = torch.from_numpy(reward).unsqueeze(dim=1).float()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "class VecNormalize(VecNormalize_):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(VecNormalize, self).__init__(*args, **kwargs)\n",
    "        self.training = True\n",
    "\n",
    "    def _obfilt(self, obs, update=True):\n",
    "        if self.ob_rms:\n",
    "            if self.training and update:\n",
    "                self.ob_rms.update(obs)\n",
    "            obs = np.clip((obs - self.ob_rms.mean) /\n",
    "                          np.sqrt(self.ob_rms.var + self.epsilon),\n",
    "                          -self.clipob, self.clipob)\n",
    "            return obs\n",
    "        else:\n",
    "            return obs\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "\n",
    "\n",
    "# Derived from\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/vec_env/vec_frame_stack.py\n",
    "class VecPyTorchFrameStack(VecEnvWrapper):\n",
    "    def __init__(self, venv, nstack, device=None):\n",
    "        self.venv = venv\n",
    "        self.nstack = nstack\n",
    "\n",
    "        wos = venv.observation_space  # wrapped ob space\n",
    "        self.shape_dim0 = wos.shape[0]\n",
    "\n",
    "        low = np.repeat(wos.low, self.nstack, axis=0)\n",
    "        high = np.repeat(wos.high, self.nstack, axis=0)\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        self.stacked_obs = torch.zeros((venv.num_envs, ) +\n",
    "                                       low.shape).to(device)\n",
    "\n",
    "        observation_space = gym.spaces.Box(\n",
    "            low=low, high=high, dtype=venv.observation_space.dtype)\n",
    "        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n",
    "\n",
    "    def step_wait(self):\n",
    "        obs, rews, news, infos = self.venv.step_wait()\n",
    "        self.stacked_obs[:, :-self.shape_dim0] = \\\n",
    "            self.stacked_obs[:, self.shape_dim0:].clone()\n",
    "        for (i, new) in enumerate(news):\n",
    "            if new:\n",
    "                self.stacked_obs[i] = 0\n",
    "        self.stacked_obs[:, -self.shape_dim0:] = obs\n",
    "        return self.stacked_obs, rews, news, infos\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.venv.reset()\n",
    "        if torch.backends.cudnn.deterministic:\n",
    "            self.stacked_obs = torch.zeros(self.stacked_obs.shape)\n",
    "        else:\n",
    "            self.stacked_obs.zero_()\n",
    "        self.stacked_obs[:, -self.shape_dim0:] = obs\n",
    "        return self.stacked_obs\n",
    "\n",
    "    def close(self):\n",
    "        self.venv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Get a render function\n",
    "def get_render_func(venv):\n",
    "    if hasattr(venv, 'envs'):\n",
    "        return venv.envs[0].render\n",
    "    elif hasattr(venv, 'venv'):\n",
    "        return get_render_func(venv.venv)\n",
    "    elif hasattr(venv, 'env'):\n",
    "        return get_render_func(venv.env)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_vec_normalize(venv):\n",
    "    if isinstance(venv, VecNormalize):\n",
    "        return venv\n",
    "    elif hasattr(venv, 'venv'):\n",
    "        return get_vec_normalize(venv.venv)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Necessary for my KFAC implementation.\n",
    "class AddBias(torch.nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super(AddBias, self).__init__()\n",
    "        self._bias = torch.nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            bias = self._bias.t().view(1, -1)\n",
    "        else:\n",
    "            bias = self._bias.t().view(1, -1, 1, 1)\n",
    "\n",
    "        return x + bias\n",
    "\n",
    "\n",
    "def update_linear_schedule(optimizer, epoch, total_num_epochs, initial_lr):\n",
    "    \"\"\"Decreases the learning rate linearly\"\"\"\n",
    "    lr = initial_lr - (initial_lr * (epoch / float(total_num_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "\n",
    "def cleanup_log_dir(log_dir):\n",
    "    try:\n",
    "        os.makedirs(log_dir)\n",
    "    except OSError:\n",
    "        files = glob.glob(os.path.join(log_dir, '*.monitor.csv'))\n",
    "        for f in files:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "def _flatten_helper(T, N, _tensor):\n",
    "    return _tensor.view(T * N, *_tensor.size()[2:])\n",
    "\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space,\n",
    "                 recurrent_hidden_state_size):\n",
    "        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.recurrent_hidden_states = torch.zeros(\n",
    "            num_steps + 1, num_processes, recurrent_hidden_state_size)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            action_shape = 1\n",
    "        else:\n",
    "            action_shape = action_space.shape[0]\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        # Masks that indicate whether it's a true terminal state\n",
    "        # or time limit end state\n",
    "        self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.step = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.obs = self.obs.to(device)\n",
    "        self.recurrent_hidden_states = self.recurrent_hidden_states.to(device)\n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.value_preds = self.value_preds.to(device)\n",
    "        self.returns = self.returns.to(device)\n",
    "        self.action_log_probs = self.action_log_probs.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "        self.bad_masks = self.bad_masks.to(device)\n",
    "\n",
    "    def insert(self, obs, recurrent_hidden_states, actions, action_log_probs,\n",
    "               value_preds, rewards, masks, bad_masks):\n",
    "        self.obs[self.step + 1].copy_(obs)\n",
    "        self.recurrent_hidden_states[self.step +\n",
    "                                     1].copy_(recurrent_hidden_states)\n",
    "        self.actions[self.step].copy_(actions)\n",
    "        self.action_log_probs[self.step].copy_(action_log_probs)\n",
    "        self.value_preds[self.step].copy_(value_preds)\n",
    "        self.rewards[self.step].copy_(rewards)\n",
    "        self.masks[self.step + 1].copy_(masks)\n",
    "        self.bad_masks[self.step + 1].copy_(bad_masks)\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def after_update(self):\n",
    "        self.obs[0].copy_(self.obs[-1])\n",
    "        self.recurrent_hidden_states[0].copy_(self.recurrent_hidden_states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        self.bad_masks[0].copy_(self.bad_masks[-1])\n",
    "\n",
    "    def compute_returns(self,\n",
    "                        next_value,\n",
    "                        use_gae,\n",
    "                        gamma,\n",
    "                        gae_lambda,\n",
    "                        use_proper_time_limits=True):\n",
    "        if use_proper_time_limits:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    gae = gae * self.bad_masks[step + 1]\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = (self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]) * self.bad_masks[step + 1] \\\n",
    "                        + (1 - self.bad_masks[step + 1]) * self.value_preds[step]\n",
    "        else:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]\n",
    "\n",
    "    def feed_forward_generator(self,\n",
    "                               advantages,\n",
    "                               num_mini_batch=None,\n",
    "                               mini_batch_size=None):\n",
    "        num_steps, num_processes = self.rewards.size()[0:2]\n",
    "        batch_size = num_processes * num_steps\n",
    "\n",
    "        if mini_batch_size is None:\n",
    "            assert batch_size >= num_mini_batch, (\n",
    "                \"PPO requires the number of processes ({}) \"\n",
    "                \"* number of steps ({}) = {} \"\n",
    "                \"to be greater than or equal to the number of PPO mini batches ({}).\"\n",
    "                \"\".format(num_processes, num_steps, num_processes * num_steps,\n",
    "                          num_mini_batch))\n",
    "            mini_batch_size = batch_size // num_mini_batch\n",
    "        sampler = BatchSampler(\n",
    "            SubsetRandomSampler(range(batch_size)),\n",
    "            mini_batch_size,\n",
    "            drop_last=True)\n",
    "        for indices in sampler:\n",
    "            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]\n",
    "            recurrent_hidden_states_batch = self.recurrent_hidden_states[:-1].view(\n",
    "                -1, self.recurrent_hidden_states.size(-1))[indices]\n",
    "            actions_batch = self.actions.view(-1,\n",
    "                                              self.actions.size(-1))[indices]\n",
    "            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]\n",
    "            return_batch = self.returns[:-1].view(-1, 1)[indices]\n",
    "            masks_batch = self.masks[:-1].view(-1, 1)[indices]\n",
    "            old_action_log_probs_batch = self.action_log_probs.view(-1,\n",
    "                                                                    1)[indices]\n",
    "            if advantages is None:\n",
    "                adv_targ = None\n",
    "            else:\n",
    "                adv_targ = advantages.view(-1, 1)[indices]\n",
    "\n",
    "            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ\n",
    "\n",
    "    def recurrent_generator(self, advantages, num_mini_batch):\n",
    "        num_processes = self.rewards.size(1)\n",
    "        assert num_processes >= num_mini_batch, (\n",
    "            \"PPO requires the number of processes ({}) \"\n",
    "            \"to be greater than or equal to the number of \"\n",
    "            \"PPO mini batches ({}).\".format(num_processes, num_mini_batch))\n",
    "        num_envs_per_batch = num_processes // num_mini_batch\n",
    "        perm = torch.randperm(num_processes)\n",
    "        for start_ind in range(0, num_processes, num_envs_per_batch):\n",
    "            obs_batch = []\n",
    "            recurrent_hidden_states_batch = []\n",
    "            actions_batch = []\n",
    "            value_preds_batch = []\n",
    "            return_batch = []\n",
    "            masks_batch = []\n",
    "            old_action_log_probs_batch = []\n",
    "            adv_targ = []\n",
    "\n",
    "            for offset in range(num_envs_per_batch):\n",
    "                ind = perm[start_ind + offset]\n",
    "                obs_batch.append(self.obs[:-1, ind])\n",
    "                recurrent_hidden_states_batch.append(\n",
    "                    self.recurrent_hidden_states[0:1, ind])\n",
    "                actions_batch.append(self.actions[:, ind])\n",
    "                value_preds_batch.append(self.value_preds[:-1, ind])\n",
    "                return_batch.append(self.returns[:-1, ind])\n",
    "                masks_batch.append(self.masks[:-1, ind])\n",
    "                old_action_log_probs_batch.append(\n",
    "                    self.action_log_probs[:, ind])\n",
    "                adv_targ.append(advantages[:, ind])\n",
    "\n",
    "            T, N = self.num_steps, num_envs_per_batch\n",
    "            # These are all tensors of size (T, N, -1)\n",
    "            obs_batch = torch.stack(obs_batch, 1)\n",
    "            actions_batch = torch.stack(actions_batch, 1)\n",
    "            value_preds_batch = torch.stack(value_preds_batch, 1)\n",
    "            return_batch = torch.stack(return_batch, 1)\n",
    "            masks_batch = torch.stack(masks_batch, 1)\n",
    "            old_action_log_probs_batch = torch.stack(\n",
    "                old_action_log_probs_batch, 1)\n",
    "            adv_targ = torch.stack(adv_targ, 1)\n",
    "\n",
    "            # States is just a (N, -1) tensor\n",
    "            recurrent_hidden_states_batch = torch.stack(\n",
    "                recurrent_hidden_states_batch, 1).view(N, -1)\n",
    "\n",
    "            # Flatten the (T, N, ...) tensors to (T * N, ...)\n",
    "            obs_batch = _flatten_helper(T, N, obs_batch)\n",
    "            actions_batch = _flatten_helper(T, N, actions_batch)\n",
    "            value_preds_batch = _flatten_helper(T, N, value_preds_batch)\n",
    "            return_batch = _flatten_helper(T, N, return_batch)\n",
    "            masks_batch = _flatten_helper(T, N, masks_batch)\n",
    "            old_action_log_probs_batch = _flatten_helper(T, N, \\\n",
    "                    old_action_log_probs_batch)\n",
    "            adv_targ = _flatten_helper(T, N, adv_targ)\n",
    "\n",
    "            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, obs_shape, action_space, base=None, base_kwargs=None):\n",
    "        super(Policy, self).__init__()\n",
    "        if base_kwargs is None:\n",
    "            base_kwargs = {}\n",
    "        if base is None:\n",
    "            if len(obs_shape) == 3:\n",
    "                base = CNNBase\n",
    "            elif len(obs_shape) == 1:\n",
    "                base = MLPBase\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        self.base = base(obs_shape[0], **base_kwargs)\n",
    "\n",
    "        if action_space.__class__.__name__ == \"Discrete\":\n",
    "            num_outputs = action_space.n\n",
    "            self.dist = Categorical(self.base.output_size, num_outputs)\n",
    "        elif action_space.__class__.__name__ == \"Box\":\n",
    "            num_outputs = action_space.shape[0]\n",
    "            self.dist = DiagGaussian(self.base.output_size, num_outputs)\n",
    "        elif action_space.__class__.__name__ == \"MultiBinary\":\n",
    "            num_outputs = action_space.shape[0]\n",
    "            self.dist = Bernoulli(self.base.output_size, num_outputs)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self.base.is_recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        \"\"\"Size of rnn_hx.\"\"\"\n",
    "        return self.base.recurrent_hidden_state_size\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, inputs, rnn_hxs, masks, deterministic=False):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        if deterministic:\n",
    "            action = dist.mode()\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action, action_log_probs, rnn_hxs\n",
    "\n",
    "    def get_value(self, inputs, rnn_hxs, masks):\n",
    "        value, _, _ = self.base(inputs, rnn_hxs, masks)\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, inputs, rnn_hxs, masks, action):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy, rnn_hxs\n",
    "\n",
    "\n",
    "class NNBase(torch.nn.Module):\n",
    "    def __init__(self, recurrent, recurrent_input_size, hidden_size):\n",
    "        super(NNBase, self).__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        self._recurrent = recurrent\n",
    "\n",
    "        if recurrent:\n",
    "            self.gru = torch.nn.GRU(recurrent_input_size, hidden_size)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self._recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        if self._recurrent:\n",
    "            return self._hidden_size\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    def _forward_gru(self, x, hxs, masks):\n",
    "        if x.size(0) == hxs.size(0):\n",
    "            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks).unsqueeze(0))\n",
    "            x = x.squeeze(0)\n",
    "            hxs = hxs.squeeze(0)\n",
    "        else:\n",
    "            # x is a (T, N, -1) tensor that has been flatten to (T * N, -1)\n",
    "            N = hxs.size(0)\n",
    "            T = int(x.size(0) / N)\n",
    "\n",
    "            # unflatten\n",
    "            x = x.view(T, N, x.size(1))\n",
    "\n",
    "            # Same deal with masks\n",
    "            masks = masks.view(T, N)\n",
    "\n",
    "            # Let's figure out which steps in the sequence have a zero for any agent\n",
    "            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n",
    "            has_zeros = ((masks[1:] == 0.0) \\\n",
    "                            .any(dim=-1)\n",
    "                            .nonzero()\n",
    "                            .squeeze()\n",
    "                            .cpu())\n",
    "\n",
    "            # +1 to correct the masks[1:]\n",
    "            if has_zeros.dim() == 0:\n",
    "                # Deal with scalar\n",
    "                has_zeros = [has_zeros.item() + 1]\n",
    "            else:\n",
    "                has_zeros = (has_zeros + 1).numpy().tolist()\n",
    "\n",
    "            # add t=0 and t=T to the list\n",
    "            has_zeros = [0] + has_zeros + [T]\n",
    "\n",
    "            hxs = hxs.unsqueeze(0)\n",
    "            outputs = []\n",
    "            for i in range(len(has_zeros) - 1):\n",
    "                # We can now process steps that don't have any zeros in masks together!\n",
    "                # This is much faster\n",
    "                start_idx = has_zeros[i]\n",
    "                end_idx = has_zeros[i + 1]\n",
    "\n",
    "                rnn_scores, hxs = self.gru(\n",
    "                    x[start_idx:end_idx],\n",
    "                    hxs * masks[start_idx].view(1, -1, 1))\n",
    "\n",
    "                outputs.append(rnn_scores)\n",
    "\n",
    "            # assert len(outputs) == T\n",
    "            # x is a (T, N, -1) tensor\n",
    "            x = torch.cat(outputs, dim=0)\n",
    "            # flatten\n",
    "            x = x.view(T * N, -1)\n",
    "            hxs = hxs.squeeze(0)\n",
    "\n",
    "        return x, hxs\n",
    "\n",
    "\n",
    "class CNNBase(NNBase):\n",
    "    def __init__(self, num_inputs, recurrent=False, hidden_size=512):\n",
    "        super(CNNBase, self).__init__(recurrent, hidden_size, hidden_size)\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            init_(torch.nn.Conv2d(num_inputs, 32, 8, stride=4)), torch.nn.ReLU(),\n",
    "            init_(torch.nn.Conv2d(32, 64, 4, stride=2)), torch.nn.ReLU(),\n",
    "            init_(torch.nn.Conv2d(64, 32, 3, stride=1)), torch.nn.ReLU(), Flatten(),\n",
    "            init_(torch.nn.Linear(32 * 7 * 7, hidden_size)), torch.nn.ReLU())\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "\n",
    "        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        x = self.main(inputs / 255.0)\n",
    "\n",
    "        if self.is_recurrent:\n",
    "            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n",
    "\n",
    "        return self.critic_linear(x), x, rnn_hxs\n",
    "\n",
    "\n",
    "class MLPBase(NNBase):\n",
    "    def __init__(self, num_inputs, recurrent=False, hidden_size=64):\n",
    "        super(MLPBase, self).__init__(recurrent, num_inputs, hidden_size)\n",
    "\n",
    "        if recurrent:\n",
    "            num_inputs = hidden_size\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), np.sqrt(2))\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            init_(torch.nn.Linear(num_inputs, hidden_size)), torch.nn.Tanh(),\n",
    "            init_(torch.nn.Linear(hidden_size, hidden_size)), torch.nn.Tanh())\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            init_(torch.nn.Linear(num_inputs, hidden_size)), torch.nn.Tanh(),\n",
    "            init_(torch.nn.Linear(hidden_size, hidden_size)), torch.nn.Tanh())\n",
    "\n",
    "        self.critic_linear = init_(torch.nn.Linear(hidden_size, 1))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        x = inputs\n",
    "\n",
    "        if self.is_recurrent:\n",
    "            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n",
    "\n",
    "        hidden_critic = self.critic(x)\n",
    "        hidden_actor = self.actor(x)\n",
    "\n",
    "        return self.critic_linear(hidden_critic), hidden_actor, rnn_hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Modify standard PyTorch distributions so they are compatible with this code.\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "# Standardize distribution interfaces\n",
    "#\n",
    "\n",
    "# Categorical\n",
    "class FixedCategorical(torch.distributions.Categorical):\n",
    "    def sample(self):\n",
    "        return super().sample().unsqueeze(-1)\n",
    "\n",
    "    def log_probs(self, actions):\n",
    "        return (\n",
    "            super()\n",
    "            .log_prob(actions.squeeze(-1))\n",
    "            .view(actions.size(0), -1)\n",
    "            .sum(-1)\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "    def mode(self):\n",
    "        return self.probs.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "# Normal\n",
    "class FixedNormal(torch.distributions.Normal):\n",
    "    def log_probs(self, actions):\n",
    "        return super().log_prob(actions).sum(-1, keepdim=True)\n",
    "\n",
    "    def entrop(self):\n",
    "        return super.entropy().sum(-1)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "\n",
    "# Bernoulli\n",
    "class FixedBernoulli(torch.distributions.Bernoulli):\n",
    "    def log_probs(self, actions):\n",
    "        return super.log_prob(actions).view(actions.size(0), -1).sum(-1).unsqueeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        return super().entropy().sum(-1)\n",
    "\n",
    "    def mode(self):\n",
    "        return torch.gt(self.probs, 0.5).float()\n",
    "\n",
    "\n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "\n",
    "        init_ = lambda m: init(\n",
    "            m,\n",
    "            nn.init.orthogonal_,\n",
    "            lambda x: nn.init.constant_(x, 0),\n",
    "            gain=0.01)\n",
    "\n",
    "        self.linear = init_(nn.Linear(num_inputs, num_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return FixedCategorical(logits=x)\n",
    "\n",
    "\n",
    "class DiagGaussian(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DiagGaussian, self).__init__()\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "\n",
    "        self.fc_mean = init_(nn.Linear(num_inputs, num_outputs))\n",
    "        self.logstd = AddBias(torch.zeros(num_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_mean = self.fc_mean(x)\n",
    "\n",
    "        #  An ugly hack for my KFAC implementation.\n",
    "        zeros = torch.zeros(action_mean.size())\n",
    "        if x.is_cuda:\n",
    "            zeros = zeros.cuda()\n",
    "\n",
    "        action_logstd = self.logstd(zeros)\n",
    "        return FixedNormal(action_mean, action_logstd.exp())\n",
    "\n",
    "\n",
    "class Bernoulli(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Bernoulli, self).__init__()\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "\n",
    "        self.linear = init_(nn.Linear(num_inputs, num_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return FixedBernoulli(logits=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class PPO():\n",
    "    def __init__(self,\n",
    "                 actor_critic,\n",
    "                 clip_param,\n",
    "                 ppo_epoch,\n",
    "                 num_mini_batch,\n",
    "                 value_loss_coef,\n",
    "                 entropy_coef,\n",
    "                 lr=None,\n",
    "                 eps=None,\n",
    "                 max_grad_norm=None,\n",
    "                 use_clipped_value_loss=True):\n",
    "\n",
    "        self.actor_critic = actor_critic\n",
    "\n",
    "        self.clip_param = clip_param\n",
    "        self.ppo_epoch = ppo_epoch\n",
    "        self.num_mini_batch = num_mini_batch\n",
    "\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.use_clipped_value_loss = use_clipped_value_loss\n",
    "\n",
    "        self.optimizer = optim.Adam(actor_critic.parameters(), lr=lr, eps=eps)\n",
    "\n",
    "    def update(self, rollouts):\n",
    "        advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]\n",
    "        advantages = (advantages - advantages.mean()) / (\n",
    "            advantages.std() + 1e-5)\n",
    "\n",
    "        value_loss_epoch = 0\n",
    "        action_loss_epoch = 0\n",
    "        dist_entropy_epoch = 0\n",
    "\n",
    "        for e in range(self.ppo_epoch):\n",
    "            if self.actor_critic.is_recurrent:\n",
    "                data_generator = rollouts.recurrent_generator(\n",
    "                    advantages, self.num_mini_batch)\n",
    "            else:\n",
    "                data_generator = rollouts.feed_forward_generator(\n",
    "                    advantages, self.num_mini_batch)\n",
    "\n",
    "            for sample in data_generator:\n",
    "                obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                   value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, \\\n",
    "                        adv_targ = sample\n",
    "\n",
    "                # Reshape to do in a single forward pass for all steps\n",
    "                values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n",
    "                    obs_batch, recurrent_hidden_states_batch, masks_batch,\n",
    "                    actions_batch)\n",
    "\n",
    "                ratio = torch.exp(action_log_probs -\n",
    "                                  old_action_log_probs_batch)\n",
    "                surr1 = ratio * adv_targ\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n",
    "                                    1.0 + self.clip_param) * adv_targ\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                if self.use_clipped_value_loss:\n",
    "                    value_pred_clipped = value_preds_batch + \\\n",
    "                        (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n",
    "                    value_losses = (values - return_batch).pow(2)\n",
    "                    value_losses_clipped = (\n",
    "                        value_pred_clipped - return_batch).pow(2)\n",
    "                    value_loss = 0.5 * torch.max(value_losses,\n",
    "                                                 value_losses_clipped).mean()\n",
    "                else:\n",
    "                    value_loss = 0.5 * (return_batch - values).pow(2).mean()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                (value_loss * self.value_loss_coef + action_loss -\n",
    "                 dist_entropy * self.entropy_coef).backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n",
    "                                         self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                value_loss_epoch += value_loss.item()\n",
    "                action_loss_epoch += action_loss.item()\n",
    "                dist_entropy_epoch += dist_entropy.item()\n",
    "\n",
    "        num_updates = self.ppo_epoch * self.num_mini_batch\n",
    "\n",
    "        value_loss_epoch /= num_updates\n",
    "        action_loss_epoch /= num_updates\n",
    "        dist_entropy_epoch /= num_updates\n",
    "\n",
    "        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_env_steps = 10e6\n",
    "num_steps = 32\n",
    "num_processes = 4\n",
    "gamma = 0.99\n",
    "seed = 0\n",
    "device = \"cpu\"\n",
    "env_name = \"sin-v0\"\n",
    "\n",
    "envs = make_vec_envs(env_name, seed, num_processes,\n",
    "                         gamma, \".\", device, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = MLPBase\n",
    "obs_shape = envs.observation_space.shape\n",
    "actor_critic = Policy(obs_shape,\n",
    "                      envs.action_space, base=base,\n",
    "                      base_kwargs={'recurrent': False})\n",
    "\n",
    "\n",
    "\n",
    "clip_param = 0.2\n",
    "ppo_epoch = 4\n",
    "num_mini_batch = 16\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "lr = 7e-4\n",
    "eps = 1e-5\n",
    "max_grad_norm=0.5\n",
    "\n",
    "\n",
    "agent = PPO(actor_critic,\n",
    "          clip_param,\n",
    "          ppo_epoch,\n",
    "          num_mini_batch,\n",
    "          value_loss_coef,\n",
    "          entropy_coef,\n",
    "          lr=lr,\n",
    "          eps=eps,\n",
    "          max_grad_norm=max_grad_norm,\n",
    "          use_clipped_value_loss=True)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = RolloutStorage(num_steps, num_processes,\n",
    "                          envs.observation_space.shape, envs.action_space,\n",
    "                          actor_critic.recurrent_hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = int(num_env_steps) // num_steps // num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates\n",
    "eval_interval=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(actor_critic, ob_rms, env_name, seed, num_processes, eval_log_dir,\n",
    "             device):\n",
    "    eval_envs = make_vec_envs(env_name, seed + num_processes, num_processes,\n",
    "                              None, eval_log_dir, device, True)\n",
    "\n",
    "    vec_norm = get_vec_normalize(eval_envs)\n",
    "    if vec_norm is not None:\n",
    "        vec_norm.eval()\n",
    "        vec_norm.ob_rms = ob_rms\n",
    "\n",
    "    eval_episode_rewards = []\n",
    "\n",
    "    obs = eval_envs.reset()\n",
    "    eval_recurrent_hidden_states = torch.zeros(\n",
    "        num_processes, actor_critic.recurrent_hidden_state_size, device=device)\n",
    "    eval_masks = torch.zeros(num_processes, 1, device=device)\n",
    "\n",
    "    while len(eval_episode_rewards) < 10:\n",
    "        with torch.no_grad():\n",
    "            _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n",
    "                obs,\n",
    "                eval_recurrent_hidden_states,\n",
    "                eval_masks,\n",
    "                deterministic=True)\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, _, done, infos = eval_envs.step(action)\n",
    "\n",
    "        eval_masks = torch.tensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done],\n",
    "            dtype=torch.float32,\n",
    "            device=device)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                eval_episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "    eval_envs.close()\n",
    "\n",
    "    print(\" Evaluation using {} episodes: mean reward {:.5f}\\n\".format(\n",
    "        len(eval_episode_rewards), np.mean(eval_episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 12 episodes: mean reward 145.21173\n",
      "\n",
      "Updates 20, num timesteps 2688, FPS 279 \n",
      " Last 10 training episodes: mean/median reward 95.0/96.7, min/max reward 79.2/106.5\n",
      "\n",
      " Evaluation using 12 episodes: mean reward 150.12272\n",
      "\n",
      " Evaluation using 12 episodes: mean reward 150.06187\n",
      "\n",
      "Updates 40, num timesteps 5248, FPS 298 \n",
      " Last 10 training episodes: mean/median reward 113.5/114.5, min/max reward 101.9/120.3\n",
      "\n",
      " Evaluation using 12 episodes: mean reward 147.82232\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-56196eaf577d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m                                  gae_lambda, use_proper_time_limits)\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mvalue_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mrollouts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d899a64cbf24>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, rollouts)\u001b[0m\n\u001b[0;32m     81\u001b[0m                  dist_entropy * self.entropy_coef).backward()\n\u001b[0;32m     82\u001b[0m                 nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n\u001b[1;32m---> 83\u001b[1;33m                                          self.max_grad_norm)\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Riccardo\\Anaconda3\\envs\\sequential-transfer-rl\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mparam_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Riccardo\\Anaconda3\\envs\\sequential-transfer-rl\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fro\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;34mr\"\"\"See :func:`torch.norm`\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Riccardo\\Anaconda3\\envs\\sequential-transfer-rl\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"nuc\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"fro\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_linear_lr_decay = False\n",
    "gamma = 0.99\n",
    "use_gae = False\n",
    "gae_lambda = 0.95\n",
    "use_proper_time_limits = False\n",
    "log_interval = 20\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for j in range(num_updates):\n",
    "\n",
    "        if use_linear_lr_decay:\n",
    "            # decrease learning rate linearly\n",
    "            utils.update_linear_schedule(\n",
    "                agent.optimizer, j, num_updates,\n",
    "                agent.optimizer.lr if args.algo == \"acktr\" else args.lr)\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Sample actions\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "            for info in infos:\n",
    "                if 'episode' in info.keys():\n",
    "                    episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "            bad_masks = torch.FloatTensor(\n",
    "                [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "                 for info in infos])\n",
    "            rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                            action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = actor_critic.get_value(\n",
    "                rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "                rollouts.masks[-1]).detach()\n",
    "\n",
    "        rollouts.compute_returns(next_value, use_gae, gamma,\n",
    "                                 gae_lambda, use_proper_time_limits)\n",
    "\n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "        rollouts.after_update()\n",
    "\n",
    "        # save for every interval-th episode or for the last epoch\n",
    "        \"\"\"\n",
    "        if (j % args.save_interval == 0\n",
    "                or j == num_updates - 1) and args.save_dir != \"\":\n",
    "            save_path = os.path.join(args.save_dir, args.algo)\n",
    "            try:\n",
    "                os.makedirs(save_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            torch.save([\n",
    "                actor_critic,\n",
    "                getattr(utils.get_vec_normalize(envs), 'ob_rms', None)\n",
    "            ], os.path.join(save_path, args.env_name + \".pt\"))\n",
    "        \"\"\"\n",
    "\n",
    "        if j % log_interval == 0 and len(episode_rewards) > 1:\n",
    "            total_num_steps = (j + 1) * num_processes * num_steps\n",
    "            end = time.time()\n",
    "            print(\n",
    "                \"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "                .format(j, total_num_steps,\n",
    "                        int(total_num_steps / (end - start)),\n",
    "                        len(episode_rewards), np.mean(episode_rewards),\n",
    "                        np.median(episode_rewards), np.min(episode_rewards),\n",
    "                        np.max(episode_rewards), dist_entropy, value_loss,\n",
    "                        action_loss))\n",
    "\n",
    "        \n",
    "        if (eval_interval is not None and len(episode_rewards) > 1\n",
    "                and j % eval_interval == 0):\n",
    "            ob_rms = get_vec_normalize(envs).ob_rms\n",
    "            evaluate(actor_critic, ob_rms, env_name, seed,\n",
    "                     num_processes, \".\", device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo_a2c.envs import make_vec_envs_multi_task\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_sin\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.214349494620322, 8.792505510258366, 6.178676205168022, 6.03752836212805]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"sin-v0\"\n",
    "seed = 0\n",
    "num_processes = 4\n",
    "gamma = None\n",
    "log_dir = \".\"\n",
    "device = \"cpu\"\n",
    "allow_early_resets = False\n",
    "amplitude_list = [10* np.random.rand() for _ in range(num_processes)]\n",
    "multi_tasks_args = [{'amplitude': amplitude_list[i]} for i in range(num_processes)]\n",
    "amplitude_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating env with amplitude 8.214349494620322\n",
      "Creating env with amplitude 8.792505510258366\n",
      "Creating env with amplitude 6.178676205168022\n",
      "Creating env with amplitude 6.03752836212805\n"
     ]
    }
   ],
   "source": [
    "envs = make_vec_envs_multi_task(env_name,\n",
    "                             seed,\n",
    "                             num_processes,\n",
    "                             gamma,\n",
    "                             log_dir,\n",
    "                             device,\n",
    "                             allow_early_resets,\n",
    "                             multi_tasks_args,\n",
    "                             num_frame_stack=None)\n",
    "\n",
    "envs.reset()\n",
    "action = torch.tensor([[1.75],[1.75],[1.75],[1.75]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = (4, )\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.nn.Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.Tanh"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tanh()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
